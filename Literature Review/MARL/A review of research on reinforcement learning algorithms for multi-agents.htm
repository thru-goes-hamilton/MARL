<!doctype html>
  <html lang="en-US" class="Preview">
    <head>
      <meta name="citation_pii" content="S0925231224008397" />
<meta name="citation_id" content="128068" />
<meta name="citation_issn" content="0925-2312" />
<meta name="citation_volume" content="599" />
<meta name="citation_publisher" content="Elsevier" />
<meta name="citation_firstpage" content="128068" />
<meta name="citation_journal_title" content="Neurocomputing" />
<meta name="citation_type" content="JOUR" />
<meta name="citation_doi" content="10.1016/j.neucom.2024.128068" />
<meta name="dc.identifier" content="10.1016/j.neucom.2024.128068" />
<meta name="citation_article_type" content="Short review" />
<meta property=og:description content="In recent years, multi-agent reinforcement learning techniques have been widely used and evolved in the field of artificial intelligence. However, traâ€¦" />
<meta property=og:image content="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224X00296-cov150h.gif" />
<meta name="citation_title" content="A review of research on reinforcement learning algorithms for multi-agents" />
<meta property=og:title content="A review of research on reinforcement learning algorithms for multi-agents" />
<meta name="citation_publication_date" content="2024/09/28" />
<meta name="citation_online_date" content="2024/06/18" />
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOCACHE,NOODP,NOYDIR" />
      <title>A review of research on reinforcement learning algorithms for multi-agents - ScienceDirect</title>
      <link rel="canonical" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231224008397" />
      <meta name="tdm-reservation" content="1">
      <meta name="tdm-policy" content="https://www.elsevier.com/tdm/tdmrep-policy.json">
      <meta property="og:type" content="article" />
      <meta name="viewport" content="initial-scale=1" />
      <meta name="SDTech" content="Proudly brought to you by the SD Technology team" />
      <script type="41105e43d13887f67dd11be4-text/javascript">(function newRelicBrowserProSPA() {
  ;
  window.NREUM || (NREUM = {});
  NREUM.init = {
    privacy: {
      cookies_enabled: true
    },
    ajax: {
      deny_list: ["bam-cell.nr-data.net"]
    }
  };
  ;
  NREUM.loader_config = {
    accountID: "2128461",
    trustKey: "2038175",
    agentID: "1118783207",
    licenseKey: "7ac4127487",
    applicationID: "814813181"
  };
  ;
  NREUM.info = {
    beacon: "bam.nr-data.net",
    errorBeacon: "bam.nr-data.net",
    licenseKey: "7ac4127487",
    applicationID: "814813181",
    sa: 1
  };
  ; /*! For license information please see nr-loader-spa-1.238.0.min.js.LICENSE.txt */
  (() => {
    "use strict";

    var e,
      t,
      r = {
        5763: (e, t, r) => {
          r.d(t, {
            P_: () => f,
            Mt: () => p,
            C5: () => s,
            DL: () => v,
            OP: () => T,
            lF: () => D,
            Yu: () => y,
            Dg: () => h,
            CX: () => c,
            GE: () => b,
            sU: () => _
          });
          var n = r(8632),
            i = r(9567);
          const o = {
              beacon: n.ce.beacon,
              errorBeacon: n.ce.errorBeacon,
              licenseKey: void 0,
              applicationID: void 0,
              sa: void 0,
              queueTime: void 0,
              applicationTime: void 0,
              ttGuid: void 0,
              user: void 0,
              account: void 0,
              product: void 0,
              extra: void 0,
              jsAttributes: {},
              userAttributes: void 0,
              atts: void 0,
              transactionName: void 0,
              tNamePlain: void 0
            },
            a = {};
          function s(e) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            if (!a[e]) throw new Error("Info for ".concat(e, " was never set"));
            return a[e];
          }
          function c(e, t) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            a[e] = (0, i.D)(t, o), (0, n.Qy)(e, a[e], "info");
          }
          var u = r(7056);
          const d = () => {
              const e = {
                blockSelector: "[data-nr-block]",
                maskInputOptions: {
                  password: !0
                }
              };
              return {
                allow_bfcache: !0,
                privacy: {
                  cookies_enabled: !0
                },
                ajax: {
                  deny_list: void 0,
                  block_internal: !0,
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                distributed_tracing: {
                  enabled: void 0,
                  exclude_newrelic_header: void 0,
                  cors_use_newrelic_header: void 0,
                  cors_use_tracecontext_headers: void 0,
                  allowed_origins: void 0
                },
                session: {
                  domain: void 0,
                  expiresMs: u.oD,
                  inactiveMs: u.Hb
                },
                ssl: void 0,
                obfuscate: void 0,
                jserrors: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                metrics: {
                  enabled: !0
                },
                page_action: {
                  enabled: !0,
                  harvestTimeSeconds: 30
                },
                page_view_event: {
                  enabled: !0
                },
                page_view_timing: {
                  enabled: !0,
                  harvestTimeSeconds: 30,
                  long_task: !1
                },
                session_trace: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                harvest: {
                  tooManyRequestsDelay: 60
                },
                session_replay: {
                  enabled: !1,
                  harvestTimeSeconds: 60,
                  sampleRate: .1,
                  errorSampleRate: .1,
                  maskTextSelector: "*",
                  maskAllInputs: !0,
                  get blockClass() {
                    return "nr-block";
                  },
                  get ignoreClass() {
                    return "nr-ignore";
                  },
                  get maskTextClass() {
                    return "nr-mask";
                  },
                  get blockSelector() {
                    return e.blockSelector;
                  },
                  set blockSelector(t) {
                    e.blockSelector += ",".concat(t);
                  },
                  get maskInputOptions() {
                    return e.maskInputOptions;
                  },
                  set maskInputOptions(t) {
                    e.maskInputOptions = {
                      ...t,
                      password: !0
                    };
                  }
                },
                spa: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                }
              };
            },
            l = {};
          function f(e) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            if (!l[e]) throw new Error("Configuration for ".concat(e, " was never set"));
            return l[e];
          }
          function h(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            l[e] = (0, i.D)(t, d()), (0, n.Qy)(e, l[e], "config");
          }
          function p(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            var r = f(e);
            if (r) {
              for (var n = t.split("."), i = 0; i < n.length - 1; i++) if ("object" != typeof (r = r[n[i]])) return;
              r = r[n[n.length - 1]];
            }
            return r;
          }
          const g = {
              accountID: void 0,
              trustKey: void 0,
              agentID: void 0,
              licenseKey: void 0,
              applicationID: void 0,
              xpid: void 0
            },
            m = {};
          function v(e) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            if (!m[e]) throw new Error("LoaderConfig for ".concat(e, " was never set"));
            return m[e];
          }
          function b(e, t) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            m[e] = (0, i.D)(t, g), (0, n.Qy)(e, m[e], "loader_config");
          }
          const y = (0, n.mF)().o;
          var w = r(385),
            A = r(6818);
          const x = {
              buildEnv: A.Re,
              bytesSent: {},
              queryBytesSent: {},
              customTransaction: void 0,
              disabled: !1,
              distMethod: A.gF,
              isolatedBacklog: !1,
              loaderType: void 0,
              maxBytes: 3e4,
              offset: Math.floor(w._A?.performance?.timeOrigin || w._A?.performance?.timing?.navigationStart || Date.now()),
              onerror: void 0,
              origin: "" + w._A.location,
              ptid: void 0,
              releaseIds: {},
              session: void 0,
              xhrWrappable: "function" == typeof w._A.XMLHttpRequest?.prototype?.addEventListener,
              version: A.q4,
              denyList: void 0
            },
            E = {};
          function T(e) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            if (!E[e]) throw new Error("Runtime for ".concat(e, " was never set"));
            return E[e];
          }
          function _(e, t) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            E[e] = (0, i.D)(t, x), (0, n.Qy)(e, E[e], "runtime");
          }
          function D(e) {
            return function (e) {
              try {
                const t = s(e);
                return !!t.licenseKey && !!t.errorBeacon && !!t.applicationID;
              } catch (e) {
                return !1;
              }
            }(e);
          }
        },
        9567: (e, t, r) => {
          r.d(t, {
            D: () => i
          });
          var n = r(50);
          function i(e, t) {
            try {
              if (!e || "object" != typeof e) return (0, n.Z)("Setting a Configurable requires an object as input");
              if (!t || "object" != typeof t) return (0, n.Z)("Setting a Configurable requires a model to set its initial properties");
              const r = Object.create(Object.getPrototypeOf(t), Object.getOwnPropertyDescriptors(t)),
                o = 0 === Object.keys(r).length ? e : r;
              for (let a in o) if (void 0 !== e[a]) try {
                "object" == typeof e[a] && "object" == typeof t[a] ? r[a] = i(e[a], t[a]) : r[a] = e[a];
              } catch (e) {
                (0, n.Z)("An error occurred while setting a property of a Configurable", e);
              }
              return r;
            } catch (e) {
              (0, n.Z)("An error occured while setting a Configurable", e);
            }
          }
        },
        6818: (e, t, r) => {
          r.d(t, {
            Re: () => i,
            gF: () => o,
            q4: () => n
          });
          const n = "1.238.0",
            i = "PROD",
            o = "CDN";
        },
        385: (e, t, r) => {
          r.d(t, {
            FN: () => a,
            IF: () => u,
            Nk: () => l,
            Tt: () => s,
            _A: () => o,
            il: () => n,
            pL: () => c,
            v6: () => i,
            w1: () => d
          });
          const n = "undefined" != typeof window && !!window.document,
            i = "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self.navigator instanceof WorkerNavigator || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis.navigator instanceof WorkerNavigator),
            o = n ? window : "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis),
            a = "" + o?.location,
            s = /iPad|iPhone|iPod/.test(navigator.userAgent),
            c = s && "undefined" == typeof SharedWorker,
            u = (() => {
              const e = navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);
              return Array.isArray(e) && e.length >= 2 ? +e[1] : 0;
            })(),
            d = Boolean(n && window.document.documentMode),
            l = !!navigator.sendBeacon;
        },
        1117: (e, t, r) => {
          r.d(t, {
            w: () => o
          });
          var n = r(50);
          const i = {
            agentIdentifier: "",
            ee: void 0
          };
          class o {
            constructor(e) {
              try {
                if ("object" != typeof e) return (0, n.Z)("shared context requires an object as input");
                this.sharedContext = {}, Object.assign(this.sharedContext, i), Object.entries(e).forEach(e => {
                  let [t, r] = e;
                  Object.keys(i).includes(t) && (this.sharedContext[t] = r);
                });
              } catch (e) {
                (0, n.Z)("An error occured while setting SharedContext", e);
              }
            }
          }
        },
        8e3: (e, t, r) => {
          r.d(t, {
            L: () => d,
            R: () => c
          });
          var n = r(8325),
            i = r(1284),
            o = r(4322),
            a = r(3325);
          const s = {};
          function c(e, t) {
            const r = {
              staged: !1,
              priority: a.p[t] || 0
            };
            u(e), s[e].get(t) || s[e].set(t, r);
          }
          function u(e) {
            e && (s[e] || (s[e] = new Map()));
          }
          function d() {
            let e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : "",
              t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : "feature";
            if (u(e), !e || !s[e].get(t)) return a(t);
            s[e].get(t).staged = !0;
            const r = [...s[e]];
            function a(t) {
              const r = e ? n.ee.get(e) : n.ee,
                a = o.X.handlers;
              if (r.backlog && a) {
                var s = r.backlog[t],
                  c = a[t];
                if (c) {
                  for (var u = 0; s && u < s.length; ++u) l(s[u], c);
                  (0, i.D)(c, function (e, t) {
                    (0, i.D)(t, function (t, r) {
                      r[0].on(e, r[1]);
                    });
                  });
                }
                delete a[t], r.backlog[t] = null, r.emit("drain-" + t, []);
              }
            }
            r.every(e => {
              let [t, r] = e;
              return r.staged;
            }) && (r.sort((e, t) => e[1].priority - t[1].priority), r.forEach(e => {
              let [t] = e;
              a(t);
            }));
          }
          function l(e, t) {
            var r = e[1];
            (0, i.D)(t[r], function (t, r) {
              var n = e[0];
              if (r[0] === n) {
                var i = r[1],
                  o = e[3],
                  a = e[2];
                i.apply(o, a);
              }
            });
          }
        },
        8325: (e, t, r) => {
          r.d(t, {
            A: () => c,
            ee: () => u
          });
          var n = r(8632),
            i = r(2210),
            o = r(5763);
          class a {
            constructor(e) {
              this.contextId = e;
            }
          }
          var s = r(3117);
          const c = "nr@context:".concat(s.a),
            u = function e(t, r) {
              var n = {},
                s = {},
                d = {},
                f = !1;
              try {
                f = 16 === r.length && (0, o.OP)(r).isolatedBacklog;
              } catch (e) {}
              var h = {
                on: g,
                addEventListener: g,
                removeEventListener: function (e, t) {
                  var r = n[e];
                  if (!r) return;
                  for (var i = 0; i < r.length; i++) r[i] === t && r.splice(i, 1);
                },
                emit: function (e, r, n, i, o) {
                  !1 !== o && (o = !0);
                  if (u.aborted && !i) return;
                  t && o && t.emit(e, r, n);
                  for (var a = p(n), c = m(e), d = c.length, l = 0; l < d; l++) c[l].apply(a, r);
                  var f = b()[s[e]];
                  f && f.push([h, e, r, a]);
                  return a;
                },
                get: v,
                listeners: m,
                context: p,
                buffer: function (e, t) {
                  const r = b();
                  if (t = t || "feature", h.aborted) return;
                  Object.entries(e || {}).forEach(e => {
                    let [n, i] = e;
                    s[i] = t, t in r || (r[t] = []);
                  });
                },
                abort: l,
                aborted: !1,
                isBuffering: function (e) {
                  return !!b()[s[e]];
                },
                debugId: r,
                backlog: f ? {} : t && "object" == typeof t.backlog ? t.backlog : {}
              };
              return h;
              function p(e) {
                return e && e instanceof a ? e : e ? (0, i.X)(e, c, () => new a(c)) : new a(c);
              }
              function g(e, t) {
                n[e] = m(e).concat(t);
              }
              function m(e) {
                return n[e] || [];
              }
              function v(t) {
                return d[t] = d[t] || e(h, t);
              }
              function b() {
                return h.backlog;
              }
            }(void 0, "globalEE"),
            d = (0, n.fP)();
          function l() {
            u.aborted = !0, u.backlog = {};
          }
          d.ee || (d.ee = u);
        },
        5546: (e, t, r) => {
          r.d(t, {
            E: () => n,
            p: () => i
          });
          var n = r(8325).ee.get("handle");
          function i(e, t, r, i, o) {
            o ? (o.buffer([e], i), o.emit(e, t, r)) : (n.buffer([e], i), n.emit(e, t, r));
          }
        },
        4322: (e, t, r) => {
          r.d(t, {
            X: () => o
          });
          var n = r(5546);
          o.on = a;
          var i = o.handlers = {};
          function o(e, t, r, o) {
            a(o || n.E, i, e, t, r);
          }
          function a(e, t, r, i, o) {
            o || (o = "feature"), e || (e = n.E);
            var a = t[o] = t[o] || {};
            (a[r] = a[r] || []).push([e, i]);
          }
        },
        3239: (e, t, r) => {
          r.d(t, {
            bP: () => s,
            iz: () => c,
            m$: () => a
          });
          var n = r(385);
          let i = !1,
            o = !1;
          try {
            const e = {
              get passive() {
                return i = !0, !1;
              },
              get signal() {
                return o = !0, !1;
              }
            };
            n._A.addEventListener("test", null, e), n._A.removeEventListener("test", null, e);
          } catch (e) {}
          function a(e, t) {
            return i || o ? {
              capture: !!e,
              passive: i,
              signal: t
            } : !!e;
          }
          function s(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            window.addEventListener(e, t, a(r, n));
          }
          function c(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            document.addEventListener(e, t, a(r, n));
          }
        },
        3117: (e, t, r) => {
          r.d(t, {
            a: () => n
          });
          const n = (0, r(4402).Rl)();
        },
        4402: (e, t, r) => {
          r.d(t, {
            Ht: () => u,
            M: () => c,
            Rl: () => a,
            ky: () => s
          });
          var n = r(385);
          const i = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";
          function o(e, t) {
            return e ? 15 & e[t] : 16 * Math.random() | 0;
          }
          function a() {
            const e = n._A?.crypto || n._A?.msCrypto;
            let t,
              r = 0;
            return e && e.getRandomValues && (t = e.getRandomValues(new Uint8Array(31))), i.split("").map(e => "x" === e ? o(t, ++r).toString(16) : "y" === e ? (3 & o() | 8).toString(16) : e).join("");
          }
          function s(e) {
            const t = n._A?.crypto || n._A?.msCrypto;
            let r,
              i = 0;
            t && t.getRandomValues && (r = t.getRandomValues(new Uint8Array(31)));
            const a = [];
            for (var s = 0; s < e; s++) a.push(o(r, ++i).toString(16));
            return a.join("");
          }
          function c() {
            return s(16);
          }
          function u() {
            return s(32);
          }
        },
        7056: (e, t, r) => {
          r.d(t, {
            Bq: () => n,
            Hb: () => o,
            oD: () => i
          });
          const n = "NRBA",
            i = 144e5,
            o = 18e5;
        },
        7894: (e, t, r) => {
          function n() {
            return Math.round(performance.now());
          }
          r.d(t, {
            z: () => n
          });
        },
        7243: (e, t, r) => {
          r.d(t, {
            e: () => o
          });
          var n = r(385),
            i = {};
          function o(e) {
            if (e in i) return i[e];
            if (0 === (e || "").indexOf("data:")) return {
              protocol: "data"
            };
            let t;
            var r = n._A?.location,
              o = {};
            if (n.il) t = document.createElement("a"), t.href = e;else try {
              t = new URL(e, r.href);
            } catch (e) {
              return o;
            }
            o.port = t.port;
            var a = t.href.split("://");
            !o.port && a[1] && (o.port = a[1].split("/")[0].split("@").pop().split(":")[1]), o.port && "0" !== o.port || (o.port = "https" === a[0] ? "443" : "80"), o.hostname = t.hostname || r.hostname, o.pathname = t.pathname, o.protocol = a[0], "/" !== o.pathname.charAt(0) && (o.pathname = "/" + o.pathname);
            var s = !t.protocol || ":" === t.protocol || t.protocol === r.protocol,
              c = t.hostname === r.hostname && t.port === r.port;
            return o.sameOrigin = s && (!t.hostname || c), "/" === o.pathname && (i[e] = o), o;
          }
        },
        50: (e, t, r) => {
          function n(e, t) {
            "function" == typeof console.warn && (console.warn("New Relic: ".concat(e)), t && console.warn(t));
          }
          r.d(t, {
            Z: () => n
          });
        },
        2587: (e, t, r) => {
          r.d(t, {
            N: () => c,
            T: () => u
          });
          var n = r(8325),
            i = r(5546),
            o = r(8e3),
            a = r(3325);
          const s = {
            stn: [a.D.sessionTrace],
            err: [a.D.jserrors, a.D.metrics],
            ins: [a.D.pageAction],
            spa: [a.D.spa],
            sr: [a.D.sessionReplay, a.D.sessionTrace]
          };
          function c(e, t) {
            const r = n.ee.get(t);
            e && "object" == typeof e && (Object.entries(e).forEach(e => {
              let [t, n] = e;
              void 0 === u[t] && (s[t] ? s[t].forEach(e => {
                n ? (0, i.p)("feat-" + t, [], void 0, e, r) : (0, i.p)("block-" + t, [], void 0, e, r), (0, i.p)("rumresp-" + t, [Boolean(n)], void 0, e, r);
              }) : n && (0, i.p)("feat-" + t, [], void 0, void 0, r), u[t] = Boolean(n));
            }), Object.keys(s).forEach(e => {
              void 0 === u[e] && (s[e]?.forEach(t => (0, i.p)("rumresp-" + e, [!1], void 0, t, r)), u[e] = !1);
            }), (0, o.L)(t, a.D.pageViewEvent));
          }
          const u = {};
        },
        2210: (e, t, r) => {
          r.d(t, {
            X: () => i
          });
          var n = Object.prototype.hasOwnProperty;
          function i(e, t, r) {
            if (n.call(e, t)) return e[t];
            var i = r();
            if (Object.defineProperty && Object.keys) try {
              return Object.defineProperty(e, t, {
                value: i,
                writable: !0,
                enumerable: !1
              }), i;
            } catch (e) {}
            return e[t] = i, i;
          }
        },
        1284: (e, t, r) => {
          r.d(t, {
            D: () => n
          });
          const n = (e, t) => Object.entries(e || {}).map(e => {
            let [r, n] = e;
            return t(r, n);
          });
        },
        4351: (e, t, r) => {
          r.d(t, {
            P: () => o
          });
          var n = r(8325);
          const i = () => {
            const e = new WeakSet();
            return (t, r) => {
              if ("object" == typeof r && null !== r) {
                if (e.has(r)) return;
                e.add(r);
              }
              return r;
            };
          };
          function o(e) {
            try {
              return JSON.stringify(e, i());
            } catch (e) {
              try {
                n.ee.emit("internal-error", [e]);
              } catch (e) {}
            }
          }
        },
        3960: (e, t, r) => {
          r.d(t, {
            K: () => a,
            b: () => o
          });
          var n = r(3239);
          function i() {
            return "undefined" == typeof document || "complete" === document.readyState;
          }
          function o(e, t) {
            if (i()) return e();
            (0, n.bP)("load", e, t);
          }
          function a(e) {
            if (i()) return e();
            (0, n.iz)("DOMContentLoaded", e);
          }
        },
        8632: (e, t, r) => {
          r.d(t, {
            EZ: () => u,
            Qy: () => c,
            ce: () => o,
            fP: () => a,
            gG: () => d,
            mF: () => s
          });
          var n = r(7894),
            i = r(385);
          const o = {
            beacon: "bam.nr-data.net",
            errorBeacon: "bam.nr-data.net"
          };
          function a() {
            return i._A.NREUM || (i._A.NREUM = {}), void 0 === i._A.newrelic && (i._A.newrelic = i._A.NREUM), i._A.NREUM;
          }
          function s() {
            let e = a();
            return e.o || (e.o = {
              ST: i._A.setTimeout,
              SI: i._A.setImmediate,
              CT: i._A.clearTimeout,
              XHR: i._A.XMLHttpRequest,
              REQ: i._A.Request,
              EV: i._A.Event,
              PR: i._A.Promise,
              MO: i._A.MutationObserver,
              FETCH: i._A.fetch
            }), e;
          }
          function c(e, t, r) {
            let i = a();
            const o = i.initializedAgents || {},
              s = o[e] || {};
            return Object.keys(s).length || (s.initializedAt = {
              ms: (0, n.z)(),
              date: new Date()
            }), i.initializedAgents = {
              ...o,
              [e]: {
                ...s,
                [r]: t
              }
            }, i;
          }
          function u(e, t) {
            a()[e] = t;
          }
          function d() {
            return function () {
              let e = a();
              const t = e.info || {};
              e.info = {
                beacon: o.beacon,
                errorBeacon: o.errorBeacon,
                ...t
              };
            }(), function () {
              let e = a();
              const t = e.init || {};
              e.init = {
                ...t
              };
            }(), s(), function () {
              let e = a();
              const t = e.loader_config || {};
              e.loader_config = {
                ...t
              };
            }(), a();
          }
        },
        7956: (e, t, r) => {
          r.d(t, {
            N: () => i
          });
          var n = r(3239);
          function i(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] && arguments[1],
              r = arguments.length > 2 ? arguments[2] : void 0,
              i = arguments.length > 3 ? arguments[3] : void 0;
            return void (0, n.iz)("visibilitychange", function () {
              if (t) return void ("hidden" == document.visibilityState && e());
              e(document.visibilityState);
            }, r, i);
          }
        },
        1214: (e, t, r) => {
          r.d(t, {
            em: () => b,
            u5: () => j,
            QU: () => O,
            _L: () => I,
            Gm: () => H,
            Lg: () => L,
            BV: () => G,
            Kf: () => K
          });
          var n = r(8325),
            i = r(3117);
          const o = "nr@original:".concat(i.a);
          var a = Object.prototype.hasOwnProperty,
            s = !1;
          function c(e, t) {
            return e || (e = n.ee), r.inPlace = function (e, t, n, i, o) {
              n || (n = "");
              const a = "-" === n.charAt(0);
              for (let s = 0; s < t.length; s++) {
                const c = t[s],
                  u = e[c];
                d(u) || (e[c] = r(u, a ? c + n : n, i, c, o));
              }
            }, r.flag = o, r;
            function r(t, r, n, s, c) {
              return d(t) ? t : (r || (r = ""), nrWrapper[o] = t, function (e, t, r) {
                if (Object.defineProperty && Object.keys) try {
                  return Object.keys(e).forEach(function (r) {
                    Object.defineProperty(t, r, {
                      get: function () {
                        return e[r];
                      },
                      set: function (t) {
                        return e[r] = t, t;
                      }
                    });
                  }), t;
                } catch (e) {
                  u([e], r);
                }
                for (var n in e) a.call(e, n) && (t[n] = e[n]);
              }(t, nrWrapper, e), nrWrapper);
              function nrWrapper() {
                var o, a, d, l;
                try {
                  a = this, o = [...arguments], d = "function" == typeof n ? n(o, a) : n || {};
                } catch (t) {
                  u([t, "", [o, a, s], d], e);
                }
                i(r + "start", [o, a, s], d, c);
                try {
                  return l = t.apply(a, o);
                } catch (e) {
                  throw i(r + "err", [o, a, e], d, c), e;
                } finally {
                  i(r + "end", [o, a, l], d, c);
                }
              }
            }
            function i(r, n, i, o) {
              if (!s || t) {
                var a = s;
                s = !0;
                try {
                  e.emit(r, n, i, t, o);
                } catch (t) {
                  u([t, r, n, i], e);
                }
                s = a;
              }
            }
          }
          function u(e, t) {
            t || (t = n.ee);
            try {
              t.emit("internal-error", e);
            } catch (e) {}
          }
          function d(e) {
            return !(e && e instanceof Function && e.apply && !e[o]);
          }
          var l = r(2210),
            f = r(385);
          const h = {},
            p = f._A.XMLHttpRequest,
            g = "addEventListener",
            m = "removeEventListener",
            v = "nr@wrapped:".concat(n.A);
          function b(e) {
            var t = function (e) {
              return (e || n.ee).get("events");
            }(e);
            if (h[t.debugId]++) return t;
            h[t.debugId] = 1;
            var r = c(t, !0);
            function i(e) {
              r.inPlace(e, [g, m], "-", o);
            }
            function o(e, t) {
              return e[1];
            }
            return "getPrototypeOf" in Object && (f.il && y(document, i), y(f._A, i), y(p.prototype, i)), t.on(g + "-start", function (e, t) {
              var n = e[1];
              if (null !== n && ("function" == typeof n || "object" == typeof n)) {
                var i = (0, l.X)(n, v, function () {
                  var e = {
                    object: function () {
                      if ("function" != typeof n.handleEvent) return;
                      return n.handleEvent.apply(n, arguments);
                    },
                    function: n
                  }[typeof n];
                  return e ? r(e, "fn-", null, e.name || "anonymous") : n;
                });
                this.wrapped = e[1] = i;
              }
            }), t.on(m + "-start", function (e) {
              e[1] = this.wrapped || e[1];
            }), t;
          }
          function y(e, t) {
            let r = e;
            for (; "object" == typeof r && !Object.prototype.hasOwnProperty.call(r, g);) r = Object.getPrototypeOf(r);
            for (var n = arguments.length, i = new Array(n > 2 ? n - 2 : 0), o = 2; o < n; o++) i[o - 2] = arguments[o];
            r && t(r, ...i);
          }
          var w = "fetch-",
            A = w + "body-",
            x = ["arrayBuffer", "blob", "json", "text", "formData"],
            E = f._A.Request,
            T = f._A.Response,
            _ = "prototype";
          const D = {};
          function j(e) {
            const t = function (e) {
              return (e || n.ee).get("fetch");
            }(e);
            if (!(E && T && f._A.fetch)) return t;
            if (D[t.debugId]++) return t;
            function r(e, r, i) {
              var o = e[r];
              "function" == typeof o && (e[r] = function () {
                var e,
                  r = [...arguments],
                  a = {};
                t.emit(i + "before-start", [r], a), a[n.A] && a[n.A].dt && (e = a[n.A].dt);
                var s = o.apply(this, r);
                return t.emit(i + "start", [r, e], s), s.then(function (e) {
                  return t.emit(i + "end", [null, e], s), e;
                }, function (e) {
                  throw t.emit(i + "end", [e], s), e;
                });
              });
            }
            return D[t.debugId] = 1, x.forEach(e => {
              r(E[_], e, A), r(T[_], e, A);
            }), r(f._A, "fetch", w), t.on(w + "end", function (e, r) {
              var n = this;
              if (r) {
                var i = r.headers.get("content-length");
                null !== i && (n.rxSize = i), t.emit(w + "done", [null, r], n);
              } else t.emit(w + "done", [e], n);
            }), t;
          }
          const C = {},
            N = ["pushState", "replaceState"];
          function O(e) {
            const t = function (e) {
              return (e || n.ee).get("history");
            }(e);
            return !f.il || C[t.debugId]++ || (C[t.debugId] = 1, c(t).inPlace(window.history, N, "-")), t;
          }
          var S = r(3239);
          const P = {},
            R = ["appendChild", "insertBefore", "replaceChild"];
          function I(e) {
            const t = function (e) {
              return (e || n.ee).get("jsonp");
            }(e);
            if (!f.il || P[t.debugId]) return t;
            P[t.debugId] = !0;
            var r = c(t),
              i = /[?&](?:callback|cb)=([^&#]+)/,
              o = /(.*)\.([^.]+)/,
              a = /^(\w+)(\.|$)(.*)$/;
            function s(e, t) {
              if (!e) return t;
              const r = e.match(a),
                n = r[1];
              return s(r[3], t[n]);
            }
            return r.inPlace(Node.prototype, R, "dom-"), t.on("dom-start", function (e) {
              !function (e) {
                if (!e || "string" != typeof e.nodeName || "script" !== e.nodeName.toLowerCase()) return;
                if ("function" != typeof e.addEventListener) return;
                var n = (a = e.src, c = a.match(i), c ? c[1] : null);
                var a, c;
                if (!n) return;
                var u = function (e) {
                  var t = e.match(o);
                  if (t && t.length >= 3) return {
                    key: t[2],
                    parent: s(t[1], window)
                  };
                  return {
                    key: e,
                    parent: window
                  };
                }(n);
                if ("function" != typeof u.parent[u.key]) return;
                var d = {};
                function l() {
                  t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                function f() {
                  t.emit("jsonp-error", [], d), t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                r.inPlace(u.parent, [u.key], "cb-", d), e.addEventListener("load", l, (0, S.m$)(!1)), e.addEventListener("error", f, (0, S.m$)(!1)), t.emit("new-jsonp", [e.src], d);
              }(e[0]);
            }), t;
          }
          const k = {};
          function H(e) {
            const t = function (e) {
              return (e || n.ee).get("mutation");
            }(e);
            if (!f.il || k[t.debugId]) return t;
            k[t.debugId] = !0;
            var r = c(t),
              i = f._A.MutationObserver;
            return i && (window.MutationObserver = function (e) {
              return this instanceof i ? new i(r(e, "fn-")) : i.apply(this, arguments);
            }, MutationObserver.prototype = i.prototype), t;
          }
          const z = {};
          function L(e) {
            const t = function (e) {
              return (e || n.ee).get("promise");
            }(e);
            if (z[t.debugId]) return t;
            z[t.debugId] = !0;
            var r = t.context,
              i = c(t),
              a = f._A.Promise;
            return a && function () {
              function e(r) {
                var n = t.context(),
                  o = i(r, "executor-", n, null, !1);
                const s = Reflect.construct(a, [o], e);
                return t.context(s).getCtx = function () {
                  return n;
                }, s;
              }
              f._A.Promise = e, Object.defineProperty(e, "name", {
                value: "Promise"
              }), e.toString = function () {
                return a.toString();
              }, Object.setPrototypeOf(e, a), ["all", "race"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  let i = !1;
                  [...(e || [])].forEach(e => {
                    this.resolve(e).then(a("all" === r), a(!1));
                  });
                  const o = n.apply(this, arguments);
                  return o;
                  function a(e) {
                    return function () {
                      t.emit("propagate", [null, !i], o, !1, !1), i = i || !e;
                    };
                  }
                };
              }), ["resolve", "reject"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  const r = n.apply(this, arguments);
                  return e !== r && t.emit("propagate", [e, !0], r, !1, !1), r;
                };
              }), e.prototype = a.prototype;
              const n = a.prototype.then;
              a.prototype.then = function () {
                var e = this,
                  o = r(e);
                o.promise = e;
                for (var a = arguments.length, s = new Array(a), c = 0; c < a; c++) s[c] = arguments[c];
                s[0] = i(s[0], "cb-", o, null, !1), s[1] = i(s[1], "cb-", o, null, !1);
                const u = n.apply(this, s);
                return o.nextPromise = u, t.emit("propagate", [e, !0], u, !1, !1), u;
              }, a.prototype.then[o] = n, t.on("executor-start", function (e) {
                e[0] = i(e[0], "resolve-", this, null, !1), e[1] = i(e[1], "resolve-", this, null, !1);
              }), t.on("executor-err", function (e, t, r) {
                e[1](r);
              }), t.on("cb-end", function (e, r, n) {
                t.emit("propagate", [n, !0], this.nextPromise, !1, !1);
              }), t.on("propagate", function (e, r, n) {
                this.getCtx && !r || (this.getCtx = function () {
                  if (e instanceof Promise) var r = t.context(e);
                  return r && r.getCtx ? r.getCtx() : this;
                });
              });
            }(), t;
          }
          const M = {},
            B = "setTimeout",
            F = "setInterval",
            U = "clearTimeout",
            q = "-start",
            Z = "-",
            V = [B, "setImmediate", F, U, "clearImmediate"];
          function G(e) {
            const t = function (e) {
              return (e || n.ee).get("timer");
            }(e);
            if (M[t.debugId]++) return t;
            M[t.debugId] = 1;
            var r = c(t);
            return r.inPlace(f._A, V.slice(0, 2), B + Z), r.inPlace(f._A, V.slice(2, 3), F + Z), r.inPlace(f._A, V.slice(3), U + Z), t.on(F + q, function (e, t, n) {
              e[0] = r(e[0], "fn-", null, n);
            }), t.on(B + q, function (e, t, n) {
              this.method = n, this.timerDuration = isNaN(e[1]) ? 0 : +e[1], e[0] = r(e[0], "fn-", this, n);
            }), t;
          }
          var W = r(50);
          const X = {},
            Q = ["open", "send"];
          function K(e) {
            var t = e || n.ee;
            const r = function (e) {
              return (e || n.ee).get("xhr");
            }(t);
            if (X[r.debugId]++) return r;
            X[r.debugId] = 1, b(t);
            var i = c(r),
              o = f._A.XMLHttpRequest,
              a = f._A.MutationObserver,
              s = f._A.Promise,
              u = f._A.setInterval,
              d = "readystatechange",
              l = ["onload", "onerror", "onabort", "onloadstart", "onloadend", "onprogress", "ontimeout"],
              h = [],
              p = f._A.XMLHttpRequest = function (e) {
                const t = new o(e),
                  n = r.context(t);
                try {
                  r.emit("new-xhr", [t], n), t.addEventListener(d, (a = n, function () {
                    var e = this;
                    e.readyState > 3 && !a.resolved && (a.resolved = !0, r.emit("xhr-resolved", [], e)), i.inPlace(e, l, "fn-", A);
                  }), (0, S.m$)(!1));
                } catch (e) {
                  (0, W.Z)("An error occurred while intercepting XHR", e);
                  try {
                    r.emit("internal-error", [e]);
                  } catch (e) {}
                }
                var a;
                return t;
              };
            function g(e, t) {
              i.inPlace(t, ["onreadystatechange"], "fn-", A);
            }
            if (function (e, t) {
              for (var r in e) t[r] = e[r];
            }(o, p), p.prototype = o.prototype, i.inPlace(p.prototype, Q, "-xhr-", A), r.on("send-xhr-start", function (e, t) {
              g(e, t), function (e) {
                h.push(e), a && (m ? m.then(w) : u ? u(w) : (v = -v, y.data = v));
              }(t);
            }), r.on("open-xhr-start", g), a) {
              var m = s && s.resolve();
              if (!u && !s) {
                var v = 1,
                  y = document.createTextNode(v);
                new a(w).observe(y, {
                  characterData: !0
                });
              }
            } else t.on("fn-end", function (e) {
              e[0] && e[0].type === d || w();
            });
            function w() {
              for (var e = 0; e < h.length; e++) g(0, h[e]);
              h.length && (h = []);
            }
            function A(e, t) {
              return t;
            }
            return r;
          }
        },
        7825: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.ajax;
        },
        6660: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.jserrors;
        },
        3081: (e, t, r) => {
          r.d(t, {
            gF: () => o,
            mY: () => i,
            t9: () => n,
            vz: () => s,
            xS: () => a
          });
          const n = r(3325).D.metrics,
            i = "sm",
            o = "cm",
            a = "storeSupportabilityMetrics",
            s = "storeEventMetrics";
        },
        4649: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageAction;
        },
        7633: (e, t, r) => {
          r.d(t, {
            Dz: () => i,
            OJ: () => a,
            qw: () => o,
            t9: () => n
          });
          const n = r(3325).D.pageViewEvent,
            i = "firstbyte",
            o = "domcontent",
            a = "windowload";
        },
        9251: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageViewTiming;
        },
        3614: (e, t, r) => {
          r.d(t, {
            BST_RESOURCE: () => i,
            END: () => s,
            FEATURE_NAME: () => n,
            FN_END: () => u,
            FN_START: () => c,
            PUSH_STATE: () => d,
            RESOURCE: () => o,
            START: () => a
          });
          const n = r(3325).D.sessionTrace,
            i = "bstResource",
            o = "resource",
            a = "-start",
            s = "-end",
            c = "fn" + a,
            u = "fn" + s,
            d = "pushState";
        },
        7836: (e, t, r) => {
          r.d(t, {
            BODY: () => x,
            CB_END: () => E,
            CB_START: () => u,
            END: () => A,
            FEATURE_NAME: () => i,
            FETCH: () => _,
            FETCH_BODY: () => v,
            FETCH_DONE: () => m,
            FETCH_START: () => g,
            FN_END: () => c,
            FN_START: () => s,
            INTERACTION: () => f,
            INTERACTION_API: () => d,
            INTERACTION_EVENTS: () => o,
            JSONP_END: () => b,
            JSONP_NODE: () => p,
            JS_TIME: () => T,
            MAX_TIMER_BUDGET: () => a,
            REMAINING: () => l,
            SPA_NODE: () => h,
            START: () => w,
            originalSetTimeout: () => y
          });
          var n = r(5763);
          const i = r(3325).D.spa,
            o = ["click", "submit", "keypress", "keydown", "keyup", "change"],
            a = 999,
            s = "fn-start",
            c = "fn-end",
            u = "cb-start",
            d = "api-ixn-",
            l = "remaining",
            f = "interaction",
            h = "spaNode",
            p = "jsonpNode",
            g = "fetch-start",
            m = "fetch-done",
            v = "fetch-body-",
            b = "jsonp-end",
            y = n.Yu.ST,
            w = "-start",
            A = "-end",
            x = "-body",
            E = "cb" + A,
            T = "jsTime",
            _ = "fetch";
        },
        5938: (e, t, r) => {
          r.d(t, {
            W: () => o
          });
          var n = r(5763),
            i = r(8325);
          class o {
            constructor(e, t, r) {
              this.agentIdentifier = e, this.aggregator = t, this.ee = i.ee.get(e, (0, n.OP)(this.agentIdentifier).isolatedBacklog), this.featureName = r, this.blocked = !1;
            }
          }
        },
        9144: (e, t, r) => {
          r.d(t, {
            j: () => m
          });
          var n = r(3325),
            i = r(5763),
            o = r(5546),
            a = r(8325),
            s = r(7894),
            c = r(8e3),
            u = r(3960),
            d = r(385),
            l = r(50),
            f = r(3081),
            h = r(8632);
          function p() {
            const e = (0, h.gG)();
            ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease", "addPageAction", "setCurrentRouteName", "setPageViewName", "setCustomAttribute", "interaction", "noticeError", "setUserId", "setApplicationVersion"].forEach(t => {
              e[t] = function () {
                for (var r = arguments.length, n = new Array(r), i = 0; i < r; i++) n[i] = arguments[i];
                return function (t) {
                  for (var r = arguments.length, n = new Array(r > 1 ? r - 1 : 0), i = 1; i < r; i++) n[i - 1] = arguments[i];
                  let o = [];
                  return Object.values(e.initializedAgents).forEach(e => {
                    e.exposed && e.api[t] && o.push(e.api[t](...n));
                  }), o.length > 1 ? o : o[0];
                }(t, ...n);
              };
            });
          }
          var g = r(2587);
          function m(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {},
              m = arguments.length > 2 ? arguments[2] : void 0,
              v = arguments.length > 3 ? arguments[3] : void 0,
              {
                init: b,
                info: y,
                loader_config: w,
                runtime: A = {
                  loaderType: m
                },
                exposed: x = !0
              } = t;
            const E = (0, h.gG)();
            y || (b = E.init, y = E.info, w = E.loader_config), (0, i.Dg)(e, b || {}), (0, i.GE)(e, w || {}), y.jsAttributes ??= {}, d.v6 && (y.jsAttributes.isWorker = !0), (0, i.CX)(e, y);
            const T = (0, i.P_)(e);
            A.denyList = [...(T.ajax?.deny_list || []), ...(T.ajax?.block_internal ? [y.beacon, y.errorBeacon] : [])], (0, i.sU)(e, A), p();
            const _ = function (e, t) {
              t || (0, c.R)(e, "api");
              const h = {};
              var p = a.ee.get(e),
                g = p.get("tracer"),
                m = "api-",
                v = m + "ixn-";
              function b(t, r, n, o) {
                const a = (0, i.C5)(e);
                return null === r ? delete a.jsAttributes[t] : (0, i.CX)(e, {
                  ...a,
                  jsAttributes: {
                    ...a.jsAttributes,
                    [t]: r
                  }
                }), A(m, n, !0, o || null === r ? "session" : void 0)(t, r);
              }
              function y() {}
              ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease"].forEach(e => h[e] = A(m, e, !0, "api")), h.addPageAction = A(m, "addPageAction", !0, n.D.pageAction), h.setCurrentRouteName = A(m, "routeName", !0, n.D.spa), h.setPageViewName = function (t, r) {
                if ("string" == typeof t) return "/" !== t.charAt(0) && (t = "/" + t), (0, i.OP)(e).customTransaction = (r || "http://custom.transaction") + t, A(m, "setPageViewName", !0)();
              }, h.setCustomAttribute = function (e, t) {
                let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2];
                if ("string" == typeof e) {
                  if (["string", "number"].includes(typeof t) || null === t) return b(e, t, "setCustomAttribute", r);
                  (0, l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t, "> was provided."));
                } else (0, l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setUserId = function (e) {
                if ("string" == typeof e || null === e) return b("enduser.id", e, "setUserId", !0);
                (0, l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setApplicationVersion = function (e) {
                if ("string" == typeof e || null === e) return b("application.version", e, "setApplicationVersion", !1);
                (0, l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e, ">."));
              }, h.interaction = function () {
                return new y().get();
              };
              var w = y.prototype = {
                createTracer: function (e, t) {
                  var r = {},
                    i = this,
                    a = "function" == typeof t;
                  return (0, o.p)(v + "tracer", [(0, s.z)(), e, r], i, n.D.spa, p), function () {
                    if (g.emit((a ? "" : "no-") + "fn-start", [(0, s.z)(), i, a], r), a) try {
                      return t.apply(this, arguments);
                    } catch (e) {
                      throw g.emit("fn-err", [arguments, this, e], r), e;
                    } finally {
                      g.emit("fn-end", [(0, s.z)()], r);
                    }
                  };
                }
              };
              function A(e, t, r, i) {
                return function () {
                  return (0, o.p)(f.xS, ["API/" + t + "/called"], void 0, n.D.metrics, p), i && (0, o.p)(e + t, [(0, s.z)(), ...arguments], r ? null : this, i, p), r ? void 0 : this;
                };
              }
              function x() {
                r.e(111).then(r.bind(r, 7438)).then(t => {
                  let {
                    setAPI: r
                  } = t;
                  r(e), (0, c.L)(e, "api");
                }).catch(() => (0, l.Z)("Downloading runtime APIs failed..."));
              }
              return ["actionText", "setName", "setAttribute", "save", "ignore", "onEnd", "getContext", "end", "get"].forEach(e => {
                w[e] = A(v, e, void 0, n.D.spa);
              }), h.noticeError = function (e, t) {
                "string" == typeof e && (e = new Error(e)), (0, o.p)(f.xS, ["API/noticeError/called"], void 0, n.D.metrics, p), (0, o.p)("err", [e, (0, s.z)(), !1, t], void 0, n.D.jserrors, p);
              }, d.il ? (0, u.b)(() => x(), !0) : x(), h;
            }(e, v);
            return (0, h.Qy)(e, _, "api"), (0, h.Qy)(e, x, "exposed"), (0, h.EZ)("activatedFeatures", g.T), _;
          }
        },
        3325: (e, t, r) => {
          r.d(t, {
            D: () => n,
            p: () => i
          });
          const n = {
              ajax: "ajax",
              jserrors: "jserrors",
              metrics: "metrics",
              pageAction: "page_action",
              pageViewEvent: "page_view_event",
              pageViewTiming: "page_view_timing",
              sessionReplay: "session_replay",
              sessionTrace: "session_trace",
              spa: "spa"
            },
            i = {
              [n.pageViewEvent]: 1,
              [n.pageViewTiming]: 2,
              [n.metrics]: 3,
              [n.jserrors]: 4,
              [n.ajax]: 5,
              [n.sessionTrace]: 6,
              [n.pageAction]: 7,
              [n.spa]: 8,
              [n.sessionReplay]: 9
            };
        }
      },
      n = {};
    function i(e) {
      var t = n[e];
      if (void 0 !== t) return t.exports;
      var o = n[e] = {
        exports: {}
      };
      return r[e](o, o.exports, i), o.exports;
    }
    i.m = r, i.d = (e, t) => {
      for (var r in t) i.o(t, r) && !i.o(e, r) && Object.defineProperty(e, r, {
        enumerable: !0,
        get: t[r]
      });
    }, i.f = {}, i.e = e => Promise.all(Object.keys(i.f).reduce((t, r) => (i.f[r](e, t), t), [])), i.u = e => "nr-spa.1097a448-1.238.0.min.js", i.o = (e, t) => Object.prototype.hasOwnProperty.call(e, t), e = {}, t = "NRBA-1.238.0.PROD:", i.l = (r, n, o, a) => {
      if (e[r]) e[r].push(n);else {
        var s, c;
        if (void 0 !== o) for (var u = document.getElementsByTagName("script"), d = 0; d < u.length; d++) {
          var l = u[d];
          if (l.getAttribute("src") == r || l.getAttribute("data-webpack") == t + o) {
            s = l;
            break;
          }
        }
        s || (c = !0, (s = document.createElement("script")).charset = "utf-8", s.timeout = 120, i.nc && s.setAttribute("nonce", i.nc), s.setAttribute("data-webpack", t + o), s.src = r), e[r] = [n];
        var f = (t, n) => {
            s.onerror = s.onload = null, clearTimeout(h);
            var i = e[r];
            if (delete e[r], s.parentNode && s.parentNode.removeChild(s), i && i.forEach(e => e(n)), t) return t(n);
          },
          h = setTimeout(f.bind(null, void 0, {
            type: "timeout",
            target: s
          }), 12e4);
        s.onerror = f.bind(null, s.onerror), s.onload = f.bind(null, s.onload), c && document.head.appendChild(s);
      }
    }, i.r = e => {
      "undefined" != typeof Symbol && Symbol.toStringTag && Object.defineProperty(e, Symbol.toStringTag, {
        value: "Module"
      }), Object.defineProperty(e, "__esModule", {
        value: !0
      });
    }, i.p = "https://js-agent.newrelic.com/", (() => {
      var e = {
        801: 0,
        92: 0
      };
      i.f.j = (t, r) => {
        var n = i.o(e, t) ? e[t] : void 0;
        if (0 !== n) if (n) r.push(n[2]);else {
          var o = new Promise((r, i) => n = e[t] = [r, i]);
          r.push(n[2] = o);
          var a = i.p + i.u(t),
            s = new Error();
          i.l(a, r => {
            if (i.o(e, t) && (0 !== (n = e[t]) && (e[t] = void 0), n)) {
              var o = r && ("load" === r.type ? "missing" : r.type),
                a = r && r.target && r.target.src;
              s.message = "Loading chunk " + t + " failed.\n(" + o + ": " + a + ")", s.name = "ChunkLoadError", s.type = o, s.request = a, n[1](s);
            }
          }, "chunk-" + t, t);
        }
      };
      var t = (t, r) => {
          var n,
            o,
            [a, s, c] = r,
            u = 0;
          if (a.some(t => 0 !== e[t])) {
            for (n in s) i.o(s, n) && (i.m[n] = s[n]);
            if (c) c(i);
          }
          for (t && t(r); u < a.length; u++) o = a[u], i.o(e, o) && e[o] && e[o][0](), e[o] = 0;
        },
        r = self["webpackChunk:NRBA-1.238.0.PROD"] = self["webpackChunk:NRBA-1.238.0.PROD"] || [];
      r.forEach(t.bind(null, 0)), r.push = t.bind(null, r.push.bind(r));
    })(), (() => {
      var e = i(50);
      class t {
        addPageAction(t, r) {
          (0, e.Z)("Call to agent api addPageAction failed. The session trace feature is not currently initialized.");
        }
        setPageViewName(t, r) {
          (0, e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.");
        }
        setCustomAttribute(t, r, n) {
          (0, e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.");
        }
        noticeError(t, r) {
          (0, e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.");
        }
        setUserId(t) {
          (0, e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.");
        }
        setApplicationVersion(t) {
          (0, e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.");
        }
        setErrorHandler(t) {
          (0, e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.");
        }
        finished(t) {
          (0, e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.");
        }
        addRelease(t, r) {
          (0, e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.");
        }
      }
      var r = i(3325),
        n = i(5763);
      const o = Object.values(r.D);
      function a(e) {
        const t = {};
        return o.forEach(r => {
          t[r] = function (e, t) {
            return !1 !== (0, n.Mt)(t, "".concat(e, ".enabled"));
          }(r, e);
        }), t;
      }
      var s = i(9144);
      var c = i(5546),
        u = i(385),
        d = i(8e3),
        l = i(5938),
        f = i(3960);
      class h extends l.W {
        constructor(e, t, r) {
          let n = !(arguments.length > 3 && void 0 !== arguments[3]) || arguments[3];
          super(e, t, r), this.auto = n, this.abortHandler, this.featAggregate, this.onAggregateImported, n && (0, d.R)(e, r);
        }
        importAggregator() {
          let t = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
          if (this.featAggregate || !this.auto) return;
          const r = u.il && !0 === (0, n.Mt)(this.agentIdentifier, "privacy.cookies_enabled");
          let o;
          this.onAggregateImported = new Promise(e => {
            o = e;
          });
          const a = async () => {
            let n;
            try {
              if (r) {
                const {
                  setupAgentSession: e
                } = await i.e(111).then(i.bind(i, 3228));
                n = e(this.agentIdentifier);
              }
            } catch (t) {
              (0, e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.", t);
            }
            try {
              if (!this.shouldImportAgg(this.featureName, n)) return (0, d.L)(this.agentIdentifier, this.featureName), void o(!1);
              const {
                  lazyFeatureLoader: e
                } = await i.e(111).then(i.bind(i, 8582)),
                {
                  Aggregate: r
                } = await e(this.featureName, "aggregate");
              this.featAggregate = new r(this.agentIdentifier, this.aggregator, t), o(!0);
            } catch (t) {
              (0, e.Z)("Downloading and initializing ".concat(this.featureName, " failed..."), t), this.abortHandler?.(), o(!1);
            }
          };
          u.il ? (0, f.b)(() => a(), !0) : a();
        }
        shouldImportAgg(e, t) {
          return e !== r.D.sessionReplay || !!n.Yu.MO && !1 !== (0, n.Mt)(this.agentIdentifier, "session_trace.enabled") && (!!t?.isNew || !!t?.state.sessionReplay);
        }
      }
      var p = i(7633),
        g = i(7894);
      class m extends h {
        static featureName = p.t9;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          if (super(e, t, p.t9, i), ("undefined" == typeof PerformanceNavigationTiming || u.Tt) && "undefined" != typeof PerformanceTiming) {
            const t = (0, n.OP)(e);
            t[p.Dz] = Math.max(Date.now() - t.offset, 0), (0, f.K)(() => t[p.qw] = Math.max((0, g.z)() - t[p.Dz], 0)), (0, f.b)(() => {
              const e = (0, g.z)();
              t[p.OJ] = Math.max(e - t[p.Dz], 0), (0, c.p)("timing", ["load", e], void 0, r.D.pageViewTiming, this.ee);
            });
          }
          this.importAggregator();
        }
      }
      var v = i(1117),
        b = i(1284);
      class y extends v.w {
        constructor(e) {
          super(e), this.aggregatedData = {};
        }
        store(e, t, r, n, i) {
          var o = this.getBucket(e, t, r, i);
          return o.metrics = function (e, t) {
            t || (t = {
              count: 0
            });
            return t.count += 1, (0, b.D)(e, function (e, r) {
              t[e] = w(r, t[e]);
            }), t;
          }(n, o.metrics), o;
        }
        merge(e, t, r, n, i) {
          var o = this.getBucket(e, t, n, i);
          if (o.metrics) {
            var a = o.metrics;
            a.count += r.count, (0, b.D)(r, function (e, t) {
              if ("count" !== e) {
                var n = a[e],
                  i = r[e];
                i && !i.c ? a[e] = w(i.t, n) : a[e] = function (e, t) {
                  if (!t) return e;
                  t.c || (t = A(t.t));
                  return t.min = Math.min(e.min, t.min), t.max = Math.max(e.max, t.max), t.t += e.t, t.sos += e.sos, t.c += e.c, t;
                }(i, a[e]);
              }
            });
          } else o.metrics = r;
        }
        storeMetric(e, t, r, n) {
          var i = this.getBucket(e, t, r);
          return i.stats = w(n, i.stats), i;
        }
        getBucket(e, t, r, n) {
          this.aggregatedData[e] || (this.aggregatedData[e] = {});
          var i = this.aggregatedData[e][t];
          return i || (i = this.aggregatedData[e][t] = {
            params: r || {}
          }, n && (i.custom = n)), i;
        }
        get(e, t) {
          return t ? this.aggregatedData[e] && this.aggregatedData[e][t] : this.aggregatedData[e];
        }
        take(e) {
          for (var t = {}, r = "", n = !1, i = 0; i < e.length; i++) t[r = e[i]] = x(this.aggregatedData[r]), t[r].length && (n = !0), delete this.aggregatedData[r];
          return n ? t : null;
        }
      }
      function w(e, t) {
        return null == e ? function (e) {
          e ? e.c++ : e = {
            c: 1
          };
          return e;
        }(t) : t ? (t.c || (t = A(t.t)), t.c += 1, t.t += e, t.sos += e * e, e > t.max && (t.max = e), e < t.min && (t.min = e), t) : {
          t: e
        };
      }
      function A(e) {
        return {
          t: e,
          min: e,
          max: e,
          sos: e * e,
          c: 1
        };
      }
      function x(e) {
        return "object" != typeof e ? [] : (0, b.D)(e, E);
      }
      function E(e, t) {
        return t;
      }
      var T = i(8632),
        _ = i(4402),
        D = i(4351);
      var j = i(7956),
        C = i(3239),
        N = i(9251);
      class O extends h {
        static featureName = N.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, N.t, r), u.il && ((0, n.OP)(e).initHidden = Boolean("hidden" === document.visibilityState), (0, j.N)(() => (0, c.p)("docHidden", [(0, g.z)()], void 0, N.t, this.ee), !0), (0, C.bP)("pagehide", () => (0, c.p)("winPagehide", [(0, g.z)()], void 0, N.t, this.ee)), this.importAggregator());
        }
      }
      var S = i(3081);
      class P extends h {
        static featureName = S.t9;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, S.t9, r), this.importAggregator();
        }
      }
      var R = i(6660);
      class I {
        constructor(e, t, r, n) {
          this.name = "UncaughtError", this.message = e, this.sourceURL = t, this.line = r, this.column = n;
        }
      }
      class k extends h {
        static featureName = R.t;
        #e = new Set();
        constructor(e, t) {
          let n = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, R.t, n);
          try {
            this.removeOnAbort = new AbortController();
          } catch (e) {}
          this.ee.on("fn-err", (e, t, n) => {
            this.abortHandler && !this.#e.has(n) && (this.#e.add(n), (0, c.p)("err", [this.#t(n), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }), this.ee.on("internal-error", e => {
            this.abortHandler && (0, c.p)("ierr", [this.#t(e), (0, g.z)(), !0], void 0, r.D.jserrors, this.ee);
          }), u._A.addEventListener("unhandledrejection", e => {
            this.abortHandler && (0, c.p)("err", [this.#r(e), (0, g.z)(), !1, {
              unhandledPromiseRejection: 1
            }], void 0, r.D.jserrors, this.ee);
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), u._A.addEventListener("error", e => {
            this.abortHandler && (this.#e.has(e.error) ? this.#e.delete(e.error) : (0, c.p)("err", [this.#n(e), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
        }
        #i() {
          this.removeOnAbort?.abort(), this.#e.clear(), this.abortHandler = void 0;
        }
        #t(e) {
          return e instanceof Error ? e : void 0 !== e?.message ? new I(e.message, e.filename || e.sourceURL, e.lineno || e.line, e.colno || e.col) : new I("string" == typeof e ? e : (0, D.P)(e));
        }
        #r(e) {
          let t = "Unhandled Promise Rejection: ";
          if (e?.reason instanceof Error) try {
            return e.reason.message = t + e.reason.message, e.reason;
          } catch (t) {
            return e.reason;
          }
          if (void 0 === e.reason) return new I(t);
          const r = this.#t(e.reason);
          return r.message = t + r.message, r;
        }
        #n(e) {
          return e.error instanceof Error ? e.error : new I(e.message, e.filename, e.lineno, e.colno);
        }
      }
      var H = i(2210);
      let z = 1;
      const L = "nr@id";
      function M(e) {
        const t = typeof e;
        return !e || "object" !== t && "function" !== t ? -1 : e === u._A ? 0 : (0, H.X)(e, L, function () {
          return z++;
        });
      }
      function B(e) {
        if ("string" == typeof e && e.length) return e.length;
        if ("object" == typeof e) {
          if ("undefined" != typeof ArrayBuffer && e instanceof ArrayBuffer && e.byteLength) return e.byteLength;
          if ("undefined" != typeof Blob && e instanceof Blob && e.size) return e.size;
          if (!("undefined" != typeof FormData && e instanceof FormData)) try {
            return (0, D.P)(e).length;
          } catch (e) {
            return;
          }
        }
      }
      var F = i(1214),
        U = i(7243);
      class q {
        constructor(e) {
          this.agentIdentifier = e;
        }
        generateTracePayload(e) {
          if (!this.shouldGenerateTrace(e)) return null;
          var t = (0, n.DL)(this.agentIdentifier);
          if (!t) return null;
          var r = (t.accountID || "").toString() || null,
            i = (t.agentID || "").toString() || null,
            o = (t.trustKey || "").toString() || null;
          if (!r || !i) return null;
          var a = (0, _.M)(),
            s = (0, _.Ht)(),
            c = Date.now(),
            u = {
              spanId: a,
              traceId: s,
              timestamp: c
            };
          return (e.sameOrigin || this.isAllowedOrigin(e) && this.useTraceContextHeadersForCors()) && (u.traceContextParentHeader = this.generateTraceContextParentHeader(a, s), u.traceContextStateHeader = this.generateTraceContextStateHeader(a, c, r, i, o)), (e.sameOrigin && !this.excludeNewrelicHeader() || !e.sameOrigin && this.isAllowedOrigin(e) && this.useNewrelicHeaderForCors()) && (u.newrelicHeader = this.generateTraceHeader(a, s, c, r, i, o)), u;
        }
        generateTraceContextParentHeader(e, t) {
          return "00-" + t + "-" + e + "-01";
        }
        generateTraceContextStateHeader(e, t, r, n, i) {
          return i + "@nr=0-1-" + r + "-" + n + "-" + e + "----" + t;
        }
        generateTraceHeader(e, t, r, n, i, o) {
          if (!("function" == typeof u._A?.btoa)) return null;
          var a = {
            v: [0, 1],
            d: {
              ty: "Browser",
              ac: n,
              ap: i,
              id: e,
              tr: t,
              ti: r
            }
          };
          return o && n !== o && (a.d.tk = o), btoa((0, D.P)(a));
        }
        shouldGenerateTrace(e) {
          return this.isDtEnabled() && this.isAllowedOrigin(e);
        }
        isAllowedOrigin(e) {
          var t = !1,
            r = {};
          if ((0, n.Mt)(this.agentIdentifier, "distributed_tracing") && (r = (0, n.P_)(this.agentIdentifier).distributed_tracing), e.sameOrigin) t = !0;else if (r.allowed_origins instanceof Array) for (var i = 0; i < r.allowed_origins.length; i++) {
            var o = (0, U.e)(r.allowed_origins[i]);
            if (e.hostname === o.hostname && e.protocol === o.protocol && e.port === o.port) {
              t = !0;
              break;
            }
          }
          return t;
        }
        isDtEnabled() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.enabled;
        }
        excludeNewrelicHeader() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.exclude_newrelic_header;
        }
        useNewrelicHeaderForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !1 !== e.cors_use_newrelic_header;
        }
        useTraceContextHeadersForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.cors_use_tracecontext_headers;
        }
      }
      var Z = i(7825),
        V = ["load", "error", "abort", "timeout"],
        G = V.length,
        W = n.Yu.REQ,
        X = n.Yu.XHR;
      class Q extends h {
        static featureName = Z.t;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, Z.t, i), (0, n.OP)(e).xhrWrappable && (this.dt = new q(e), this.handler = (e, t, r, n) => (0, c.p)(e, t, r, n, this.ee), (0, F.u5)(this.ee), (0, F.Kf)(this.ee), function (e, t, i, o) {
            function a(e) {
              var t = this;
              t.totalCbs = 0, t.called = 0, t.cbTime = 0, t.end = E, t.ended = !1, t.xhrGuids = {}, t.lastSize = null, t.loadCaptureCalled = !1, t.params = this.params || {}, t.metrics = this.metrics || {}, e.addEventListener("load", function (r) {
                _(t, e);
              }, (0, C.m$)(!1)), u.IF || e.addEventListener("progress", function (e) {
                t.lastSize = e.loaded;
              }, (0, C.m$)(!1));
            }
            function s(e) {
              this.params = {
                method: e[0]
              }, T(this, e[1]), this.metrics = {};
            }
            function c(t, r) {
              var i = (0, n.DL)(e);
              i.xpid && this.sameOrigin && r.setRequestHeader("X-NewRelic-ID", i.xpid);
              var a = o.generateTracePayload(this.parsedOrigin);
              if (a) {
                var s = !1;
                a.newrelicHeader && (r.setRequestHeader("newrelic", a.newrelicHeader), s = !0), a.traceContextParentHeader && (r.setRequestHeader("traceparent", a.traceContextParentHeader), a.traceContextStateHeader && r.setRequestHeader("tracestate", a.traceContextStateHeader), s = !0), s && (this.dt = a);
              }
            }
            function d(e, r) {
              var n = this.metrics,
                i = e[0],
                o = this;
              if (n && i) {
                var a = B(i);
                a && (n.txSize = a);
              }
              this.startTime = (0, g.z)(), this.listener = function (e) {
                try {
                  "abort" !== e.type || o.loadCaptureCalled || (o.params.aborted = !0), ("load" !== e.type || o.called === o.totalCbs && (o.onloadCalled || "function" != typeof r.onload) && "function" == typeof o.end) && o.end(r);
                } catch (e) {
                  try {
                    t.emit("internal-error", [e]);
                  } catch (e) {}
                }
              };
              for (var s = 0; s < G; s++) r.addEventListener(V[s], this.listener, (0, C.m$)(!1));
            }
            function l(e, t, r) {
              this.cbTime += e, t ? this.onloadCalled = !0 : this.called += 1, this.called !== this.totalCbs || !this.onloadCalled && "function" == typeof r.onload || "function" != typeof this.end || this.end(r);
            }
            function f(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && !this.xhrGuids[r] && (this.xhrGuids[r] = !0, this.totalCbs += 1);
            }
            function h(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && this.xhrGuids[r] && (delete this.xhrGuids[r], this.totalCbs -= 1);
            }
            function p() {
              this.endTime = (0, g.z)();
            }
            function m(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-added", [e[1], e[2]], r);
            }
            function v(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-removed", [e[1], e[2]], r);
            }
            function b(e, t, r) {
              t instanceof X && ("onload" === r && (this.onload = !0), ("load" === (e[0] && e[0].type) || this.onload) && (this.xhrCbStart = (0, g.z)()));
            }
            function y(e, r) {
              this.xhrCbStart && t.emit("xhr-cb-time", [(0, g.z)() - this.xhrCbStart, this.onload, r], r);
            }
            function w(e) {
              var t,
                r = e[1] || {};
              if ("string" == typeof e[0] ? 0 === (t = e[0]).length && u.il && (t = "" + u._A.location.href) : e[0] && e[0].url ? t = e[0].url : u._A?.URL && e[0] && e[0] instanceof URL ? t = e[0].href : "function" == typeof e[0].toString && (t = e[0].toString()), "string" == typeof t && 0 !== t.length) {
                t && (this.parsedOrigin = (0, U.e)(t), this.sameOrigin = this.parsedOrigin.sameOrigin);
                var n = o.generateTracePayload(this.parsedOrigin);
                if (n && (n.newrelicHeader || n.traceContextParentHeader)) if (e[0] && e[0].headers) s(e[0].headers, n) && (this.dt = n);else {
                  var i = {};
                  for (var a in r) i[a] = r[a];
                  i.headers = new Headers(r.headers || {}), s(i.headers, n) && (this.dt = n), e.length > 1 ? e[1] = i : e.push(i);
                }
              }
              function s(e, t) {
                var r = !1;
                return t.newrelicHeader && (e.set("newrelic", t.newrelicHeader), r = !0), t.traceContextParentHeader && (e.set("traceparent", t.traceContextParentHeader), t.traceContextStateHeader && e.set("tracestate", t.traceContextStateHeader), r = !0), r;
              }
            }
            function A(e, t) {
              this.params = {}, this.metrics = {}, this.startTime = (0, g.z)(), this.dt = t, e.length >= 1 && (this.target = e[0]), e.length >= 2 && (this.opts = e[1]);
              var r,
                n = this.opts || {},
                i = this.target;
              "string" == typeof i ? r = i : "object" == typeof i && i instanceof W ? r = i.url : u._A?.URL && "object" == typeof i && i instanceof URL && (r = i.href), T(this, r);
              var o = ("" + (i && i instanceof W && i.method || n.method || "GET")).toUpperCase();
              this.params.method = o, this.txSize = B(n.body) || 0;
            }
            function x(e, t) {
              var n;
              this.endTime = (0, g.z)(), this.params || (this.params = {}), this.params.status = t ? t.status : 0, "string" == typeof this.rxSize && this.rxSize.length > 0 && (n = +this.rxSize);
              var o = {
                txSize: this.txSize,
                rxSize: n,
                duration: (0, g.z)() - this.startTime
              };
              i("xhr", [this.params, o, this.startTime, this.endTime, "fetch"], this, r.D.ajax);
            }
            function E(e) {
              var t = this.params,
                n = this.metrics;
              if (!this.ended) {
                this.ended = !0;
                for (var o = 0; o < G; o++) e.removeEventListener(V[o], this.listener, !1);
                t.aborted || (n.duration = (0, g.z)() - this.startTime, this.loadCaptureCalled || 4 !== e.readyState ? null == t.status && (t.status = 0) : _(this, e), n.cbTime = this.cbTime, i("xhr", [t, n, this.startTime, this.endTime, "xhr"], this, r.D.ajax));
              }
            }
            function T(e, t) {
              var r = (0, U.e)(t),
                n = e.params;
              n.hostname = r.hostname, n.port = r.port, n.protocol = r.protocol, n.host = r.hostname + ":" + r.port, n.pathname = r.pathname, e.parsedOrigin = r, e.sameOrigin = r.sameOrigin;
            }
            function _(e, t) {
              e.params.status = t.status;
              var r = function (e, t) {
                var r = e.responseType;
                return "json" === r && null !== t ? t : "arraybuffer" === r || "blob" === r || "json" === r ? B(e.response) : "text" === r || "" === r || void 0 === r ? B(e.responseText) : void 0;
              }(t, e.lastSize);
              if (r && (e.metrics.rxSize = r), e.sameOrigin) {
                var n = t.getResponseHeader("X-NewRelic-App-Data");
                n && (e.params.cat = n.split(", ").pop());
              }
              e.loadCaptureCalled = !0;
            }
            t.on("new-xhr", a), t.on("open-xhr-start", s), t.on("open-xhr-end", c), t.on("send-xhr-start", d), t.on("xhr-cb-time", l), t.on("xhr-load-added", f), t.on("xhr-load-removed", h), t.on("xhr-resolved", p), t.on("addEventListener-end", m), t.on("removeEventListener-end", v), t.on("fn-end", y), t.on("fetch-before-start", w), t.on("fetch-start", A), t.on("fn-start", b), t.on("fetch-done", x);
          }(e, this.ee, this.handler, this.dt), this.importAggregator());
        }
      }
      var K = i(3614);
      const {
        BST_RESOURCE: Y,
        RESOURCE: J,
        START: ee,
        END: te,
        FEATURE_NAME: re,
        FN_END: ne,
        FN_START: ie,
        PUSH_STATE: oe
      } = K;
      var ae = i(7836);
      const {
        FEATURE_NAME: se,
        START: ce,
        END: ue,
        BODY: de,
        CB_END: le,
        JS_TIME: fe,
        FETCH: he,
        FN_START: pe,
        CB_START: ge,
        FN_END: me
      } = ae;
      var ve = i(4649);
      class be extends h {
        static featureName = ve.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, ve.t, r), this.importAggregator();
        }
      }
      new class extends t {
        constructor(t) {
          let r = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : (0, _.ky)(16);
          super(), u._A ? (this.agentIdentifier = r, this.sharedAggregator = new y({
            agentIdentifier: this.agentIdentifier
          }), this.features = {}, this.desiredFeatures = new Set(t.features || []), this.desiredFeatures.add(m), Object.assign(this, (0, s.j)(this.agentIdentifier, t, t.loaderType || "agent")), this.start()) : (0, e.Z)("Failed to initial the agent. Could not determine the runtime environment.");
        }
        get config() {
          return {
            info: (0, n.C5)(this.agentIdentifier),
            init: (0, n.P_)(this.agentIdentifier),
            loader_config: (0, n.DL)(this.agentIdentifier),
            runtime: (0, n.OP)(this.agentIdentifier)
          };
        }
        start() {
          const t = "features";
          try {
            const n = a(this.agentIdentifier),
              i = [...this.desiredFeatures];
            i.sort((e, t) => r.p[e.featureName] - r.p[t.featureName]), i.forEach(t => {
              if (n[t.featureName] || t.featureName === r.D.pageViewEvent) {
                const i = function (e) {
                  switch (e) {
                    case r.D.ajax:
                      return [r.D.jserrors];
                    case r.D.sessionTrace:
                      return [r.D.ajax, r.D.pageViewEvent];
                    case r.D.sessionReplay:
                      return [r.D.sessionTrace];
                    case r.D.pageViewTiming:
                      return [r.D.pageViewEvent];
                    default:
                      return [];
                  }
                }(t.featureName);
                i.every(e => n[e]) || (0, e.Z)("".concat(t.featureName, " is enabled but one or more dependent features has been disabled (").concat((0, D.P)(i), "). This may cause unintended consequences or missing data...")), this.features[t.featureName] = new t(this.agentIdentifier, this.sharedAggregator);
              }
            }), (0, T.Qy)(this.agentIdentifier, this.features, t);
          } catch (r) {
            (0, e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -", r);
            for (const e in this.features) this.features[e].abortHandler?.();
            const n = (0, T.fP)();
            return delete n.initializedAgents[this.agentIdentifier]?.api, delete n.initializedAgents[this.agentIdentifier]?.[t], delete this.sharedAggregator, n.ee?.abort(), delete n.ee?.get(this.agentIdentifier), !1;
          }
        }
        addToTrace(t) {
          (0, e.Z)("Call to agent api addToTrace failed. The page action feature is not currently initialized.");
        }
        setCurrentRouteName(t) {
          (0, e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.");
        }
        interaction() {
          (0, e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.");
        }
      }({
        features: [Q, m, O, class extends h {
          static featureName = re;
          constructor(e, t) {
            if (super(e, t, re, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            const n = this.ee;
            let i;
            (0, F.QU)(n), this.eventsEE = (0, F.em)(n), this.eventsEE.on(ie, function (e, t) {
              this.bstStart = (0, g.z)();
            }), this.eventsEE.on(ne, function (e, t) {
              (0, c.p)("bst", [e[0], t, this.bstStart, (0, g.z)()], void 0, r.D.sessionTrace, n);
            }), n.on(oe + ee, function (e) {
              this.time = (0, g.z)(), this.startPath = location.pathname + location.hash;
            }), n.on(oe + te, function (e) {
              (0, c.p)("bstHist", [location.pathname + location.hash, this.startPath, this.time], void 0, r.D.sessionTrace, n);
            });
            try {
              i = new PerformanceObserver(e => {
                const t = e.getEntries();
                (0, c.p)(Y, [t], void 0, r.D.sessionTrace, n);
              }), i.observe({
                type: J,
                buffered: !0
              });
            } catch (e) {}
            this.importAggregator({
              resourceObserver: i
            });
          }
        }, P, be, k, class extends h {
          static featureName = se;
          constructor(e, t) {
            if (super(e, t, se, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            if (!(0, n.OP)(e).xhrWrappable) return;
            try {
              this.removeOnAbort = new AbortController();
            } catch (e) {}
            let r,
              i = 0;
            const o = this.ee.get("tracer"),
              a = (0, F._L)(this.ee),
              s = (0, F.Lg)(this.ee),
              c = (0, F.BV)(this.ee),
              d = (0, F.Kf)(this.ee),
              l = this.ee.get("events"),
              f = (0, F.u5)(this.ee),
              h = (0, F.QU)(this.ee),
              p = (0, F.Gm)(this.ee);
            function m(e, t) {
              h.emit("newURL", ["" + window.location, t]);
            }
            function v() {
              i++, r = window.location.hash, this[pe] = (0, g.z)();
            }
            function b() {
              i--, window.location.hash !== r && m(0, !0);
              var e = (0, g.z)();
              this[fe] = ~~this[fe] + e - this[pe], this[me] = e;
            }
            function y(e, t) {
              e.on(t, function () {
                this[t] = (0, g.z)();
              });
            }
            this.ee.on(pe, v), s.on(ge, v), a.on(ge, v), this.ee.on(me, b), s.on(le, b), a.on(le, b), this.ee.buffer([pe, me, "xhr-resolved"], this.featureName), l.buffer([pe], this.featureName), c.buffer(["setTimeout" + ue, "clearTimeout" + ce, pe], this.featureName), d.buffer([pe, "new-xhr", "send-xhr" + ce], this.featureName), f.buffer([he + ce, he + "-done", he + de + ce, he + de + ue], this.featureName), h.buffer(["newURL"], this.featureName), p.buffer([pe], this.featureName), s.buffer(["propagate", ge, le, "executor-err", "resolve" + ce], this.featureName), o.buffer([pe, "no-" + pe], this.featureName), a.buffer(["new-jsonp", "cb-start", "jsonp-error", "jsonp-end"], this.featureName), y(f, he + ce), y(f, he + "-done"), y(a, "new-jsonp"), y(a, "jsonp-end"), y(a, "cb-start"), h.on("pushState-end", m), h.on("replaceState-end", m), window.addEventListener("hashchange", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("load", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("popstate", function () {
              m(0, i > 1);
            }, (0, C.m$)(!0, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
          }
          #i() {
            this.removeOnAbort?.abort(), this.abortHandler = void 0;
          }
        }],
        loaderType: "spa"
      });
    })();
  })();
})()</script>
      <link rel='shortcut icon' href='https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico' type='image/x-icon' />
      <link rel='icon' href='https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico' type='image/x-icon'>
      <link rel='stylesheet' href='https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/e444bc27e06cd87e0d0139060246bdab684588aa/arp.css'>
      <link href="//cdn.pendo.io" rel="dns-prefetch" />
      <link href="https://cdn.pendo.io" rel="preconnect" crossorigin="anonymous" />
      <link rel='dns-prefetch' href='https://smetrics.elsevier.com'>
      
      <script type="41105e43d13887f67dd11be4-text/javascript">
        var targetServerState = JSON.stringify({"4D6368F454EC41940A4C98A6@AdobeOrg":{"sdid":{"supplementalDataIDCurrent":"2444B4EC0A7EFEF6-51BA36D336639FA5","supplementalDataIDCurrentConsumed":{"payload:target-global-mbox":true},"supplementalDataIDLastConsumed":{}}}});
        window.appData = window.appData || [];
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":false,"crawler":"","journal":"Neurocomputing","auth":"AE"};
        window.arp = {
          config: {"adobeSuite":"elsevier-sd-prod","arsUrl":"https://ars.els-cdn.com","recommendationsFeedback":{"enabled":true,"url":"https://feedback.recs.d.elsevier.com/raw/events","timeout":60000},"googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","strictMode":false,"seamlessAccess":{"enableSeamlessAccess":true,"scriptUrl":"https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js","persistenceUrl":"https://service.seamlessaccess.org/ps/","persistenceContext":"seamlessaccess.org","scienceDirectUrl":"https://www.sciencedirect.com","shibAuthUrl":"https://auth.elsevier.com/ShibAuth/institutionLogin"},"reaxys":{"apiUrl":"https://reaxys-sdlc.reaxys.com","origin":"sciencedirect","queryBuilderHostPath":"https://www.reaxys.com/reaxys/secured/hopinto.do","url":"https://www.reaxys.com"},"oneTrustCookie":{"enabled":true},"ssrn":{"url":"https://papers.ssrn.com","path":"/sol3/papers.cfm"},"plumX":"https://api.plu.mx/widget/elsevier/artifact","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/e444bc27e06cd87e0d0139060246bdab684588aa"},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.addEventListener('beforeprint', () => pendo.onGuideDismissed());
      </script>
    </head>
    <body data-sd-ui-layer-boundary="true">
      <script type="41105e43d13887f67dd11be4-text/javascript">
        window.__PRELOADED_STATE__ = {"abstracts":{"content":[{"$$":[{"$":{"id":"d1e4531"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"d1e4534"},"#name":"simple-para","$$":[{"#name":"text","$$":[{"#name":"__text__","_":"In recent years, multi-agent "},{"#name":"topic-link","_":"reinforcement learning techniques","$":{"href":"/topics/engineering/reinforcement-learning-technique","term":"reinforcement learning techniques"}},{"#name":"__text__","_":" have been widely used and evolved in the field of "}]},{"#name":"topic-link","_":"artificial intelligence","$":{"href":"/topics/computer-science/artificial-intelligence","term":"artificial intelligence"}},{"#name":"text","$$":[{"#name":"__text__","_":". However, traditional "},{"#name":"topic-link","_":"reinforcement learning","$":{"href":"/topics/computer-science/reinforcement-learning","term":"reinforcement learning"}},{"#name":"__text__","_":" methods have limitations such as long training time, large sample data requirements, and highly delayed rewards. Therefore, this paper systematically and specifically studies the MARL algorithm. Firstly, this paper uses Citespace software to visually analyze the existing literature on multi-agent reinforcement learning and briefly indicates the research hotspots and key research directions in this field. Secondly, the applications of traditional reinforcement learning algorithms under two task objects, namely single-agent and multi-agent systems, are described in detail. Then, the paper highlights the diverse applications, challenges, and corresponding solutions of MARL algorithmic techniques in the field of MAS. Finally, the paper points out future research directions based on the existing limitations of the algorithm. Through this paper, readers will gain a systematic and in-depth understanding of MARL algorithms and how they can be utilized to better address the various challenges posed by MAS."}]}]}],"$":{"view":"all","id":"d1e4533"},"#name":"abstract-sec"}],"$":{"view":"all","id":"d1e4530","class":"author"},"#name":"abstract"}],"floats":[],"footnotes":[],"attachments":[]},"accessbarConfig":{"fallback":false,"id":"accessbar","version":"0.0.1","analytics":{"location":"accessbar","eventName":"ctaImpression"},"ariaLabel":{"accessbar":"Download options and search","componentsList":"PDF Options"},"banners":[{"id":"BannerSsrn"}],"components":[{"analytics":[{"ids":["accessbar:clinicalkeycta:inst-known:sdcust-unknown:ent-no:ra-no:source-linkinghub:method-ip"],"eventName":"ctaClick"}],"label":"Access through&nbsp;**ClinicalKey**","id":"ClinicalKey"},{"ariaLabel":"Access through your organization","institutionLogoAltText":"Seamless access","analytics":[{"ids":["accessbar:accesscta:inst-unknown:sdcust-unknown:ent-unknown:ra-yes:source-seamlessaccess:method-shib"],"eventName":"ctaClick"}],"labelPrefix":"Access through","defaultLabelSuffix":"your organization","href":"/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0925231224008397","id":"RemoteAccess"},{"analytics":[{"ids":["accessbar:purchase-pdf"],"eventName":"ctaClick"}],"label":"Purchase PDF","href":"/getaccess/pii/S0925231224008397/purchase","rel":"noreferrer noopener","target":"_blank","id":"PurchasePDF"},{"analytics":[{"ids":["accessbar:another-institution"],"eventName":"ctaClick"}],"label":"Access through another organization","href":"/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0925231224008397","id":"RemoteAccessOther"},{"id":"ViewClinicalKeyFullText","analytics":[{"eventName":"ctaClick","ids":["accessbar:fta:clinical-key"]}],"label":"View full text","rel":"noopener noreferrer","target":"_blank","href":"https://www.clinicalkey.com/#!/content/journal/1-s2.0-S0925231224008397"}],"search":{"inputPlaceHolder":"Search ScienceDirect","ariaLabel":{"input":"Search ScienceDirect","submit":"Submit search"},"formAction":"/search#submit","analytics":[{"ids":["accessbar:search"],"eventName":"searchStart"}],"id":"QuickSearch"}},"adobeTarget":{"sd:genai-question-and-answer":{}},"article":{"accessOptions":{},"analyticsMetadata":{"accountId":"228598","accountName":"ScienceDirect Guests","loginStatus":"anonymous","userId":"12975512","isLoggedIn":false},"article-number":"128068","cid":"271597","content-family":"serial","copyright-line":"Â© 2024 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.","cover-date-years":["2024"],"cover-date-start":"2024-09-28","cover-date-text":"28 September 2024","document-subtype":"ssu","document-type":"article","eid":"1-s2.0-S0925231224008397","doi":"10.1016/j.neucom.2024.128068","first-fp":"128068","hub-eid":"1-s2.0-S0925231224X00296","issuePii":"S0925231224X00296","item-weight":"FULL-TEXT","language":"en","last-author":{"#name":"last-author","$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"$$":[{"#name":"author","$":{"author-id":"S0925231224008397-3036e1b95e5e1be1474a6b4bc5925629","biographyid":"bio8","id":"au000008","orcid":"0000-0003-4681-9129"},"$$":[{"#name":"given-name","_":"Min"},{"#name":"surname","_":"Xia"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/resources"},"_":"Resources"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"cross-ref","$":{"id":"d1e4465","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTglMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBeGlhbWluJTQwbnVpc3QuZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMnhpYW1pbiU0MG51aXN0LmVkdS5jbiUyMiU3RA=="}]}]},"normalized-first-auth-initial":"K","normalized-first-auth-surname":"HU","open-research":{"#name":"open-research","$":{"xmlns:xocs":true},"$$":[{"#name":"or-embargo-opening-date","_":"2026-06-28T00:00:00.000Z"}]},"pages":[{"first-page":"128068"}],"pii":"S0925231224008397","self-archiving":{"#name":"self-archiving","$":{"xmlns:xocs":true},"$$":[{"#name":"sa-start-date","_":"2026-06-28T00:00:00.000Z"},{"#name":"sa-user-license","_":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}]},"srctitle":"Neurocomputing","suppl":"C","timestamp":"2024-08-10T18:30:11.772491Z","title":{"content":[{"#name":"dochead","$":{"id":"d1e4313"},"$$":[{"#name":"textfn","_":"Survey Paper"}]},{"#name":"title","$":{"id":"d1e4316"},"_":"A review of research on reinforcement learning algorithms for multi-agents"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"599","vol-iss-suppl-text":"Volume 599","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"document-references":120,"freeHtmlGiven":false,"ssoUrls":["//acw.clinicalkey.com/SSOCore/update?acw=abd5e8df27df7944d579e0439fa50121e935gxrqb%7C%24%7C2B0B896E4E77701C2DB83658ABDCE5FF1F9C807A2A02798BC822C60BA7153B232FDF8648AE69D34953859A3645904E8AF4347650A1DE78E90E9169905BBD791CB0469A67597464825D387A21AFA2E514&utt=3a44-b8d18eb5491eea985a6abf2cb7b405d0f9-KTn","//acw.scopus.com/SSOCore/update?acw=abd5e8df27df7944d579e0439fa50121e935gxrqb%7C%24%7C2B0B896E4E77701C2DB83658ABDCE5FF1F9C807A2A02798BC822C60BA7153B232FDF8648AE69D34953859A3645904E8AF4347650A1DE78E90E9169905BBD791CB0469A67597464825D387A21AFA2E514&utt=3a44-b8d18eb5491eea985a6abf2cb7b405d0f9-KTn","//acw.sciencedirect.com/SSOCore/update?acw=abd5e8df27df7944d579e0439fa50121e935gxrqb%7C%24%7C2B0B896E4E77701C2DB83658ABDCE5FF1F9C807A2A02798BC822C60BA7153B232FDF8648AE69D34953859A3645904E8AF4347650A1DE78E90E9169905BBD791CB0469A67597464825D387A21AFA2E514&utt=3a44-b8d18eb5491eea985a6abf2cb7b405d0f9-KTn","//acw.elsevier.com/SSOCore/update?acw=abd5e8df27df7944d579e0439fa50121e935gxrqb%7C%24%7C2B0B896E4E77701C2DB83658ABDCE5FF1F9C807A2A02798BC822C60BA7153B232FDF8648AE69D34953859A3645904E8AF4347650A1DE78E90E9169905BBD791CB0469A67597464825D387A21AFA2E514&utt=3a44-b8d18eb5491eea985a6abf2cb7b405d0f9-KTn"],"userProfile":{"departmentName":"ScienceDirect Guests","accessType":"GUEST","accountId":"228598","webUserId":"12975512","accountName":"ScienceDirect Guests","departmentId":"291352","userType":"NORMAL","hasMultipleOrganizations":false,"accountNumber":"C000228598"},"access":{"openAccess":false,"openArchive":false},"aipType":"none","articleEntitlement":{"entitled":false,"isCasaUser":false,"usageInfo":"(12975512,U|291352,D|228598,A|3,P|2,PL)(SDFE,CON|abd5e8df27df7944d579e0439fa50121e935gxrqb,SSO|ANON_GUEST,ACCESS_TYPE)","entitledByAccount":false,"entitlementOrigin":"SD"},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"dates":{"Available online":"18 June 2024","Received":"1 November 2023","Revised":["11 March 2024"],"Accepted":"12 June 2024","Publication date":"28 September 2024","Version of Record":"28 June 2024"},"downloadFullIssue":false,"entitlementReason":"unsubscribed","features":["aamAttachments","keywords","data-availability","references","biography","preview"],"hasBody":true,"has-large-authors":false,"hasScholarlyAbstract":true,"headerConfig":{"contactUrl":"https://service.elsevier.com/app/contact/supporthub/sciencedirect/","userName":"","userEmail":"","orgName":"ScienceDirect Guests","webUserId":"12975512","libraryBanner":null,"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasInstitutionalAssociation":false,"hasMultiOrg":false,"userType":"GUEST","userAnonymity":"ANON_GUEST","allowCart":true,"environment":"prod","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com"},"isCorpReq":false,"isPdfFullText":false,"issn":"09252312","issn-primary-formatted":"0925-2312","issRange":"","isThirdParty":false,"pageCount":33,"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Elsevier","id":"47"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"","oaArticleCount":583,"openArchiveStatus":false,"openArchiveArticleCount":19,"openAccessStartDate":"","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S0925231224X00296-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224X00296/cover/DOWNSAMPLED200/image/gif/fb942912d7a16568fe9d5b5d1cee69e2/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"3707","pixel-height":"200","pixel-width":"150"},{"attachment-eid":"1-s2.0-S0925231224X00296-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224X00296/cover/DOWNSAMPLED150/image/gif/1bdb488f8799cda2c86d611254a200c5/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"2656","pixel-height":"150","pixel-width":"113"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S09252312.gif","title":"neurocomputing","contentTypeCode":"JL","images":{"coverImage":"https://ars.els-cdn.com/content/image/1-s2.0-S0925231224X00296-cov150h.gif","logo":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/e444bc27e06cd87e0d0139060246bdab684588aa/image/elsevier-non-solus.png","logoAltText":"Elsevier"},"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S0925231224X00296-cov150h.gif"},"volRange":"599","titleString":"A review of research on reinforcement learning algorithms for multi-agents","ssrn":{},"renderingMode":"Preview","isAbstract":true,"isContentVisible":false,"ajaxLinks":{"referredToBy":true,"authorMetadata":true},"pdfEmbed":false,"displayViewFullText":false},"authors":{"content":[{"#name":"author-group","$":{"id":"d1e4319"},"$$":[{"#name":"author","$":{"author-id":"S0925231224008397-b644b39f6b9091c26fb9e8ca0e1a04e4","biographyid":"bio1","id":"au000001","orcid":"0000-0001-7181-9935"},"$$":[{"#name":"given-name","_":"Kai"},{"#name":"surname","_":"Hu"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"cross-ref","$":{"id":"d1e4333","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"d1e4336","refid":"aff2"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTElMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBMDAxNjAwJTQwbnVpc3QuZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMjAwMTYwMCU0MG51aXN0LmVkdS5jbiUyMiU3RA=="}]},{"#name":"author","$":{"author-id":"S0925231224008397-ba5f50bb32788079a5964bc44b5213f6","biographyid":"bio2","id":"au000002","orcid":"0000-0002-4879-6437"},"$$":[{"#name":"given-name","_":"Mingyang"},{"#name":"surname","_":"Li"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/data-curation"},"_":"Data curation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"cross-ref","$":{"id":"d1e4354","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTIlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBMjAyMjEyNDkwNTA3JTQwbnVpc3QuZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMjIwMjIxMjQ5MDUwNyU0MG51aXN0LmVkdS5jbiUyMiU3RA=="}]},{"#name":"author","$":{"author-id":"S0925231224008397-acbf9680bdaaf958fcf1fdc3f66de77c","biographyid":"bio3","id":"au000003","orcid":"0009-0000-5120-3447"},"$$":[{"#name":"given-name","_":"Zhiqiang"},{"#name":"surname","_":"Song"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"cross-ref","$":{"id":"d1e4372","refid":"aff3"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"cross-ref","$":{"id":"d1e4375","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTMlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBenFzb25nJTQwY3d4dS5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIyenFzb25nJTQwY3d4dS5lZHUuY24lMjIlN0Q="}]},{"#name":"author","$":{"author-id":"S0925231224008397-7f0b12b37a60baef26ece5a17d50a370","biographyid":"bio4","id":"au000004","orcid":"0000-0003-0799-483X"},"$$":[{"#name":"given-name","_":"Keer"},{"#name":"surname","_":"Xu"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/data-curation"},"_":"Data curation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/visualization"},"_":"Visualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"cross-ref","$":{"id":"d1e4393","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTQlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBMjAyMjEyNDkwNTAyJTQwbnVpc3QuZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMjIwMjIxMjQ5MDUwMiU0MG51aXN0LmVkdS5jbiUyMiU3RA=="}]},{"#name":"author","$":{"author-id":"S0925231224008397-735b5a51b951ef71d61b96937a133d94","biographyid":"bio5","id":"au000005","orcid":"0000-0003-0829-6750"},"$$":[{"#name":"given-name","_":"Qingfeng"},{"#name":"surname","_":"Xia"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/project-administration"},"_":"Project administration"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"cross-ref","$":{"id":"d1e4411","refid":"aff3"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTUlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBeHFmJTQwY3d4dS5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIyeHFmJTQwY3d4dS5lZHUuY24lMjIlN0Q="}]},{"#name":"author","$":{"author-id":"S0925231224008397-a171207aab44d338ff492bf69ceb2af0","biographyid":"bio6","id":"au000006","orcid":"0000-0002-0280-5983"},"$$":[{"#name":"given-name","_":"Ning"},{"#name":"surname","_":"Sun"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"cross-ref","$":{"id":"d1e4427","refid":"aff3"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTYlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBMDAxNzY0JTQwY3d4dS5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIyMDAxNzY0JTQwY3d4dS5lZHUuY24lMjIlN0Q="}]},{"#name":"author","$":{"author-id":"S0925231224008397-c739d9e8dfb69ffb0dd81a570d45accf","biographyid":"bio7","id":"au000007","orcid":"0009-0002-1914-1666"},"$$":[{"#name":"given-name","_":"Peng"},{"#name":"surname","_":"Zhou"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/data-curation"},"_":"Data curation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/visualization"},"_":"Visualization"},{"#name":"cross-ref","$":{"id":"d1e4449","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTclMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBMjAyMzEyNDkwMzc4JTQwbnVpc3QuZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMjIwMjMxMjQ5MDM3OCU0MG51aXN0LmVkdS5jbiUyMiU3RA=="}]},{"#name":"author","$":{"author-id":"S0925231224008397-3036e1b95e5e1be1474a6b4bc5925629","biographyid":"bio8","id":"au000008","orcid":"0000-0003-4681-9129"},"$$":[{"#name":"given-name","_":"Min"},{"#name":"surname","_":"Xia"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/resources"},"_":"Resources"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"cross-ref","$":{"id":"d1e4465","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTglMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBeGlhbWluJTQwbnVpc3QuZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMnhpYW1pbiU0MG51aXN0LmVkdS5jbiUyMiU3RA=="}]},{"#name":"affiliation","$":{"affiliation-id":"S0925231224008397-292b5bb0fcdbd4461276125f185ce9d3","id":"aff1"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"School of Automation, Nanjing University of Information Science and Technology, Nanjing, 210044, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Automation, Nanjing University of Information Science and Technology"},{"#name":"city","_":"Nanjing"},{"#name":"postal-code","_":"210044"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"afs121"},"_":"organization=School of Automation, Nanjing University of Information Science and Technology, city=Nanjing, postcode=210044, country=China"}]},{"#name":"affiliation","$":{"affiliation-id":"S0925231224008397-7db6995111dbef3d0292d019104b27b8","id":"aff2"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"CICAEET, Nanjing University of Information Science and Technology, Nanjing, 210044, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"CICAEET, Nanjing University of Information Science and Technology"},{"#name":"city","_":"Nanjing"},{"#name":"postal-code","_":"210044"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"afs122"},"_":"organization=CICAEET, Nanjing University of lnformation Science and Technology, city=Nanjing, postcode=210044, country=China"}]},{"#name":"affiliation","$":{"affiliation-id":"S0925231224008397-3141937c746f9929277d95ee92c3809a","id":"aff3"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"School of Automation, Wuxi University, Wuxi, 214000, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Automation, Wuxi University"},{"#name":"city","_":"Wuxi"},{"#name":"postal-code","_":"214000"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"afs123"},"_":"organization=School of Automation, Wuxi University, city=Wuxi, postcode=214000, country=China"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"affiliations":{"aff1":{"#name":"affiliation","$":{"affiliation-id":"S0925231224008397-292b5bb0fcdbd4461276125f185ce9d3","id":"aff1"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"School of Automation, Nanjing University of Information Science and Technology, Nanjing, 210044, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Automation, Nanjing University of Information Science and Technology"},{"#name":"city","_":"Nanjing"},{"#name":"postal-code","_":"210044"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"afs121"},"_":"organization=School of Automation, Nanjing University of Information Science and Technology, city=Nanjing, postcode=210044, country=China"}]},"aff2":{"#name":"affiliation","$":{"affiliation-id":"S0925231224008397-7db6995111dbef3d0292d019104b27b8","id":"aff2"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"CICAEET, Nanjing University of Information Science and Technology, Nanjing, 210044, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"CICAEET, Nanjing University of Information Science and Technology"},{"#name":"city","_":"Nanjing"},{"#name":"postal-code","_":"210044"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"afs122"},"_":"organization=CICAEET, Nanjing University of lnformation Science and Technology, city=Nanjing, postcode=210044, country=China"}]},"aff3":{"#name":"affiliation","$":{"affiliation-id":"S0925231224008397-3141937c746f9929277d95ee92c3809a","id":"aff3"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"School of Automation, Wuxi University, Wuxi, 214000, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Automation, Wuxi University"},{"#name":"city","_":"Wuxi"},{"#name":"postal-code","_":"214000"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"afs123"},"_":"organization=School of Automation, Wuxi University, city=Wuxi, postcode=214000, country=China"}]}},"correspondences":{"cor1":{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","_":"Corresponding author."}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"authorMetadata":[],"banner":{"expanded":false},"biographies":{"content":[{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio1","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14255","locator":"fx1","href":"pii:S0925231224008397/fx1","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14256","view":"all"},"$$":[{"#name":"bold","_":"Kai Hu"},{"#name":"__text__","_":", male, Ph.D, 1981.4. Associate professor and Doctoral Supervisor of School of Automation, Nanjing University of Information Science and Technology, China. His research interest areas include remote sensing, disturbed learning, action recognition, image restoration, and published more than 50 SCI(E) indexed papers."}]},{"#name":"simple-para","$":{"id":"d1e14260","view":"all"},"_":"ORCID: 0000-0001-7181-9935."},{"#name":"simple-para","$":{"id":"d1e14262","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref7","href":"mailto:001600@nuist.edu.cn","type":"simple"},"_":"001600@nuist.edu.cn"},{"#name":"__text__","_":", "},{"#name":"inter-ref","$":{"id":"interref8","href":"mailto:nuistpanda@163.com","type":"simple"},"_":"nuistpanda@163.com"}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio2","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14271","locator":"fx2","href":"pii:S0925231224008397/fx2","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14272","view":"all"},"$$":[{"#name":"bold","_":"Mingyang Li"},{"#name":"__text__","_":", male, Candidate for Masterâ€™s degree, 1999. Student of School of Automation, Nanjing University of Information Science and Technology, China. His research interest area is Multi-agent system collaboration and control."}]},{"#name":"simple-para","$":{"id":"d1e14276","view":"all"},"_":"ORCID: 0000-0002-4879-6437."},{"#name":"simple-para","$":{"id":"d1e14278","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref9","href":"mailto:202212490507@nuist.edu.cn","type":"simple"},"_":"202212490507@nuist.edu.cn"}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio3","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14284","locator":"fx3","href":"pii:S0925231224008397/fx3","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14285","view":"all"},"$$":[{"#name":"bold","_":"Zhiqiang SONG"},{"#name":"__text__","_":", born in 1977, Ph.D, associate professor. His research interests include intelligent control, swarm intelligence system task planning and optimal scheduling, etc."}]},{"#name":"simple-para","$":{"id":"d1e14289","view":"all"},"_":"ORCID: 0009-0000-5120-3447"},{"#name":"simple-para","$":{"id":"d1e14291","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref10","href":"mailto:zqsong@cwxu.edu.cn","type":"simple"},"_":"zqsong@cwxu.edu.cn"}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio4","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14297","locator":"fx4","href":"pii:S0925231224008397/fx4","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14298","view":"all"},"$$":[{"#name":"bold","_":"Keer Xu"},{"#name":"__text__","_":", famale, Candidate for Masterâ€™s degree, 1999. Student of School of Automation, Nanjing University of Information Science and Technology, China. Her research interest area is Multi-agent system collaboration and control."}]},{"#name":"simple-para","$":{"id":"d1e14302","view":"all"},"_":"ORCID: 0000-0003-0799-483X."},{"#name":"simple-para","$":{"id":"d1e14304","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref11","href":"mailto:202212490502@nuist.edu.cn","type":"simple"},"_":"202212490502@nuist.edu.cn"}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio5","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14310","locator":"fx5","href":"pii:S0925231224008397/fx5","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14311","view":"all"},"$$":[{"#name":"bold","_":"Qingfeng Xia"},{"#name":"__text__","_":", male, Ph.D, 1982, associate professor. Wuxi University, China. His research interest area is Multi-agent system collaboration and control."}]},{"#name":"simple-para","$":{"id":"d1e14315","view":"all"},"_":"ORCID: 0000-0003-0829-6750."},{"#name":"simple-para","$":{"id":"d1e14317","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref12","href":"mailto:xqf@cwxu.edu.cn","type":"simple"},"_":"xqf@cwxu.edu.cn"}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio6","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14325","locator":"fx6","href":"pii:S0925231224008397/fx6","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14326","view":"all"},"$$":[{"#name":"bold","_":"Ning Sun"},{"#name":"__text__","_":", born in 1981, professor. His research interests include Artificial Intelligence and System Integration, etc."}]},{"#name":"simple-para","$":{"id":"d1e14330","view":"all"},"_":"ORCID: 0000-0002-0280-5983"},{"#name":"simple-para","$":{"id":"d1e14332","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref13","href":"mailto:001764@cwxu.edu.cn","type":"simple"},"_":"001764@cwxu.edu.cn"}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio7","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14338","locator":"fx7","href":"pii:S0925231224008397/fx7","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14339","view":"all"},"$$":[{"#name":"bold","_":"Peng Zhou"},{"#name":"__text__","_":", male, Candidate for Masterâ€™s degree, 2000. Student of School of Automation, Nanjing University of Information Science and Technology, China. His research interest areas include Multi-agent system collaboration and control."}]},{"#name":"simple-para","$":{"id":"d1e14343","view":"all"},"_":"ORCID: 0009-0002-1914-1666."},{"#name":"simple-para","$":{"id":"d1e14345","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref14","href":"mailto:202312490378@nuist.edu.cn","type":"simple"},"_":"202312490378@nuist.edu.cn"}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio8","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14351","locator":"fx8","href":"pii:S0925231224008397/fx8","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14352","view":"all"},"$$":[{"#name":"bold","_":"Min Xia"},{"#name":"__text__","_":", male, Ph.D., 1983.10. Professor and Ph.D. Supervisor of School of Automation, Nanjing University of Information Science and Technology. Now his main research interests are artificial intelligence and big data analysis theory, specifically: relevance knowledge mining of large-scale heterogeneous data, machine learning theory, multi-source image data analysis, satellite remote sensing image classification, etc. He has published more than 100 papers in domestic and international journals and conferences, with more than 2000 citations."}]},{"#name":"simple-para","$":{"id":"d1e14356","view":"all"},"_":"ORCID: 0000-0003-4681-9129."},{"#name":"simple-para","$":{"id":"d1e14358","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref15","href":"mailto:xiamin@nuist.edu.cn","type":"simple"},"_":"xiamin@nuist.edu.cn"}]}]}],"floats":[],"footnotes":[],"attachments":[{"attachment-eid":"1-s2.0-S0925231224008397-fx2.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx2/DOWNSAMPLED/image/jpeg/745598e5792fb84d32a82479b707a1b5/fx2.jpg","file-basename":"fx2","filename":"fx2.jpg","extension":"jpg","filesize":"16193","pixel-height":"132","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231224008397-fx3.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx3/DOWNSAMPLED/image/jpeg/c395b0e10e69f2d6f93e6eaa7e4ced20/fx3.jpg","file-basename":"fx3","filename":"fx3.jpg","extension":"jpg","filesize":"15710","pixel-height":"132","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231224008397-fx1.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx1/DOWNSAMPLED/image/jpeg/c9adfaae216bf77d2e4d5a38eeb559f3/fx1.jpg","file-basename":"fx1","filename":"fx1.jpg","extension":"jpg","filesize":"15596","pixel-height":"132","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231224008397-fx6.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx6/DOWNSAMPLED/image/jpeg/5f2d9ce563097e292a610aea789f63ce/fx6.jpg","file-basename":"fx6","filename":"fx6.jpg","extension":"jpg","filesize":"16178","pixel-height":"132","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231224008397-fx7.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx7/DOWNSAMPLED/image/jpeg/6cf6f0481de237bb2023fa115845edc0/fx7.jpg","file-basename":"fx7","filename":"fx7.jpg","extension":"jpg","filesize":"15447","pixel-height":"132","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231224008397-fx4.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx4/DOWNSAMPLED/image/jpeg/c40011a3da4b91681935c4fa47792c9d/fx4.jpg","file-basename":"fx4","filename":"fx4.jpg","extension":"jpg","filesize":"15451","pixel-height":"132","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231224008397-fx5.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx5/DOWNSAMPLED/image/jpeg/4546b63b1fd5dafd0706b7346e5a2e63/fx5.jpg","file-basename":"fx5","filename":"fx5.jpg","extension":"jpg","filesize":"16346","pixel-height":"132","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231224008397-fx8.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx8/DOWNSAMPLED/image/jpeg/6ff3067d5b792fc8d1f99c284c8f0eb0/fx8.jpg","file-basename":"fx8","filename":"fx8.jpg","extension":"jpg","filesize":"15162","pixel-height":"132","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231224008397-fx2.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx2/THUMBNAIL/image/gif/a4350d9855bce3eba721078edfda09af/fx2.sml","file-basename":"fx2","filename":"fx2.sml","extension":"sml","filesize":"22008","pixel-height":"164","pixel-width":"140","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231224008397-fx3.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx3/THUMBNAIL/image/gif/3ef9b499a402ff7ad9250f844ef718af/fx3.sml","file-basename":"fx3","filename":"fx3.sml","extension":"sml","filesize":"22328","pixel-height":"164","pixel-width":"140","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231224008397-fx1.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx1/THUMBNAIL/image/gif/476c867c2aeb5d3412e41559206064e3/fx1.sml","file-basename":"fx1","filename":"fx1.sml","extension":"sml","filesize":"24735","pixel-height":"164","pixel-width":"140","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231224008397-fx6.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx6/THUMBNAIL/image/gif/39afa64b5a11c3a01d6b3025985ab8f5/fx6.sml","file-basename":"fx6","filename":"fx6.sml","extension":"sml","filesize":"20755","pixel-height":"164","pixel-width":"140","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231224008397-fx7.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx7/THUMBNAIL/image/gif/258557d854536687c9210c2f6a795e1a/fx7.sml","file-basename":"fx7","filename":"fx7.sml","extension":"sml","filesize":"19932","pixel-height":"164","pixel-width":"140","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231224008397-fx4.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx4/THUMBNAIL/image/gif/8ab712f247fb08ac52c3dc89d3c0783f/fx4.sml","file-basename":"fx4","filename":"fx4.sml","extension":"sml","filesize":"20555","pixel-height":"164","pixel-width":"140","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231224008397-fx5.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx5/THUMBNAIL/image/gif/4b40d27d1d5caf95f5fb6d4413e870b6/fx5.sml","file-basename":"fx5","filename":"fx5.sml","extension":"sml","filesize":"23459","pixel-height":"164","pixel-width":"140","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231224008397-fx8.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/fx8/THUMBNAIL/image/gif/34fa27cdde2009f9cc9ac10d55e3622c/fx8.sml","file-basename":"fx8","filename":"fx8.sml","extension":"sml","filesize":"21240","pixel-height":"164","pixel-width":"140","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231224008397-fx2_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/HIGHRES/image/jpeg/7479eaa64bba63e5b597fc4436b1fa23/fx2_lrg.jpg","file-basename":"fx2","filename":"fx2_lrg.jpg","extension":"jpg","filesize":"58301","pixel-height":"584","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231224008397-fx3_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/HIGHRES/image/jpeg/3e7bc1f56366ad9f73675a92513d1a9c/fx3_lrg.jpg","file-basename":"fx3","filename":"fx3_lrg.jpg","extension":"jpg","filesize":"62166","pixel-height":"584","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231224008397-fx1_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/HIGHRES/image/jpeg/0b4d492c8c1e0d16d7028519141f065c/fx1_lrg.jpg","file-basename":"fx1","filename":"fx1_lrg.jpg","extension":"jpg","filesize":"57661","pixel-height":"584","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231224008397-fx6_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/HIGHRES/image/jpeg/8c9017c504c732628359984380ee7cc9/fx6_lrg.jpg","file-basename":"fx6","filename":"fx6_lrg.jpg","extension":"jpg","filesize":"54249","pixel-height":"584","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231224008397-fx7_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/HIGHRES/image/jpeg/0eb70f43b16f62941cf8404f2ced3210/fx7_lrg.jpg","file-basename":"fx7","filename":"fx7_lrg.jpg","extension":"jpg","filesize":"61532","pixel-height":"584","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231224008397-fx4_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/HIGHRES/image/jpeg/d6014bbd0bd4b65ce73ce90dbae5babe/fx4_lrg.jpg","file-basename":"fx4","filename":"fx4_lrg.jpg","extension":"jpg","filesize":"54798","pixel-height":"584","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231224008397-fx5_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/HIGHRES/image/jpeg/f12343fa8d9185e02784c5babe939ff6/fx5_lrg.jpg","file-basename":"fx5","filename":"fx5_lrg.jpg","extension":"jpg","filesize":"57826","pixel-height":"584","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231224008397-fx8_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231224008397/HIGHRES/image/jpeg/29c6a0c2578a765c6caae34fc088f5d6/fx8_lrg.jpg","file-basename":"fx8","filename":"fx8_lrg.jpg","extension":"jpg","filesize":"56695","pixel-height":"584","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"}]},"body":{},"chapters":{"toc":[],"isLoading":false},"changeViewLinks":{"showFullTextLink":true,"showAbstractLink":false},"citingArticles":{"hitCount":5,"viewMoreUrl":"http://www.scopus.com/scopus/inward/citedby.url?partnerID=10&rel=3.0.0&eid=2-s2.0-85196977035&md5=d28982e1e7ba8a2f62b7d3ace93282","articles":[{"articleTitle":"Intelligent machines as information and communication technology and their influence on sustainable marketing practices for beneficial impact on business performance: A conceptual framework","authors":"Behera R.K., Rehman A.U., Imtiaz A.","doi":"10.1016/j.jclepro.2024.143676","externalArticle":false,"issn":"09596526","openAccess":0,"pii":"S0959652624031251","publicationDate":"2024-10-10","publicationTitle":"Journal of Cleaner Production","publicationYear":"2024","scopusArticleUrl":"http://www.scopus.com/scopus/inward/record.url?partnerID=10&rel=3.0.0&view=basic&eid=2-s2.0-85204428071&md5=339fb0d042a6416d121c7fdf22d7a3c","scopusEid":"2-s2.0-85204428071","snippets":["To increase the accuracy and focus of the final solution, data must be pre-processed and structured in a way to solve the problem by applying the most suitable algorithm and establishing a model (i.e., emulating decision-making based on sourced data), which represents the blueprint of what needs to be done. In continuation, to enable AI learning in machines, several machine intelligence techniques have been proposed, such as reinforcement learning (Hu et al., 2024) and imitation learning (Huang et al., 2024). In reinforcement learning, a machine learns to do a task through repeated trial-and-error interactions in a dynamic operating environment."],"thirdParty":false,"volume":"Volume 475","abstract":{"$$":[{"$$":[{"$$":[{"#name":"attachment-eid","_":"1-s2.0-S0959652624031251-ga1.jpg"},{"#name":"ucs-locator","_":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0959652624031251/ga1/DOWNSAMPLED/image/jpeg/13e9d6f756d51b2c329bd371c990d561/ga1.jpg"},{"#name":"file-basename","_":"ga1"},{"#name":"abstract-attachment","_":"true"},{"#name":"filename","_":"ga1.jpg"},{"#name":"extension","_":"jpg"},{"#name":"filesize","_":"119236"},{"#name":"pixel-height","_":"160"},{"#name":"pixel-width","_":"500"},{"#name":"attachment-type","_":"IMAGE-DOWNSAMPLED"}],"$":{"xmlns:xocs":true},"#name":"attachment"},{"$$":[{"#name":"attachment-eid","_":"1-s2.0-S0959652624031251-ga1.sml"},{"#name":"ucs-locator","_":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0959652624031330/ga1/THUMBNAIL/image/gif/9fd9ec3d419fbcdfc10f364a13a9a82a/ga1.sml"},{"#name":"file-basename","_":"ga1"},{"#name":"abstract-attachment","_":"true"},{"#name":"filename","_":"ga1.sml"},{"#name":"extension","_":"sml"},{"#name":"filesize","_":"76176"},{"#name":"pixel-height","_":"70"},{"#name":"pixel-width","_":"219"},{"#name":"attachment-type","_":"IMAGE-THUMBNAIL"}],"$":{"xmlns:xocs":true},"#name":"attachment"},{"$$":[{"#name":"attachment-eid","_":"1-s2.0-S0959652624031251-ga1_lrg.jpg"},{"#name":"ucs-locator","_":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0959652624031330/HIGHRES/image/jpeg/3bebabdbec84f810e249c0946a1ce1aa/ga1_lrg.jpg"},{"#name":"file-basename","_":"ga1"},{"#name":"abstract-attachment","_":"true"},{"#name":"filename","_":"ga1_lrg.jpg"},{"#name":"extension","_":"jpg"},{"#name":"filesize","_":"496624"},{"#name":"pixel-height","_":"706"},{"#name":"pixel-width","_":"2213"},{"#name":"attachment-type","_":"IMAGE-HIGH-RES"}],"$":{"xmlns:xocs":true},"#name":"attachment"}],"#name":"attachments"},{"$$":[{"$":{"id":"sectitle0010"},"#name":"section-title","_":"Abstract"},{"$$":[{"$$":[{"#name":"underline","_":"Intelligent machines"},{"#name":"__text__","_":" are the machines or devices that make use of artificial intelligence and robotics technologies. It has the ability to accomplish a specific task in the presence of uncertainty and variability in its operating environment. Certainly, it can be "},{"#name":"underline","_":"used to"},{"#name":"__text__","_":" support "},{"#name":"underline","_":"information and communication technology"},{"#name":"__text__","_":" to streamline the creation, collection, processing, transmission, and storage of information for "},{"#name":"underline","_":"sustainable marketing practices"},{"#name":"__text__","_":". The flawless application of sustainable marketing practices results in "},{"#name":"underline","_":"beneficial impacts"},{"#name":"__text__","_":" on "},{"#name":"underline","_":"business performance"},{"#name":"__text__","_":". In fact, the issue of unsustainable marketing practices can be effectively managed by intelligent machines. Therefore, this study is undertaken to uncover how intelligent machines can influence sustainable marketing practices for beneficial impacts on retailersâ€™ business performance by proposing a unique conceptual framework. The theoretical contributions discuss two "},{"#name":"underline","_":"techno-sustainable marketing applications"},{"#name":"__text__","_":". First, intelligent machines improve incremental innovation. This allows retailers to balance technology risk with sustainable marketing and lower the cost of innovations. Second, intelligent machines increase business efficiency by automating sustainable marketing practices. This allows retailers to efficiently manage the inventory, improve fulfilment efficiency, and optimise stock levels. The managerial implications discuss two goals of sustainable marketing practices. First, it can attract sustainability-minded customers who support the retail business for their own well-being. Second, it builds a strong sustainable brand reputation that can lower the "},{"#name":"underline","_":"price sensitivity"},{"#name":"__text__","_":"."}],"$":{"view":"all","id":"abspara0010"},"#name":"simple-para"}],"$":{"view":"all","id":"abssec0010"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0010","lang":"en","class":"author"},"#name":"abstract"},{"$$":[{"$":{"id":"sectitle0015"},"#name":"section-title","_":"Graphical abstract"},{"$$":[{"$$":[{"$$":[{"$$":[{"$":{"role":"short","id":"alttext0010"},"#name":"alt-text","_":"Image 1"},{"$":{"role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","xmlns:xlink":true,"href":"pii:S0959652624031251/ga1","id":"aep-link-id4","type":"simple","locator":"ga1"},"#name":"link"}],"$":{"id":"undfig1"},"#name":"figure"}],"#name":"display"}],"$":{"view":"all","id":"abspara0015"},"#name":"simple-para"}],"$":{"view":"all","id":"abssec0015"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0015","class":"graphical"},"#name":"abstract"},{"$$":[{"$":{"id":"sectitle0020"},"#name":"section-title","_":"Highlights"},{"$$":[{"$$":[{"$$":[{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0010"},"#name":"para","_":"Theoretically presented the use of intelligent machines to support ICT."}],"$":{"id":"u0010"},"#name":"list-item"},{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0015"},"#name":"para","_":"Intelligent machines capabilities positively influence sustainable marketing practice."}],"$":{"id":"u0015"},"#name":"list-item"},{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0020"},"#name":"para","_":"Sustainable marketing practice have a beneficial impact on business performance."}],"$":{"id":"u0020"},"#name":"list-item"},{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0025"},"#name":"para","_":"Intelligent machines improve incremental innovation."}],"$":{"id":"u0025"},"#name":"list-item"},{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0030"},"#name":"para","_":"Intelligent machines increase business efficiency."}],"$":{"id":"u0030"},"#name":"list-item"}],"$":{"id":"ulist0010"},"#name":"list"}],"$":{"view":"all","id":"abspara0020"},"#name":"simple-para"}],"$":{"view":"all","id":"abssec0020"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0020","lang":"en","class":"author-highlights"},"#name":"abstract"}],"$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"#name":"abstracts"}},{"articleTitle":"Review of Satellite Remote Sensing of Carbon Dioxide Inversion and Assimilation","authors":"Hu K., Feng X., Xia M.","doi":"10.3390/rs16183394","externalArticle":true,"openAccess":0,"publicationDate":"September 2024","publicationTitle":"Remote Sensing","publicationYear":"2024","scopusArticleUrl":"http://www.scopus.com/scopus/inward/record.url?partnerID=10&rel=3.0.0&view=basic&eid=2-s2.0-85205237784&md5=20ac7969597931a333f1a61ac5baf","scopusEid":"2-s2.0-85205237784","thirdParty":false,"volume":"Volume 16","abstract":{}},{"articleTitle":"Classification and extraction method of hidden dangers along railway lines based on semantic segmentation network","authors":"Ye X., Dong S., Sheng T.","doi":"10.1080/01431161.2024.2411069","externalArticle":true,"openAccess":0,"page":"9480-9512","publicationDate":"2024","publicationTitle":"International Journal of Remote Sensing","publicationYear":"2024","scopusArticleUrl":"http://www.scopus.com/scopus/inward/record.url?partnerID=10&rel=3.0.0&view=basic&eid=2-s2.0-85205859049&md5=5e1bc860484de0ed285478a3db58fa94","scopusEid":"2-s2.0-85205859049","thirdParty":false,"volume":"Volume 45","abstract":{}},{"articleTitle":"Robust Optimal Control for Cable-Driven Parallel Robots via Event-Triggered ADP","authors":"Lu Y., Li P., Wang X.","doi":"10.1109/ACCESS.2024.3471798","externalArticle":true,"openAccess":0,"page":"145038-145053","publicationDate":"2024","publicationTitle":"IEEE Access","publicationYear":"2024","scopusArticleUrl":"http://www.scopus.com/scopus/inward/record.url?partnerID=10&rel=3.0.0&view=basic&eid=2-s2.0-85205837342&md5=b223587ab34588ce84e4488144ca03a","scopusEid":"2-s2.0-85205837342","thirdParty":false,"volume":"Volume 12","abstract":{}},{"articleTitle":"Optimization of Knitting Path of Flat Knitting Machine Based on Reinforcement Learning","authors":"Yang T.","doi":"10.14569/IJACSA.2024.0150839","externalArticle":true,"openAccess":0,"page":"399-409","publicationDate":"2024","publicationTitle":"International Journal of Advanced Computer Science and Applications","publicationYear":"2024","scopusArticleUrl":"http://www.scopus.com/scopus/inward/record.url?partnerID=10&rel=3.0.0&view=basic&eid=2-s2.0-85202762099&md5=ff7593cec52967178553749f16f38bc","scopusEid":"2-s2.0-85202762099","thirdParty":false,"volume":"Volume 15","abstract":{}}]},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"class":"keyword","id":"d1e4536","view":"all","lang":"en"},"$$":[{"#name":"section-title","$":{"id":"d1e4537"},"_":"Keywords"},{"#name":"keyword","$":{"id":"d1e4539"},"$$":[{"#name":"text","_":"Agent"}]},{"#name":"keyword","$":{"id":"d1e4542"},"$$":[{"#name":"text","_":"Reinforcement learning"}]},{"#name":"keyword","$":{"id":"d1e4545"},"$$":[{"#name":"text","_":"Multi-agent reinforcement learning"}]},{"#name":"keyword","$":{"id":"d1e4548"},"$$":[{"#name":"text","_":"Multi-agent systems"}]}]}]},{"#name":"miscellaneous","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"d1e4528"},"_":"Communicated by L. Zou"},{"#name":"data-availability","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"da1","view":"all"},"$$":[{"#name":"section-title","$":{"id":"d1e4553"},"_":"Data availability"},{"#name":"para","$":{"id":"d1e4555","view":"all"},"_":"No data was used for the research described in the article."}]}],"floats":[],"footnotes":[],"attachments":[]},"crossMark":{"isOpen":false},"domainConfig":{"cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/e444bc27e06cd87e0d0139060246bdab684588aa"},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"downloadCapOpen":false,"articles":[],"selected":[]},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"exam":{},"helpText":{"keyDates":{"html":"<div class=\"key-dates-help\"><h3 class=\"u-margin-s-bottom u-h4\">Publication milestones</h3><p class=\"u-margin-m-bottom\">The dates displayed for an article provide information on when various publication milestones were reached at the journal that has published the article. Where applicable, activities on preceding journals at which the article was previously under consideration are not shown (for instance submission, revisions, rejection).</p><p class=\"u-margin-xs-bottom\">The publication milestones include:</p><ul class=\"key-dates-help-list u-margin-m-bottom u-padding-s-left\"><li><span class=\"u-text-italic\">Received</span>: The date the article was originally submitted to the journal.</li><li><span class=\"u-text-italic\">Revised</span>: The date the most recent revision of the article was submitted to the journal. Dates corresponding to intermediate revisions are not shown.</li><li><span class=\"u-text-italic\">Accepted</span>: The date the article was accepted for publication in the journal.</li><li><span class=\"u-text-italic\">Available online</span>: The date a version of the article was made available online in the journal.</li><li><span class=\"u-text-italic\">Version of Record</span>: The date the finalized version of the article was made available in the journal.</li></ul><p>More information on publishing policies can be found on the <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/about/policies-and-standards/publishing-ethics\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing Ethics Policies</span></span></a> page. View our <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/researcher/author/submit-your-paper\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing with Elsevier: step-by-step</span></span></a> page to learn more about the publishing process. For any questions on your own submission or other questions related to publishing an article, <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://service.elsevier.com/app/phone/supporthub/publishing\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">contact our Researcher support team.</span></span></a></p></div>","title":"What do these dates mean?"}},"glossary":{},"issueNavigation":{"previous":{},"next":{}},"linkingHubLinks":{},"metrics":{"metricGroup":{"citations":[],"captures":[],"mentions":[],"socialMedia":[]},"isLoading":false,"error":false},"preview":{"content":[{"#name":"introduction","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec1","role":"introduction","view":"all"},"$$":[{"#name":"label","_":"1"},{"#name":"section-title","$":{"id":"d1e4562"},"_":"Introduction"},{"#name":"section","$":{"id":"sec1.1","role":"background","view":"all"},"$$":[{"#name":"label","_":"1.1"},{"#name":"section-title","$":{"id":"d1e4567"},"_":"Background"},{"#name":"para","$":{"id":"d1e4569","view":"all"},"_":"With the continuous development of society and economy, many problems in modern society have become intricate and complex, and there is an urgent need for more efficient, flexible and intelligent solutions. Multi-agent Reinforcement Learning (MARL) helps to better simulate and understand complex social systems and interactions and provides smarter and more efficient solutions to these problems. Using MARL algorithms, multi-agent can work together to complete complex tasks, thus improving the efficiency and accuracy of the whole system. In addition, it can be used for AI decision-making, assisting it to make smarter decisions in diverse and complex environments."},{"#name":"para","$":{"id":"d1e4571","view":"all"},"_":"Multi-agent systems (MAS) encompass multiple distributed entities, i.e., agents, that make decisions independently and interact with each other in a shared environment [1]. With the diversity of tasks, complex interactions between agents may occur to decide whether to collaborate or adopt competitive strategies to outperform competitors. Determining intelligent behavior in a pre-programmed manner in complex systems is a challenging task [2]."},{"#name":"para","$":{"id":"d1e4581","view":"all"},"_":"MARL is an application of Reinforcement Learning (RL) in MAS, which focuses on the problem of multi-agent completing a task with interaction and collaboration to promote interaction and collaboration among the agents. In MAS, each agent possesses its own state, action, and reward signals, and they reach a common goal by interacting with each other [3]. Unlike single-agent RL, it needs to consider the interactions and collaborations between agents, which increases the complexity and challenge of the problem. In MARL, the agents can choose different strategies, such as cooperation, competition, coordination, etc., which can be rule-based or learned. Usually, the goal of the above methods is to achieve some kind of globally optimal solution through collaboration between the agents."},{"#name":"para","$":{"id":"d1e4587","view":"all"},"_":"MARL combines Collaborative Learning (CL) and RL to address the collaboration and competition problem in MAS, whereby each agent interacts with other agents by sensing the environment to learn the best behavioral strategies to optimize overall performance.CL focuses on cooperation and coordination between multi-agent, while RL explores how agents can derive rewards from the environment and optimize strategies."},{"#name":"para","$":{"id":"d1e4589","view":"all"},"_":"CL is an approach in which multi-agent cooperate to achieve a common goal, which usually occurs in the field of artificial agents in scenarios such as MAS and robot collaboration. In this context, agents can collaborate by communicating, sharing information, and coordinating decisions, to improve the performance of the whole system through the interoperability of the agents. Fig. 1 shows a generic model diagram for RL."},{"#name":"para","$":{"id":"d1e4595","view":"all"},"_":"RL is a branch of machine learning (ML) that focuses on how agents can achieve predefined goals through interaction with the environment in a trial-and-error learning process. In RL, an agent interacts with the environment, obtains rewards or penalties based on the state of the environment and the behavior of the agent, and adjusts its behavioral strategies intending to maximize long-term cumulative rewards [4]. It allows the agent to learn the optimal policy by interacting with the environment to maximize rewards in future environments."},{"#name":"para","$":{"id":"d1e4601","view":"all"},"_":"In MARL, each agent is an independent RL agent, and the value of each state and action is assessed through a value function, which allows the agents to continuously adjust their behavior based on the feedback signals from the environment to maximize the expected long-term rewards. However, in a multi-agent environment, the value function needs to take into account the effects of the actions of other agents on the current one, considering that the actions of the agents interact with each other. To address this issue, CL is introduced into MARL.CL enables collaboration between agents by sharing information such as states, rewards, and actions."},{"#name":"para","$":{"id":"d1e4605","view":"all"},"_":"MARL is a product of the organic combination of CL and RL, which makes full use of the decision-making ability of RL and the coordination ability of CL to achieve collaboration and optimal decision-making among multi-agent."}]},{"#name":"section","$":{"id":"sec1.2","view":"all"},"$$":[{"#name":"label","_":"1.2"},{"#name":"section-title","$":{"id":"d1e4610"},"_":"Development history"},{"#name":"para","$":{"id":"d1e4612","view":"all"},"_":"With the continuous development of AI technology, MARL has received more and more attention as an important part of it. It applies CL to solve the learning problem of MAS, aiming to achieve optimal decision-making of the whole system by exploiting the collaboration between agents. During the past decades, researchers have proposed diverse MARL algorithms. This paper aims to analyze the development of MARL in recent years and provide a systematic introduction."},{"#name":"para","$":{"id":"d1e4614","view":"all"},"_":"Before 2000, the field of MARL was in its infancy, with research focusing primarily on theoretical exploration and the initial establishment of methodologies. Researchers began to realize the potential value of applying reinforcement learning in MAS, even though significant progress has not yet been made in terms of algorithms and frameworks, during this period some researchers have started to investigate MARL methods in both independent and collaborative environments, e.g., Tan [5], Caroline Claus and Craig Boutilier [6] addressed MARL related issues are discussed."},{"#name":"para","$":{"id":"d1e4624","view":"all"},"_":"During the initial development phase from 2000 to 2010, MARL gradually became an independent research direction and made some progress. Researchers began to enrich the theoretical foundation of MARL and proposed a series of new algorithms and methods, including game theory-based methods, centralized agent RL methods, and distributed control algorithms. The focus of research in this stage is centered on issues such as how multi-agent collaborate to complete tasks and how to resolve conflicts between different agents. In this phase, MARL emerged with representative algorithms such as Joint action learning. These works laid the foundation for subsequent research, but challenges still exist in dealing with complex multi-agent interaction problems."},{"#name":"para","$":{"id":"d1e4626","view":"all"},"_":"Between 2010 and 2017, MARL entered its mid-development phase, and the field of artificial agents witnessed it making more significant progress. Researchers continue to propose new algorithms and methods, including elements such as distributed agent reinforcement learning, deep reinforcement learning (DRL), and meta-learning. The problems of how to improve the efficiency, robustness, and scalability of MAS are the focus of research in this stage, and algorithms such as Deep Q-Network (DQN) and DDPG play a great role in solving these problems."},{"#name":"para","$":{"id":"d1e4628","view":"all"},"_":"Since 2017, MARL has been further developed and researchers have proposed more advanced algorithms and methods, including multi-agent evolutionary algorithms as well as hierarchical RL, and some new application areas, such as humanâ€“computer collaboration, robot control, and the Internet of Things. New issues have also emerged from this phase of research, such as the robustness problem of MAS, how to improve the efficiency and scalability of MAS, and the realization of more complex multi-agent tasks. In this field, many algorithms such as MAAC, QMIX, PPO, etc. have achieved remarkable success."},{"#name":"para","$":{"id":"d1e4630","view":"all"},"_":"Due to the rise of Deep Learning (DL) technology and its remarkable achievements in many fields, the fusion of Deep Neural Networks (DQNs) and RL into DRL has become the focus of extensive research. Significant breakthroughs in this approach have been achieved in areas such as computer vision, robot control, and large-scale real-time policy games. The great success of DRL has sparked more researchers to focus on the field of multi-agent. Researchers have boldly attempted to introduce DRL methods into MAS to solve complex tasks in multi-agent environments, and this approach has laid the foundation for the rise of multi-agent deep reinforcement learning (MADRL)."},{"#name":"para","$":{"id":"d1e4632","view":"all"},"_":"This paper provides an extensive review of recent advances in the field of MADRL."},{"#name":"para","$":{"id":"d1e4634","view":"all"},"_":"Firstly, MADRL involves agents (or decision makers) interacting with each other and with the environment to achieve a common goal cooperatively or competitively. In traditional RL, agents learn how to make optimal decisions by interacting with the environment to obtain reward signals. However, in multi-agent CL, as the behaviors of multi-agent interact with each other, a single dimension of reward signals often fails to adequately reflect this complex collaborative relationship. To consider the collaborative relationship between multi-agent more comprehensively, the MADRL method introduces multi-dimensional reward signals. These reward signals can cover multiple dimensions, including individual reward, team reward, and collaborative reward, to comprehensively consider the collaborative effects among multi-agent."},{"#name":"para","$":{"id":"d1e4636","view":"all"},"_":"In addition, the MADRL method can also apply techniques such as Hierarchical Reinforcement Learning and Evolutionary Algorithms to optimize the decision-making of the agents, as a way to further improve the learning effect.MADRL not only extends the traditional RL but also integrates MARL with DL, which represents the latest advances in the field of artificial agents, of which the MADDPG algorithm is a typical example."},{"#name":"para","$":{"id":"d1e4639","view":"all"},"_":"The MADRL method has a broad application prospect in the field of MARL. It can solve the problem of collaboration among multi-agent more effectively under the CL framework, improve learning efficiency, and provide useful references and lessons for a wider range of MAS applications in the future."},{"#name":"para","$":{"id":"d1e4641","view":"all"},"_":"After years of innovation and development, MADRL has spawned a variety of algorithms, rules, and frameworks that have been widely applied in various real-world domains. The evolutionary trajectory from single to multi-agent, from simple to complex tasks, and from low-dimensional to high-dimensional environments suggests that MADRL technology has emerged and gradually become a high-profile research and application direction in the field of ML and even artificial agent, which is of great research value and practical significance. Fig. 2 provides a summary of the stages of MARL development."},{"#name":"para","$":{"id":"d1e4647","view":"all"},"_":"However, in MAS, the optimization of interactions and collaboration between agents, the non-stationarity of the system, uncertainty, and how to improve the learning efficiency are all issues that researchers need to address urgently. Therefore, it is of great significance to classify and review MARL algorithms in this paper, and, it helps researchers to better understand the current state of research and future research directions in this field."},{"#name":"para","$":{"id":"d1e4649","view":"all"}}]},{"#name":"section","$":{"id":"sec1.3","view":"all"},"$$":[{"#name":"label","_":"1.3"},{"#name":"section-title","$":{"id":"d1e4655"},"_":"Survey and contributions"},{"#name":"para","$":{"id":"d1e4657","view":"all"},"_":"Nowadays, MAS has a wide range of applications in various fields, including disaster rescue, environmental monitoring, traffic management, and so on. However, the applicability of multi-agent systems is affected by realistic factors, such as communication delays, environmental changes, and even mission requirements and constraints. Due to the uncertainty between the collaboration of agents, how to flexibly build a MAS to effectively integrate and coordinate the heterogeneity and maximize the collaboration capability between agents becomes a huge challenge to be solved nowadays."},{"#name":"para","$":{"id":"d1e4659","view":"all"},"_":"Therefore, researchers continue to improve and optimize MAS algorithms, aiming to promote the application of MAS in complex environments, improve the adaptability, synergy, and performance of the system, and promote the achievement of MAS technology in practical engineering applications. However, there is a wide variety of MAS algorithms, and the theory needs to be transformed into practical solutions in practical applications. By systematically analyzing the challenges involved in MAS, this paper can provide a reference framework for relevant researchers as well as engineers to help them better apply MAS algorithms to practical scenarios and solve practical problems."},{"#name":"para","$":{"id":"d1e4661","view":"all"},"_":"In this paper, the Science Citation Index-Expanded (SCIE) database was searched by the keywords of â€œMASâ€, â€œMARLâ€, â€œMARL algorithmsâ€ and other keywords, a total of 120 related articles were obtained, including reviews as well as specific algorithms. In addition, this paper summarizes some evaluation metrics as performance measures of MARL algorithms."},{"#name":"para","$":{"id":"d1e4663","view":"all"},"_":"In this paper, existing MARL algorithms are carefully categorized and summarized to facilitate further exploration by researchers. Before this, excellent review articles have been published on the field of MARL."},{"#name":"para","$":{"id":"d1e4665","view":"all"},"_":"Although the concept of MARL was first proposed in the 1980s, unfortunately, MARL did not receive much attention until 2000, and therefore no systematic review articles appeared. However, some researchers at that time had already started to study MARL-related issues in the areas of MAS, RL, and game theory. For example, the first formalization of IQL and its application to the field of MARL were presented in Tan [5]; Caroline Claus and Craig Boutilier [6] investigated Q-Learning in collaborative MAS from different perspectives. The early research results laid a theoretical foundation for the field of MARL and provided important references and inspirations for future research."},{"#name":"para","$":{"id":"d1e4675","view":"all"},"_":"Over time, the combination of MAS and RL has been in a permanent research boom. As one of the most representative first surveys in the field, Stone and Veloso [7] analyzed MAS from an ML perspective, categorizing all the literature reviewed by them according to the structure of the agent as well as the perspective of learning algorithms. The authors address research issues and challenges in MAS related to system dynamics, cooperative and competitive games, social norms, etc. in some detail. Shoham et al. [8], Pieter Jan â€™t Hoen et al. [9], Busoniu et al. [10], in emphasizing the complexity as well as the importance of MAS premise, not only explored different forms and applications of MARL but also evaluated multiple approaches. Although these review articles have made a great contribution in outlining and summarizing the current state of research, issues and methods in MARL, there are still some inevitable limitations, such as the lack of a comprehensive focus and the fact that the research only covers some specific types of MAS and tasks."},{"#name":"para","$":{"id":"d1e4694","view":"all"},"_":"The booming development of MARL has attracted more and more researchers to explore it in depth, and this phase can be regarded as the maturity phase of MARL, with numerous review articles exploring the various issues involved. Matignon et al. [11] identified several challenges in coordinating independent learners in a fully collaborative MG: Pareto selection, non-smoothness, stochasticity, altered exploration, and shaded equilibrium. Furthermore, the researchers analyzed the conditions under which algorithms can solve such coordination problems. Another work by Tuyls and Weiss [12] explains the historical development of MARL and raises non-technical challenges. The above studies provide a more systematic and in-depth look at the research directions and focus of MARL and suggest viable options for future research directions."},{"#name":"para","$":{"id":"d1e4704","view":"all"},"_":"With the emergence of DL methods and breakthroughs, the field of MARL has attracted new attention from the community and a large body of related literature has emerged during the past few years. Researchers have challenged real-world complexity problems by overcoming historical limitation issues through DL approaches (Baker et al. [13]; Berner et al. [14]; Jaderberg et al. [15]; Vinyals et al. [16]). Nguyen et al. [17] presented five technical challenges including non-smoothness, partial observability, continuous space, training schemes, and transfer learning, while also discussing possible solutions and their practical applications. Hernandez-Leal et al. [18] focus on four main categories: emergency behavior, learning communication, learning cooperation, and analysis of agent modeling. In addition, they provide an in-depth survey of further research on MADRL in specific sub-domains. Oroojlooyjadid and Hajinezhad [19] review recent work in collaborative environments, whereas the studies of Da Silva and Costa [20] and Da Silva et al. [21] are more focussed on knowledge reuse. Lazaridou and Baroni [22], while reviewing the emergence of language, linked two perspectives, including the conditions under which language develops in a community and the ability to solve problems through dynamic communication. Based on theoretical analyses, Zhang et al. [23] focused on MARL algorithms and presented challenges from a mathematical perspective."},{"#name":"para","$":{"id":"d1e4752","view":"all"},"_":"The above review articles provide a comprehensive and exhaustive summary and evaluation of the history, current status, and future of MARL while advancing important research results in the field of MARL to a deeper stage of development."},{"#name":"para","$":{"id":"d1e4755","view":"all"},"_":"Over time, more innovative approaches to MARL have emerged, especially algorithms in the field of MADRL, and as a result, there is a need for the emergence of more systematic and comprehensive reviews to fill in the gaps in this area and to update the classification of past algorithms. In this paper, we classify the algorithms in relative detail according to their functional characteristics and show the performance of different methods"},{"#name":"para","$":{"id":"d1e4757","view":"all"},"_":"The contribution of this paper can be summarized as follows."},{"#name":"para","$":{"id":"d1e4759","view":"all"},"$$":[{"#name":"list","$":{"id":"d1e4761"},"$$":[{"#name":"list-item","$":{"id":"lst1"},"$$":[{"#name":"label","_":"(1)"},{"#name":"para","$":{"id":"d1e4765","view":"all"},"_":"Agent collaboration algorithms are categorized from the perspective of task objects. This paper details the application of RL algorithms to both single agents and MAS with two task objects, highlights specific existing algorithms, and compares their strengths and weaknesses to help readers better understand the characteristics and development of certain types of algorithms."}]},{"#name":"list-item","$":{"id":"lst2"},"$$":[{"#name":"label","_":"(2)"},{"#name":"para","$":{"id":"d1e4770","view":"all"},"_":"The diverse applications of MARL algorithms in subtasks are explored. Focusing on an overview of the current phase of MARL algorithms and describing the challenges and their solutions in the field of MAS collaboration, this paper aims to help readers gain insights into the characteristics and future trends of various MARL algorithms."}]},{"#name":"list-item","$":{"id":"lst3"},"$$":[{"#name":"label","_":"(3)"},{"#name":"para","$":{"id":"d1e4775","view":"all"},"_":"The focus is on the study and outlook of the latest MADRL algorithms in subtasks. The paper summarizes the algorithms applicable to MARL, reveals the difficulties and some solutions in the field of MAS, and provides some innovative ideas for the future development of MARL."}]}]}]}]},{"#name":"section","$":{"id":"sec1.4","view":"all"},"$$":[{"#name":"label","_":"1.4"},{"#name":"section-title","$":{"id":"d1e4780"},"_":"Broad structure of the thesis"},{"#name":"para","$":{"id":"d1e4782","view":"all"},"_":"The rest of the paper is organized as follows, Section 2 gives a careful statistical overview of the literature on currently available algorithms and presents them visually employing CiteSpace graphing. Section 3 describes the traditional algorithms of MARL. In Section 4, some commonly used quality evaluation metrics in MARL algorithms are listed, existing MARL algorithms are categorized, and some existing and imperfect MARL algorithms are introduced. In Section 5, algorithms that have emerged and are not widely used are briefly discussed and presented. Section 6 summarizes the problems in the existing algorithms and suggests some future research directions."}]}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec2","view":"all"},"$$":[{"#name":"label","_":"2"},{"#name":"section-title","$":{"id":"d1e4813"},"_":"Citespace statistic"},{"#name":"para","$":{"id":"d1e4815","view":"all"},"_":"Since the introduction of MARL, it has been widely used in the field of artificial agents. As shown in the figure below, this paper searches the Web of Science Core Collection with the keywords of â€œmulti-agent systemâ€, â€œmulti-agent reinforcement learningâ€, â€œmulti-agent reinforcement learning algorithmâ€, etc., and obtains a total of 732 papers published in the past 20 years. Science Core Collection and obtained 732 MARL papers published in the past 20 years. On this basis, this paper produced a"}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec3","view":"all"},"$$":[{"#name":"label","_":"3"},{"#name":"section-title","$":{"id":"d1e4856"},"_":"RL conventional algorithm"},{"#name":"para","$":{"id":"d1e4858","view":"all"},"_":"Traditional RL algorithms describe the interaction between agents and the decision-making process by defining the state space, action space, reward function, and policy function. Different algorithms may use multiple learning methods and models to process this information, such as the two major research directions of Model-Free Reinforcement Learning(MFRL), and Model-Based Reinforcement Learning(MBRL). The common goal of these approaches is to bring the MAS to an overall optimal state, rather"}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec4","view":"all"},"$$":[{"#name":"label","_":"4"},{"#name":"section-title","$":{"id":"d1e6789"},"_":"MARL algorithm"},{"#name":"para","$":{"id":"d1e6791","view":"all"},"_":"In recent years, DL has been widely used in multi-agent collaboration techniques due to its excellent performance in big data processing, adaptive learning, and handling of diverse data types, as well as improving accuracy and scalability. Meanwhile, MARL algorithms have been rapidly developed."},{"#name":"section","$":{"id":"sec4.1","view":"all"},"$$":[{"#name":"label","_":"4.1"},{"#name":"section-title","$":{"id":"d1e6796"},"_":"Algorithms performance measures"},{"#name":"para","$":{"id":"d1e6798","view":"all"},"_":"MARL algorithm performance metrics can not only be used to evaluate the performance and applicability of MARL algorithms in different scenarios and help developers to improve the algorithms but also be"}]}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec5","role":"discussion","view":"all"},"$$":[{"#name":"label","_":"5"},{"#name":"section-title","$":{"id":"d1e8316"},"_":"Discussion"},{"#name":"para","$":{"id":"d1e8318","view":"all"},"_":"One of the key goals of artificial agents is to develop agents with excellent decision-making capabilities in complex and uncertain environments. In recent years, the rapid development of DQNs has enabled RL-based agents to show good performance in complex tasks, such as aircraft control, plant scheduling tasks [102], [103], and industrial process control in the field of autonomy control, etc. The advantages of RL over traditional control policies lie in its applicability to situations where"}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec6","view":"all"},"$$":[{"#name":"label","_":"6"},{"#name":"section-title","$":{"id":"d1e8410"},"_":"Summary and expectation"},{"#name":"section","$":{"id":"sec6.1","view":"all"},"$$":[{"#name":"label","_":"6.1"},{"#name":"section-title","$":{"id":"d1e8415"},"_":"Summary"},{"#name":"para","$":{"id":"d1e8417","view":"all"},"_":"After years of development and innovation, RL has received widespread attention as an important AI technology. In the field of single-agent reinforcement learning, many excellent algorithms have emerged. However, with the increase and complexity of practical application scenarios, the limitations of RL methods have gradually emerged, mainly in the inability to effectively deal with the problems of synergy or competition. The field of artificial intelligence needs to introduce new methods to"}]}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"d1e8509","view":"all"},"$$":[{"#name":"section-title","$":{"id":"d1e8510"},"_":"CRediT authorship contribution statement"},{"#name":"para","$":{"id":"d1e8512","view":"all"},"$$":[{"#name":"bold","_":"Kai Hu:"},{"#name":"__text__","_":" Supervision, Conceptualization, Methodology, Writing â€“ review & editing. "},{"#name":"bold","_":"Mingyang Li:"},{"#name":"__text__","_":" Data curation, Investigation, Validation, Writing â€“ original draft. "},{"#name":"bold","_":"Zhiqiang Song:"},{"#name":"__text__","_":" Methodology, Supervision, Validation, Writing â€“ review & editing. "},{"#name":"bold","_":"Keer Xu:"},{"#name":"__text__","_":" Data curation, Software, Visualization, Writing â€“ original draft. "},{"#name":"bold","_":"Qingfeng Xia:"},{"#name":"__text__","_":" Project administration, Validation, Writing â€“ original draft, Writing â€“ review & editing. "},{"#name":"bold","_":"Ning Sun:"},{"#name":"__text__","_":" Formal analysis, Investigation, Writing â€“ original draft. "},{"#name":"bold","_":"Peng Zhou:"}]}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"d1e8538","role":"author-disclosure","view":"all"},"$$":[{"#name":"section-title","$":{"id":"d1e8539"},"_":"Declaration of Generative AI and AI-assisted technologies in the writing process"},{"#name":"para","$":{"id":"d1e8541","view":"all"},"_":"During the preparation of this work the author (s) used [ChatGPT] in order to [improve language and readability]. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication."}]}]},{"#name":"section","$$":[{"#name":"conflict-of-interest","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"coi1","view":"all"},"$$":[{"#name":"section-title","$":{"id":"d1e8544"},"_":"Declaration of competing interest"},{"#name":"para","$":{"id":"d1e8546","view":"all"},"_":"The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}]}]},{"#name":"section","$$":[{"#name":"acknowledgment","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"d1e8548","view":"all"},"$$":[{"#name":"section-title","$":{"id":"d1e8549"},"_":"Acknowledgments"},{"#name":"para","$":{"id":"d1e8551","view":"all"},"$$":[{"#name":"__text__","_":"The research in this article is supported by the financial support of "},{"#name":"grant-sponsor","$":{"id":"GS1","role":"http://www.elsevier.com/xml/linking-roles/grant-sponsor","type":"simple"},"_":"Qing Lan Project of Jiangsu Province, China"},{"#name":"__text__","_":" is deeply appreciated. The authors would like to express heartfelt thanks to the reviewers and editors who submitted valuable revisions to this article."}]},{"#name":"section","$":{"id":"d1e8560","role":"funding","view":"all"},"$$":[{"#name":"section-title","$":{"id":"d1e8561"},"_":"Funding"},{"#name":"para","$":{"id":"d1e8563","view":"all"},"_":"The Research in this article is supported by the National Natural Science Foundation of China (42075130)."}]}]}]},{"#name":"section","$$":[{"#name":"biography","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio1","view":"all"},"$$":[{"#name":"link","$":{"id":"d1e14255","locator":"fx1","href":"pii:S0925231224008397/fx1","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"d1e14256","view":"all"},"$$":[{"#name":"bold","_":"Kai Hu"},{"#name":"__text__","_":", male, Ph.D, 1981.4. Associate professor and Doctoral Supervisor of School of Automation, Nanjing University of Information Science and Technology, China. His research interest areas include remote sensing, disturbed learning, action recognition, image restoration, and published more than 50 SCI(E) indexed papers."}]},{"#name":"simple-para","$":{"id":"d1e14260","view":"all"},"_":"ORCID: 0000-0001-7181-9935."},{"#name":"simple-para","$":{"id":"d1e14262","view":"all"},"$$":[{"#name":"__text__","_":"Email: "},{"#name":"inter-ref","$":{"id":"interref7","href":"mailto:001600@nuist.edu.cn","type":"simple"},"_":"001600@nuist.edu.cn"},{"#name":"__text__","_":", "},{"#name":"inter-ref","$":{"id":"interref8","href":"mailto:nuistpanda@163.com","type":"simple"},"_":"nuistpanda@163.com"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"rawtext":"","recommendations":{"articles":[{"pii":"S0925231224008774","doi":"10.1016/j.neucom.2024.128106","journalTitle":"Neurocomputing","publicationYear":"2024","publicationDate":"2024-09-28","volumeSupText":"Volume 599","articleNumber":"128106","pageRange":"128106","trace-token":"AAAAQG7si4QdblTLYePFa8BDqz5IWct_5yxJEQWZHLuqhZfHF8q720Md2ApFBK0HT6gHV2qz8R4mpOqrKg05opD2zKHyckCNUIn9dIlAcNpfyiOneP6rbw","authors":{"content":[{"#name":"author-group","$":{"id":"ag0005"},"$$":[{"#name":"author","$":{"id":"au0005","author-id":"S0925231224008774-9d7f4e89fca9436c1a8a5df03fabb3dc","biographyid":"bio1","orcid":"0000-0003-0937-049X"},"$$":[{"#name":"given-name","_":"Xiao"},{"#name":"surname","_":"Shen"},{"#name":"cross-ref","$":{"id":"cr0015","refid":"aff0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"eadd0005","type":"email","href":"mailto:xshen@hainanu.edu.cn"},"_":"xshen@hainanu.edu.cn"}]},{"#name":"author","$":{"id":"au0010","author-id":"S0925231224008774-94528743c072c24464fc80fdd9459865","biographyid":"bio2","orcid":"0000-0003-0836-7088"},"$$":[{"#name":"given-name","_":"Kup-Sze"},{"#name":"surname","_":"Choi"},{"#name":"cross-ref","$":{"id":"cr0020","refid":"aff0010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"eadd0010","type":"email","href":"mailto:kschoi@ieee.org"},"_":"kschoi@ieee.org"}]},{"#name":"author","$":{"id":"au0015","author-id":"S0925231224008774-4c5f660b6f55eda222d35775cf519244","biographyid":"bio3","orcid":"0000-0002-6943-5585"},"$$":[{"#name":"given-name","_":"Xi"},{"#name":"surname","_":"Zhou"},{"#name":"cross-ref","$":{"id":"cr0025","refid":"aff0015"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"cross-ref","$":{"id":"cr0030","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"eadd0015","type":"email","href":"mailto:xzhou@hainanu.edu.cn"},"_":"xzhou@hainanu.edu.cn"}]},{"#name":"affiliation","$":{"id":"aff0005","affiliation-id":"S0925231224008774-bcbe2b178b5ba43f3eb60f1205d2056a"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"School of Computer Science and Technology, Hainan University,Â Haikou 570228,Â China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Computer Science and Technology, Hainan University"},{"#name":"city","_":"Haikou"},{"#name":"postal-code","_":"570228"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"srct0010"},"_":"School of Computer Science and Technology, Hainan University, Haikou, 570228 China"}]},{"#name":"affiliation","$":{"id":"aff0010","affiliation-id":"S0925231224008774-e8ec222c2a09862c0eb4e3794ba795ef"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"TechCosmos Limited, Hong Kong, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"TechCosmos Limited"},{"#name":"city","_":"Hong Kong"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"srct0015"},"_":"Centre for Smart Health, The Hong Kong Polytechnic University, Hong Kong, China"}]},{"#name":"affiliation","$":{"id":"aff0015","affiliation-id":"S0925231224008774-09c8f2602b9489adacf0f1021d0e07a2"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"School of Tropical Agriculture and Forestry, Hainan University, Haikou 570228, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Tropical Agriculture and Forestry, Hainan University"},{"#name":"city","_":"Haikou"},{"#name":"postal-code","_":"570228"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"srct0020"},"_":"College of Tropical Crops, Hainan University, Haikou, 570228 China"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au0015","author-id":"S0925231224008774-4c5f660b6f55eda222d35775cf519244","biographyid":"bio3","orcid":"0000-0002-6943-5585"},"$$":[{"#name":"given-name","_":"Xi"},{"#name":"surname","_":"Zhou"},{"#name":"cross-ref","$":{"id":"cr0025","refid":"aff0015"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"cross-ref","$":{"id":"cr0030","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"eadd0015","type":"email","href":"mailto:xzhou@hainanu.edu.cn"},"_":"xzhou@hainanu.edu.cn"}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"tit0005"},"_":"Dual separated attention-based graph neural network"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","pdf":{"urlType":null},"iss-first":"","vol-first":"599","isThirdParty":false,"language":"en","issn-primary-unformatted":"09252312","issn-primary-formatted":"0925-2312"},{"pii":"S0925231224008312","doi":"10.1016/j.neucom.2024.128060","journalTitle":"Neurocomputing","publicationYear":"2024","publicationDate":"2024-09-14","volumeSupText":"Volume 598","articleNumber":"128060","pageRange":"128060","trace-token":"AAAAQG7si4QdblTLYePFa8BDqz5IWct_5yxJEQWZHLuqhZfH3CsoKIoA5QdhktPuQjRGunUtfmYZvKhxTNJNNT7cIZWBLxHoOXy1ohUxZt9UepQ47LQjug","authors":{"content":[{"#name":"author-group","$":{"id":"d1e298"},"$$":[{"#name":"author","$":{"author-id":"S0925231224008312-549b9faa55b8dce4c584639b01ac60f3","biographyid":"bio1","id":"au000001","orcid":"0009-0008-9320-3347"},"$$":[{"#name":"given-name","_":"Hannes"},{"#name":"surname","_":"KÃ¶hler"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/visualization"},"_":"Visualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea1","type":"email","href":"mailto:hannes.koehler@uni-bayreuth.de"},"_":"hannes.koehler@uni-bayreuth.de"}]},{"#name":"affiliation","$":{"affiliation-id":"S0925231224008312-f4affd9908c42f3a228fb682a2fc0963","id":"aff1"},"$$":[{"#name":"textfn","_":"Department of Mathematics, University of Bayreuth, 95440 Bayreuth, Germany"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Mathematics, University of Bayreuth"},{"#name":"city","_":"Bayreuth"},{"#name":"postal-code","_":"95440"},{"#name":"country","_":"Germany"}]},{"#name":"source-text","$":{"id":"afs74"},"_":"organization=Department of Mathematics, University of Bayreuth, addressline=95440 Bayreuth, country=Germany"}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"author-id":"S0925231224008312-549b9faa55b8dce4c584639b01ac60f3","biographyid":"bio1","id":"au000001","orcid":"0009-0008-9320-3347"},"$$":[{"#name":"given-name","_":"Hannes"},{"#name":"surname","_":"KÃ¶hler"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/visualization"},"_":"Visualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea1","type":"email","href":"mailto:hannes.koehler@uni-bayreuth.de"},"_":"hannes.koehler@uni-bayreuth.de"}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"d1e295"},"_":"Lp- and risk consistency of localized SVMs"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":true,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","pdf":{"urlType":null},"iss-first":"","vol-first":"598","isThirdParty":false,"language":"en","issn-primary-unformatted":"09252312","issn-primary-formatted":"0925-2312"},{"pii":"S0020025524009472","doi":"10.1016/j.ins.2024.121033","journalTitle":"Information Sciences","publicationYear":"2024","publicationDate":"2024-09-01","volumeSupText":"Volume 679","articleNumber":"121033","pageRange":"121033","trace-token":"AAAAQG7si4QdblTLYePFa8BDqz4zSkwJWt4aGoNustaiDu1Fh3Zb6kjgjMIdQlKtdiPVtbADnGkjd_oaZaL9TSRaAxY4VvJG2nwevD6c667OX5b2ueOjxQ","authors":{"content":[{"#name":"author-group","$":{"id":"ag0010"},"$$":[{"#name":"author","$":{"id":"au0010","author-id":"S0020025524009472-eb321705ef51ad288e6a2614795ab8bb"},"$$":[{"#name":"given-name","_":"Kangjia"},{"#name":"surname","_":"Qiao"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"cross-ref","$":{"refid":"aff0010","id":"crf0030"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au0020","author-id":"S0020025524009472-db9dacf32e1c54e4a05d548ed01f5f83"},"$$":[{"#name":"given-name","_":"Jing"},{"#name":"surname","_":"Liang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"cross-ref","$":{"refid":"aff0010","id":"crf0040"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"aff0020","id":"crf0050"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"refid":"cr0010","id":"crf0060"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"type":"email","href":"mailto:liangjing@zzu.edu.cn","id":"ea0010"},"_":"liangjing@zzu.edu.cn"}]},{"#name":"author","$":{"id":"au0030","author-id":"S0020025524009472-1a5f1d66a5fdd8120429d4a9caf7270f"},"$$":[{"#name":"given-name","_":"Wei-Feng"},{"#name":"surname","_":"Guo"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"cross-ref","$":{"refid":"aff0010","id":"crf0070"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"cr0010","id":"crf0080"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"type":"email","href":"mailto:guowf@zzu.edu.cn","id":"ea0020"},"_":"guowf@zzu.edu.cn"}]},{"#name":"author","$":{"id":"au0040","author-id":"S0020025524009472-adf8b89968c0489b29cb1f46dee2621c"},"$$":[{"#name":"given-name","_":"Zhuo"},{"#name":"surname","_":"Hu"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"cross-ref","$":{"refid":"aff0010","id":"crf0090"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au0050","author-id":"S0020025524009472-c73431f0bdf1aa60ef35ae2f67c2ca9b"},"$$":[{"#name":"given-name","_":"Kunjie"},{"#name":"surname","_":"Yu"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/visualization"},"_":"Visualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"cross-ref","$":{"refid":"aff0010","id":"crf0100"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au0060","author-id":"S0020025524009472-5cbf4b944079db798ccc0a5e2390e78d"},"$$":[{"#name":"given-name","_":"P.N."},{"#name":"surname","_":"Suganthan"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"cross-ref","$":{"refid":"aff0030","id":"crf0110"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]}]},{"#name":"affiliation","$":{"id":"aff0010","affiliation-id":"S0020025524009472-49e9456eb3bbad6aada9de4879434202"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"School of Electrical and Information Engineering, Zhengzhou University, Zhengzhou, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Electrical and Information Engineering"},{"#name":"organization","_":"Zhengzhou University"},{"#name":"city","_":"Zhengzhou"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"srct0005"},"_":"School of Electrical and Information Engineering, Zhengzhou University, Zhengzhou, China"}]},{"#name":"affiliation","$":{"id":"aff0020","affiliation-id":"S0020025524009472-96139184488310f4fb5e6d513083f7fa"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"School of Electrical Engineering and Automation, Henan Institute of Technology, Xinxiang, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Electrical Engineering and Automation"},{"#name":"organization","_":"Henan Institute of Technology"},{"#name":"city","_":"Xinxiang"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"srct0010"},"_":"School of Electrical Engineering and Automation, Henan Institute of Technology, Xinxiang, China"}]},{"#name":"affiliation","$":{"id":"aff0030","affiliation-id":"S0020025524009472-8a9fad8f84bfa39dd809a545dfd6023d"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"KINDI Center for Computing Research, College of Engineering, Qatar University, Doha, Qatar"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"KINDI Center for Computing Research"},{"#name":"organization","_":"College of Engineering"},{"#name":"organization","_":"Qatar University"},{"#name":"city","_":"Doha"},{"#name":"country","_":"Qatar"}]},{"#name":"source-text","$":{"id":"srct0015"},"_":"KINDI Center for Computing Research, College of Engineering, Qatar University, Doha, Qatar"}]},{"#name":"correspondence","$":{"id":"cr0010"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","_":"Corresponding authors at: School of Electrical and Information Engineering, Zhengzhou University, Zhengzhou, China."},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Electrical and Information Engineering"},{"#name":"organization","_":"Zhengzhou University"},{"#name":"city","_":"Zhengzhou"},{"#name":"country","_":"China"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au0060","author-id":"S0020025524009472-5cbf4b944079db798ccc0a5e2390e78d"},"$$":[{"#name":"given-name","_":"P.N."},{"#name":"surname","_":"Suganthan"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"cross-ref","$":{"refid":"aff0030","id":"crf0110"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"ti0010"},"_":"Knowledge-embedded constrained multiobjective evolutionary algorithm based on structural network control principles for personalized drug targets recognition in cancer"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","pdf":{"urlType":null},"iss-first":"","vol-first":"679","isThirdParty":false,"language":"en","issn-primary-unformatted":"00200255","issn-primary-formatted":"0020-0255"},{"pii":"S0957417424012922","doi":"10.1016/j.eswa.2024.124426","journalTitle":"Expert Systems with Applications","publicationYear":"2024","publicationDate":"2024-12-01","volumeSupText":"Volume 255, Part A","articleNumber":"124426","pageRange":"124426","trace-token":"AAAAQG7si4QdblTLYePFa8BDqz6Eg7gV3KtFspis5eDZm_6g_sOHFZrYWJ3jmmSkjdOMSpySSDUF1sjCDHzbrNqZHmAXpBtEVcucxOCg7qcI_09rUpXhqQ","authors":{"content":[{"#name":"author-group","$":{"id":"d1e518"},"$$":[{"#name":"author","$":{"author-id":"S0957417424012922-3b2a095a2c1a3711c931966867d18649","id":"au000001","orcid":"0000-0003-2071-0272"},"$$":[{"#name":"given-name","_":"Farzad"},{"#name":"surname","_":"Hosseinali"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea1","type":"email","href":"mailto:f.hosseinali@gwu.edu"},"_":"f.hosseinali@gwu.edu"}]},{"#name":"affiliation","$":{"affiliation-id":"S0957417424012922-05faf501897947d1e095f7252e7ad126","id":"aff1"},"$$":[{"#name":"textfn","_":"Department of Data Science, The George Washington University, Washington, DC, 20052, USA"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Data Science, The George Washington University"},{"#name":"city","_":"Washington"},{"#name":"state","_":"DC"},{"#name":"postal-code","_":"20052"},{"#name":"country","_":"USA"}]},{"#name":"source-text","$":{"id":"afs23"},"_":"Department of Data Science, The George Washington University, Washington, DC, 20052, USA"}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"author-id":"S0957417424012922-3b2a095a2c1a3711c931966867d18649","id":"au000001","orcid":"0000-0003-2071-0272"},"$$":[{"#name":"given-name","_":"Farzad"},{"#name":"surname","_":"Hosseinali"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea1","type":"email","href":"mailto:f.hosseinali@gwu.edu"},"_":"f.hosseinali@gwu.edu"}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"d1e515"},"_":"Accelerated gradient descent using improved Selective Backpropagation"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","pdf":{"urlType":null},"iss-first":"","vol-first":"255","isThirdParty":false,"language":"en","issn-primary-unformatted":"09574174","issn-primary-formatted":"0957-4174"},{"pii":"S0925231224008452","doi":"10.1016/j.neucom.2024.128074","journalTitle":"Neurocomputing","publicationYear":"2024","publicationDate":"2024-09-28","volumeSupText":"Volume 599","articleNumber":"128074","pageRange":"128074","trace-token":"AAAAQG7si4QdblTLYePFa8BDqz5IWct_5yxJEQWZHLuqhZfHgh49aTySfr5Ce_hO6z8xTIBCHkyA8oPKpbj-GqK79sO2_HsChH8jpYWNDnOeXNC-1rLwsg","authors":{"content":[{"#name":"author-group","$":{"id":"d1e166"},"$$":[{"#name":"author","$":{"author-id":"S0925231224008452-3efce036b6b04c802c92507f9b02a9e4","biographyid":"bio1","id":"au000001","orcid":"0009-0000-1324-2031"},"$$":[{"#name":"given-name","_":"Xiaoning"},{"#name":"surname","_":"Lv"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing â€“ original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea1","type":"email","href":"mailto:xiaoning_lv@163.com"},"_":"xiaoning_lv@163.com"}]},{"#name":"author","$":{"author-id":"S0925231224008452-2b24a5351029c2c42f1742dc9a08912a","biographyid":"bio2","id":"au000002","orcid":"0000-0003-0153-3223"},"$$":[{"#name":"given-name","_":"Wei"},{"#name":"surname","_":"Wei"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea2","type":"email","href":"mailto:weiwei_wlj@163.com"},"_":"weiwei_wlj@163.com"}]},{"#name":"author","$":{"author-id":"S0925231224008452-d1b04ea53cfff5e9217cef6f3963b14d","biographyid":"bio3","id":"au000003"},"$$":[{"#name":"given-name","_":"Weihai"},{"#name":"surname","_":"Zhang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/funding-acquisition"},"_":"Funding acquisition"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"cross-ref","$":{"id":"d1e214","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea3","type":"email","href":"mailto:w_hzhang@163.com"},"_":"w_hzhang@163.com"}]},{"#name":"affiliation","$":{"affiliation-id":"S0925231224008452-a842dd17845789160eac468938e14097","id":"aff1"},"$$":[{"#name":"textfn","_":"College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao 266590, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"College of Electrical Engineering and Automation, Shandong University of Science and Technology"},{"#name":"city","_":"Qingdao"},{"#name":"postal-code","_":"266590"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"afs44"},"_":"College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao 266590, China"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"author-id":"S0925231224008452-d1b04ea53cfff5e9217cef6f3963b14d","biographyid":"bio3","id":"au000003"},"$$":[{"#name":"given-name","_":"Weihai"},{"#name":"surname","_":"Zhang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing â€“ review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/funding-acquisition"},"_":"Funding acquisition"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"cross-ref","$":{"id":"d1e214","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea3","type":"email","href":"mailto:w_hzhang@163.com"},"_":"w_hzhang@163.com"}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"article-footnote","$":{"id":"aep-article-footnote-id1"},"$$":[{"#name":"label","_":"â˜†"},{"#name":"note-para","$":{"id":"d1e151","view":"all"},"$$":[{"#name":"__text__","_":"This work was supported by the "},{"#name":"grant-sponsor","$":{"xmlns:xlink":true,"id":"GS1","role":"http://www.elsevier.com/xml/linking-roles/grant-sponsor","type":"simple"},"_":"National Natural Science Foundation of China"},{"#name":"__text__","_":" under Grant No. "},{"#name":"grant-number","$":{"id":"d1e160","refid":"GS1"},"_":"62373229"},{"#name":"__text__","_":"."}]}]},{"#name":"title","$":{"id":"d1e163"},"_":"Neuroadaptive nonsingular fixed-time tracking control for deferred state-constrained systems based on improved command filter"}],"floats":[],"footnotes":[{"#name":"article-footnote","$":{"id":"aep-article-footnote-id1"},"$$":[{"#name":"label","_":"â˜†"},{"#name":"note-para","$":{"id":"d1e151","view":"all"},"$$":[{"#name":"__text__","_":"This work was supported by the "},{"#name":"grant-sponsor","$":{"xmlns:xlink":true,"id":"GS1","role":"http://www.elsevier.com/xml/linking-roles/grant-sponsor","type":"simple"},"_":"National Natural Science Foundation of China"},{"#name":"__text__","_":" under Grant No. "},{"#name":"grant-number","$":{"id":"d1e160","refid":"GS1"},"_":"62373229"},{"#name":"__text__","_":"."}]}]}],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","pdf":{"urlType":null},"iss-first":"","vol-first":"599","isThirdParty":false,"language":"en","issn-primary-unformatted":"09252312","issn-primary-formatted":"0925-2312"},{"pii":"S2214785322044170","doi":"10.1016/j.matpr.2022.06.399","journalTitle":"Materials Today: Proceedings","publicationYear":"2022","publicationDate":"2022-01-01","volumeSupText":"Volume 66, Part 6","articleNumber":"","pageRange":"3174-3177","trace-token":"AAAAQG7si4QdblTLYePFa8BDqz6IbQBmxiDMRKmHvjcaMAGOx06-0JsPcvSD8MGz2XKtIICepXWhBhjWBd-hEo1wdY6kjDOXwHxl-GEff7IV7MG1XojPsQ","authors":{"content":[{"#name":"author-group","$":{"id":"ag005"},"$$":[{"#name":"author","$":{"id":"au005","author-id":"S2214785322044170-9e364f4ce69520416d05e9bce5d9f685"},"$$":[{"#name":"given-name","_":"Nonthanan"},{"#name":"surname","_":"Sitpathom"},{"#name":"cross-ref","$":{"refid":"af005","id":"c0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"af010","id":"c0010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"refid":"cor1","id":"c0015"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"type":"email","href":"mailto:nonthanan.sit@kmutt.ac.th","id":"em005"},"_":"nonthanan.sit@kmutt.ac.th"}]},{"#name":"author","$":{"id":"au010","author-id":"S2214785322044170-7452423e2be09b6021b831ff7ace9745"},"$$":[{"#name":"given-name","_":"Tanyakorn"},{"#name":"surname","_":"Muangnapoh"},{"#name":"cross-ref","$":{"refid":"af015","id":"c0020"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]}]},{"#name":"author","$":{"id":"au015","author-id":"S2214785322044170-c7cd07e3624a58b9e79e76216e9a14c4"},"$$":[{"#name":"given-name","_":"Pisist"},{"#name":"surname","_":"Kumnorkaew"},{"#name":"cross-ref","$":{"refid":"af015","id":"c0025"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]}]},{"#name":"author","$":{"id":"au020","author-id":"S2214785322044170-011d65112a1db6260d711ac5e0f2eab4"},"$$":[{"#name":"given-name","_":"Sujin"},{"#name":"surname","_":"Suwanna"},{"#name":"cross-ref","$":{"refid":"af005","id":"c0030"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au025","author-id":"S2214785322044170-1d17d1509f30123059b2baa9c4aef463"},"$$":[{"#name":"given-name","_":"Asawin"},{"#name":"surname","_":"Sinsarp"},{"#name":"cross-ref","$":{"refid":"af005","id":"c0035"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au030","author-id":"S2214785322044170-2dcd74b051f7faa930a3ef273d376cba"},"$$":[{"#name":"given-name","_":"Tanakorn"},{"#name":"surname","_":"Osotchan"},{"#name":"cross-ref","$":{"refid":"af005","id":"c0040"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"affiliation","$":{"id":"af005","affiliation-id":"S2214785322044170-487980884b34ef89676058098a4358b3"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Department of Physics, Faculty of Science, Mahidol University, Bangkok 10400, Thailand"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Physics"},{"#name":"organization","_":"Faculty of Science"},{"#name":"organization","_":"Mahidol University"},{"#name":"city","_":"Bangkok"},{"#name":"postal-code","_":"10400"},{"#name":"country","_":"Thailand"}]},{"#name":"source-text","$":{"id":"stx005"},"_":"Department of Physics, Faculty of Science, Mahidol University, Bangkok 10400, Thailand"}]},{"#name":"affiliation","$":{"id":"af010","affiliation-id":"S2214785322044170-1da31c7879b8691f062f544d627098b5"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Department of Physics, Faculty of Science, King Mongkuttâ€™s University of Technology Thonburi, Bangkok 10140, Thailand"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Physics"},{"#name":"organization","_":"Faculty of Science"},{"#name":"organization","_":"King Mongkuttâ€™s University of Technology Thonburi"},{"#name":"city","_":"Bangkok"},{"#name":"postal-code","_":"10140"},{"#name":"country","_":"Thailand"}]},{"#name":"source-text","$":{"id":"stx010"},"_":"Department of Physics, Faculty of Science, King Mongkuttâ€™s University of Technology Thonburi, Bangkok 10140, Thailand"}]},{"#name":"affiliation","$":{"id":"af015","affiliation-id":"S2214785322044170-282bcb5e5cd3836ef1f175d293b69cb8"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"National Nanotechnology Center, National Science and Technology Development Agency, Pathum Thani 12120, Thailand"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"National Nanotechnology Center"},{"#name":"organization","_":"National Science and Technology Development Agency"},{"#name":"state","_":"Pathum Thani"},{"#name":"postal-code","_":"12120"},{"#name":"country","_":"Thailand"}]},{"#name":"source-text","$":{"id":"stx015"},"_":"National Nanotechnology Center, National Science and Technology Development Agency, Pathum Thani 12120, Thailand"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au030","author-id":"S2214785322044170-2dcd74b051f7faa930a3ef273d376cba"},"$$":[{"#name":"given-name","_":"Tanakorn"},{"#name":"surname","_":"Osotchan"},{"#name":"cross-ref","$":{"refid":"af005","id":"c0040"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"tm005"},"_":"Photonic density of states and photonic bandgap of deformed titanium dioxide inverse opal structure"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","pdf":{"urlType":null},"iss-first":"","vol-first":"66","isThirdParty":false,"language":"en","issn-primary-unformatted":"22147853","issn-primary-formatted":"2214-7853"}]},"references":{"content":[{"#name":"bibliography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bib1","view":"all"},"$$":[{"#name":"section-title","$":{"id":"d1e8583"},"_":"References"},{"#name":"bibliography-sec","$":{"id":"d1e8585","view":"all"},"$$":[{"#name":"bib-reference","$":{"id":"b1"},"$$":[{"#name":"label","_":"[1]"},{"#name":"reference","$":{"id":"sb1","refId":"1"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Weiss"},{"#name":"given-name","_":"G."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multiagent Systems: A Modern Approach To Distributed Artificial Intelligence"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"1999"},{"#name":"publisher","$$":[{"#name":"name","_":"MIT Press"}]}]}]}]},{"#name":"source-text","$":{"id":"afs1"},"_":"G. Weiss, Multiagent systems: a modern approach to distributed artificial intelligence, MIT press, 0 (1999) 0."}]},{"#name":"bib-reference","$":{"id":"b2"},"$$":[{"#name":"label","_":"[2]"},{"#name":"reference","$":{"id":"sb2","refId":"2"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Gronauer"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Diepold"},{"#name":"given-name","_":"K."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep reinforcement learning: A survey"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Artif. Intell. Rev."}]}]},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"49"}]}]}]},{"#name":"source-text","$":{"id":"afs2"},"_":"S. Gronauer, K. Diepold, Multi-agent deep reinforcement learning: a survey, Artificial Intelligence Review 0 (2022) 1â€“49."}]},{"#name":"bib-reference","$":{"id":"b3"},"$$":[{"#name":"label","_":"[3]"},{"#name":"reference","$":{"id":"sb3","refId":"3"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"E."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xia"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Weng"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lin"},{"#name":"given-name","_":"H."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Mcanet: A multi-branch network for cloud/snow segmentation in high-resolution remote sensing images"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Remote Sens."}]},{"#name":"volume-nr","_":"15"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1055"}]}]}]},{"#name":"source-text","$":{"id":"afs3"},"_":"K. Hu, E. Zhang, M. Xia, L. Weng, H. Lin, Mcanet: a multi-branch network for cloud/snow segmentation in high-resolution remote sensing images, Remote Sensing 15 (4) (2023) 1055."}]},{"#name":"bib-reference","$":{"id":"b4"},"$$":[{"#name":"label","_":"[4]"},{"#name":"reference","$":{"id":"sb4","refId":"4"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Shen"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Weng"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhou"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xia"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Weng"},{"#name":"given-name","_":"L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Overview of underwater 3d reconstruction technology based on optical images"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"J. Mar. Sci. Eng."}]},{"#name":"volume-nr","_":"11"}]},{"#name":"issue-nr","_":"5"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"949"}]}]}]},{"#name":"source-text","$":{"id":"afs4"},"_":"K. Hu, T. Wang, C. Shen, C. Weng, F. Zhou, M. Xia, L. Weng, Overview of underwater 3d reconstruction technology based on optical images, Journal of Marine Science and Engineering 11 (5) (2023) 949."}]},{"#name":"bib-reference","$":{"id":"b5"},"$$":[{"#name":"label","_":"[5]"},{"#name":"other-ref","$":{"id":"sb5","refId":"5"},"$$":[{"#name":"textref","_":"M. Tan, Multi-agent reinforcement learning: Independent vs. cooperative agents, in: Proceedings of the Tenth International Conference on Machine Learning, 1993, pp. 330â€“337."}]}]},{"#name":"bib-reference","$":{"id":"b6"},"$$":[{"#name":"label","_":"[6]"},{"#name":"reference","$":{"id":"sb6","refId":"6"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Claus"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Boutilier"},{"#name":"given-name","_":"C."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"The dynamics of reinforcement learning in cooperative multiagent systems"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"AAAI/IAAI"}]},{"#name":"volume-nr","_":"1998"}]},{"#name":"issue-nr","_":"746â€“752"},{"#name":"date","_":"1998"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"2"}]}]}]},{"#name":"source-text","$":{"id":"afs6"},"_":"C. Claus, C. Boutilier, The dynamics of reinforcement learning in cooperative multiagent systems, AAAI/IAAI 1998 (746-752) (1998) 2."}]},{"#name":"bib-reference","$":{"id":"b7"},"$$":[{"#name":"label","_":"[7]"},{"#name":"reference","$":{"id":"sb7","refId":"7"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Stone"},{"#name":"given-name","_":"P."}]},{"#name":"author","$$":[{"#name":"surname","_":"Veloso"},{"#name":"given-name","_":"M."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multiagent systems: A survey from a machine learning perspective"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Auton. Robots"}]},{"#name":"volume-nr","_":"8"}]},{"#name":"date","_":"2000"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"345"},{"#name":"last-page","_":"383"}]}]}]},{"#name":"source-text","$":{"id":"afs7"},"_":"P. Stone, M. Veloso, Multiagent systems: A survey from a machine learning perspective, Autonomous Robots 8 (2000) 345â€“383."}]},{"#name":"bib-reference","$":{"id":"b8"},"$$":[{"#name":"label","_":"[8]"},{"#name":"reference","$":{"id":"sb8","refId":"8"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Shoham"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Powers"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Grenager"},{"#name":"given-name","_":"T."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-Agent Reinforcement Learning: A Critical Survey"},{"#name":"subtitle","_":"Tech. rep."}]}]},{"#name":"host","$$":[{"#name":"book","$":{"class":"report"},"$$":[{"#name":"date","_":"2003"},{"#name":"publisher","$$":[{"#name":"name","_":"Citeseer"}]}]}]}]},{"#name":"source-text","$":{"id":"afs8"},"_":"Y. Shoham, R. Powers, T. Grenager, Multi-agent reinforcement learning: a critical survey, Tech. rep., Citeseer (2003)."}]},{"#name":"bib-reference","$":{"id":"b9"},"$$":[{"#name":"label","_":"[9]"},{"#name":"reference","$":{"id":"sb9","refId":"9"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hoen"},{"#name":"given-name","_":"P.J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Tuyls"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Panait"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Luke"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"La Poutre"},{"#name":"given-name","_":"J.A."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"An overview of cooperative and competitive multiagent learning"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Learning and Adaption in Multi-Agent Systems: First International Workshop, LAMAS 2005, Utrecht, the Netherlands, July 25 2005, Revised Selected Papers"}]},{"#name":"date","_":"2006"},{"#name":"publisher","$$":[{"#name":"name","_":"Springer"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"46"}]}]}]},{"#name":"source-text","$":{"id":"afs9"},"_":"P. J. Hoen, K. Tuyls, L. Panait, S. Luke, J. A. La Poutre, An overview of cooperative and competitive multiagent learning, in: Learning and Adaption in Multi-Agent Systems: First International Workshop, LAMAS 2005, Utrecht, The Netherlands, July 25 2005, Revised Selected Papers, Springer, 0 (2006) 1â€“46."}]},{"#name":"bib-reference","$":{"id":"b10"},"$$":[{"#name":"label","_":"[10]"},{"#name":"reference","$":{"id":"sb10","refId":"10"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Busoniu"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Babuska"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"De Schutter"},{"#name":"given-name","_":"B."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A comprehensive survey of multiagent reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Syst. Man Cybern. C (Appl. Rev.)"}]},{"#name":"volume-nr","_":"38"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2008"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"156"},{"#name":"last-page","_":"172"}]}]}]},{"#name":"source-text","$":{"id":"afs10"},"_":"L. Busoniu, R. Babuska, B. De Schutter, A comprehensive survey of multiagent reinforcement learning, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 38 (2) (2008) 156â€“172."}]},{"#name":"bib-reference","$":{"id":"b11"},"$$":[{"#name":"label","_":"[11]"},{"#name":"reference","$":{"id":"sb11","refId":"11"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Matignon"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Laurent"},{"#name":"given-name","_":"G.J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Le Fort-Piat"},{"#name":"given-name","_":"N."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Independent reinforcement learners in cooperative markov games: A survey regarding coordination problems"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Knowl. Eng. Rev."}]},{"#name":"volume-nr","_":"27"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2012"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"31"}]}]}]},{"#name":"source-text","$":{"id":"afs11"},"_":"L. Matignon, G. J. Laurent, N. Le Fort-Piat, Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems, The Knowledge Engineering Review 27 (1) (2012) 1â€“31."}]},{"#name":"bib-reference","$":{"id":"b12"},"$$":[{"#name":"label","_":"[12]"},{"#name":"reference","$":{"id":"sb12","refId":"12"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Tuyls"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Weiss"},{"#name":"given-name","_":"G."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multiagent learning: Basics, challenges, and prospects"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"AI Mag."}]},{"#name":"volume-nr","_":"33"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2012"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"41"}]}]}]},{"#name":"source-text","$":{"id":"afs12"},"_":"K. Tuyls, G. Weiss, Multiagent learning: Basics, challenges, and prospects, Ai Magazine 33 (3) (2012) 41â€“41."}]},{"#name":"bib-reference","$":{"id":"b13"},"$$":[{"#name":"label","_":"[13]"},{"#name":"reference","$":{"id":"sb13","refId":"13"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Baker"},{"#name":"given-name","_":"B."}]},{"#name":"author","$$":[{"#name":"surname","_":"Kanitscheider"},{"#name":"given-name","_":"I."}]},{"#name":"author","$$":[{"#name":"surname","_":"Markov"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Powell"},{"#name":"given-name","_":"G."}]},{"#name":"author","$$":[{"#name":"surname","_":"McGrew"},{"#name":"given-name","_":"B."}]},{"#name":"author","$$":[{"#name":"surname","_":"Mordatch"},{"#name":"given-name","_":"I."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Emergent tool use from multi-agent autocurricula"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2019"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref1","href":"arxiv:1909.07528","role":"http://www.elsevier.com/xml/linking-roles/preprint","type":"simple"},"_":"arXiv:1909.07528"}]}]},{"#name":"source-text","$":{"id":"afs13"},"_":"B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, I. Mordatch, Emergent tool use from multi-agent autocurricula, arXiv preprint arXiv:1909.07528 (2019)."}]},{"#name":"bib-reference","$":{"id":"b14"},"$$":[{"#name":"label","_":"[14]"},{"#name":"reference","$":{"id":"sb14","refId":"14"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Berner"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Brockman"},{"#name":"given-name","_":"G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chan"},{"#name":"given-name","_":"B."}]},{"#name":"author","$$":[{"#name":"surname","_":"Cheung"},{"#name":"given-name","_":"V."}]},{"#name":"author","$$":[{"#name":"surname","_":"DÄ™biak"},{"#name":"given-name","_":"P."}]},{"#name":"author","$$":[{"#name":"surname","_":"Dennison"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Farhi"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Fischer"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hashme"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hesse"},{"#name":"given-name","_":"C."}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Dota 2 with large scale deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2019"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref2","href":"arxiv:1912.06680","role":"http://www.elsevier.com/xml/linking-roles/preprint","type":"simple"},"_":"arXiv:1912.06680"}]}]},{"#name":"source-text","$":{"id":"afs14"},"_":"C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, others, Dota 2 with large scale deep reinforcement learning, arXiv preprint arXiv:1912.06680 (2019)."}]},{"#name":"bib-reference","$":{"id":"b15"},"$$":[{"#name":"label","_":"[15]"},{"#name":"reference","$":{"id":"sb15","refId":"15"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Jaderberg"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Czarnecki"},{"#name":"given-name","_":"W.M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Dunning"},{"#name":"given-name","_":"I."}]},{"#name":"author","$$":[{"#name":"surname","_":"Marris"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lever"},{"#name":"given-name","_":"G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Castaneda"},{"#name":"given-name","_":"A.G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Beattie"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Rabinowitz"},{"#name":"given-name","_":"N.C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Morcos"},{"#name":"given-name","_":"A.S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Ruderman"},{"#name":"given-name","_":"A."}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Human-level performance in 3d multiplayer games with population-based reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Science"}]},{"#name":"volume-nr","_":"364"}]},{"#name":"issue-nr","_":"6443"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"859"},{"#name":"last-page","_":"865"}]}]}]},{"#name":"source-text","$":{"id":"afs15"},"_":"M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, others, Human-level performance in 3d multiplayer games with population-based reinforcement learning, Science 364 (6443) (2019) 859â€“865."}]},{"#name":"bib-reference","$":{"id":"b16"},"$$":[{"#name":"label","_":"[16]"},{"#name":"reference","$":{"id":"sb16","refId":"16"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Vinyals"},{"#name":"given-name","_":"O."}]},{"#name":"author","$$":[{"#name":"surname","_":"Babuschkin"},{"#name":"given-name","_":"I."}]},{"#name":"author","$$":[{"#name":"surname","_":"Czarnecki"},{"#name":"given-name","_":"W.M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Mathieu"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Dudzik"},{"#name":"given-name","_":"A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chung"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Choi"},{"#name":"given-name","_":"D.H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Powell"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Ewalds"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Georgiev"},{"#name":"given-name","_":"P."}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Grandmaster level in starcraft ii using multi-agent reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Nature"}]},{"#name":"volume-nr","_":"575"}]},{"#name":"issue-nr","_":"7782"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"350"},{"#name":"last-page","_":"354"}]}]}]},{"#name":"source-text","$":{"id":"afs16"},"_":"O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, others, Grandmaster level in starcraft ii using multi-agent reinforcement learning, Nature 575 (7782) (2019) 350â€“354."}]},{"#name":"bib-reference","$":{"id":"b17"},"$$":[{"#name":"label","_":"[17]"},{"#name":"reference","$":{"id":"sb17","refId":"17"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Nguyen"},{"#name":"given-name","_":"T.T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Nguyen"},{"#name":"given-name","_":"N.D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Nahavandi"},{"#name":"given-name","_":"S."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Cybern."}]},{"#name":"volume-nr","_":"50"}]},{"#name":"issue-nr","_":"9"},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"3826"},{"#name":"last-page","_":"3839"}]}]}]},{"#name":"source-text","$":{"id":"afs17"},"_":"T. T. Nguyen, N. D. Nguyen, S. Nahavandi, Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications, IEEE transactions on cybernetics 50 (9) (2020) 3826â€“3839."}]},{"#name":"bib-reference","$":{"id":"b18"},"$$":[{"#name":"label","_":"[18]"},{"#name":"reference","$":{"id":"sb18","refId":"18"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hernandez-Leal"},{"#name":"given-name","_":"P."}]},{"#name":"author","$$":[{"#name":"surname","_":"Kartal"},{"#name":"given-name","_":"B."}]},{"#name":"author","$$":[{"#name":"surname","_":"Taylor"},{"#name":"given-name","_":"M.E."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A survey and critique of multiagent deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Auton. Agents Multi-Agent Syst."}]},{"#name":"volume-nr","_":"33"}]},{"#name":"issue-nr","_":"6"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"750"},{"#name":"last-page","_":"797"}]}]}]},{"#name":"source-text","$":{"id":"afs18"},"_":"P. Hernandez-Leal, B. Kartal, M. E. Taylor, A survey and critique of multiagent deep reinforcement learning, Autonomous Agents and Multi-Agent Systems 33 (6) (2019) 750â€“797."}]},{"#name":"bib-reference","$":{"id":"b19"},"$$":[{"#name":"label","_":"[19]"},{"#name":"reference","$":{"id":"sb19","refId":"19"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Oroojlooy"},{"#name":"given-name","_":"A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hajinezhad"},{"#name":"given-name","_":"D."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A review of cooperative multi-agent deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Appl. Intell."}]},{"#name":"volume-nr","_":"53"}]},{"#name":"issue-nr","_":"11"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"13677"},{"#name":"last-page","_":"13722"}]}]}]},{"#name":"source-text","$":{"id":"afs19"},"_":"A. Oroojlooy, D. Hajinezhad, A review of cooperative multi-agent deep reinforcement learning, Applied Intelligence 53 (11) (2023) 13677â€“13722."}]},{"#name":"bib-reference","$":{"id":"b20"},"$$":[{"#name":"label","_":"[20]"},{"#name":"reference","$":{"id":"sb20","refId":"20"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Da Silva"},{"#name":"given-name","_":"F.L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Costa"},{"#name":"given-name","_":"A.H.R."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A survey on transfer learning for multiagent reinforcement learning systems"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"J. Artificial Intelligence Res."}]},{"#name":"volume-nr","_":"64"}]},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"645"},{"#name":"last-page","_":"703"}]}]}]},{"#name":"source-text","$":{"id":"afs20"},"_":"F. L. Da Silva, A. H. R. Costa, A survey on transfer learning for multiagent reinforcement learning systems, Journal of Artificial Intelligence Research 64 (2019) 645â€“703."}]},{"#name":"bib-reference","$":{"id":"b21"},"$$":[{"#name":"label","_":"[21]"},{"#name":"reference","$":{"id":"sb21","refId":"21"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Da Silva"},{"#name":"given-name","_":"F.L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Warnell"},{"#name":"given-name","_":"G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Costa"},{"#name":"given-name","_":"A.H.R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Stone"},{"#name":"given-name","_":"P."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Agents teaching agents: A survey on inter-agent transfer learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Auton. Agents Multi-Agent Syst."}]},{"#name":"volume-nr","_":"34"}]},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"17"}]}]}]},{"#name":"source-text","$":{"id":"afs21"},"_":"F. L. Da Silva, G. Warnell, A. H. R. Costa, P. Stone, Agents teaching agents: a survey on inter-agent transfer learning, Autonomous Agents and Multi-Agent Systems 34 (2020) 1â€“17."}]},{"#name":"bib-reference","$":{"id":"b22"},"$$":[{"#name":"label","_":"[22]"},{"#name":"reference","$":{"id":"sb22","refId":"22"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Lazaridou"},{"#name":"given-name","_":"A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Baroni"},{"#name":"given-name","_":"M."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Emergent multi-agent communication in the deep learning era"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2020"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref3","href":"arxiv:2006.02419","role":"http://www.elsevier.com/xml/linking-roles/preprint","type":"simple"},"_":"arXiv:2006.02419"}]}]},{"#name":"source-text","$":{"id":"afs22"},"_":"A. Lazaridou, M. Baroni, Emergent multi-agent communication in the deep learning era, arXiv preprint arXiv:2006.02419 (2020)."}]},{"#name":"bib-reference","$":{"id":"b23"},"$$":[{"#name":"label","_":"[23]"},{"#name":"reference","$":{"id":"sb23","refId":"23"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"BaÅŸar"},{"#name":"given-name","_":"T."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent reinforcement learning: A selective overview of theories and algorithms"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Handb. Reinf. Learn. Control"}]}]},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"321"},{"#name":"last-page","_":"384"}]}]}]},{"#name":"source-text","$":{"id":"afs23"},"_":"K. Zhang, Z. Yang, T. BaÅŸar, Multi-agent reinforcement learning: A selective overview of theories and algorithms, Handbook of reinforcement learning and control 0 (2021) 321â€“384."}]},{"#name":"bib-reference","$":{"id":"b24"},"$$":[{"#name":"label","_":"[24]"},{"#name":"reference","$":{"id":"sb24","refId":"24"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Guojun"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Shimin"},{"#name":"given-name","_":"G."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Improved q-learning algorithm and its application in path planning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"J. Taiyuan Univ. Technol."}]},{"#name":"volume-nr","_":"52"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"91"},{"#name":"last-page","_":"97"}]}]}]},{"#name":"source-text","$":{"id":"afs24"},"_":"M. Guojun, G. Shimin, Improved q-learning algorithm and its application in path planning, Journal of Taiyuan University of Technology 52 (1) (2021) 91â€“97."}]},{"#name":"bib-reference","$":{"id":"b25"},"$$":[{"#name":"label","_":"[25]"},{"#name":"reference","$":{"id":"sb25","refId":"25"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"KrÃ¶se"},{"#name":"given-name","_":"B.J.A."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Learning from delayed rewards"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Robot. Auton. Syst."}]},{"#name":"volume-nr","_":"15"}]},{"#name":"date","_":"1995"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"233"},{"#name":"last-page","_":"235"}]}]}]},{"#name":"source-text","$":{"id":"afs25"},"_":"B. J. A. KrÃ¶se, Learning from delayed rewards, Robotics Auton. Syst. 15 (1995) 233â€“235."}]},{"#name":"bib-reference","$":{"id":"b26"},"$$":[{"#name":"label","_":"[26]"},{"#name":"reference","$":{"id":"sb26","refId":"26"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Guo"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Liu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Malec"},{"#name":"given-name","_":"J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A new q-learning algorithm based on the metropolis criterion"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Syst. Man Cybern. B"}]},{"#name":"volume-nr","_":"34"}]},{"#name":"issue-nr","_":"5"},{"#name":"date","_":"2004"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"2140"},{"#name":"last-page","_":"2143"}]}]}]},{"#name":"source-text","$":{"id":"afs26"},"_":"M. Guo, Y. Liu, J. Malec, A new q-learning algorithm based on the metropolis criterion, IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 34 (5) (2004) 2140â€“2143."}]},{"#name":"bib-reference","$":{"id":"b27"},"$$":[{"#name":"label","_":"[27]"},{"#name":"reference","$":{"id":"sb27","refId":"27"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Lin"},{"#name":"given-name","_":"Y.-P."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"X.-Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Reinforcement learning based on local state feature learning and policy adjustment"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Inform. Sci."}]},{"#name":"volume-nr","_":"154"}]},{"#name":"issue-nr","_":"1â€“2"},{"#name":"date","_":"2003"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"59"},{"#name":"last-page","_":"70"}]}]}]},{"#name":"source-text","$":{"id":"afs28"},"_":"Y.-P. Lin, X.-Y. Li, Reinforcement learning based on local state feature learning and policy adjustment, Information Sciences 154 (1-2) (2003) 59â€“70."}]},{"#name":"bib-reference","$":{"id":"b28"},"$$":[{"#name":"label","_":"[28]"},{"#name":"reference","$":{"id":"sb28","refId":"28"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Sharma"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Gopal"},{"#name":"given-name","_":"M."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A markov game-adaptive fuzzy controller for robot manipulators"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Fuzzy Syst."}]},{"#name":"volume-nr","_":"16"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2008"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"171"},{"#name":"last-page","_":"186"}]}]}]},{"#name":"source-text","$":{"id":"afs29"},"_":"R. Sharma, M. Gopal, A markov game-adaptive fuzzy controller for robot manipulators, IEEE Transactions on Fuzzy Systems 16 (1) (2008) 171â€“186."}]},{"#name":"bib-reference","$":{"id":"b29"},"$$":[{"#name":"label","_":"[29]"},{"#name":"reference","$":{"id":"sb29","refId":"29"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Boubertakh"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Tadjine"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Glorennec"},{"#name":"given-name","_":"P.-Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A new mobile robot navigation method using fuzzy logic and a modified q-learning algorithm"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"J. Intell. Fuzzy Systems"}]},{"#name":"volume-nr","_":"21"}]},{"#name":"issue-nr","_":"1, 2"},{"#name":"date","_":"2010"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"113"},{"#name":"last-page","_":"119"}]}]}]},{"#name":"source-text","$":{"id":"afs30"},"_":"H. Boubertakh, M. Tadjine, P.-Y. Glorennec, A new mobile robot navigation method using fuzzy logic and a modified q-learning algorithm, Journal of Intelligent & Fuzzy Systems 21 (1, 2) (2010) 113â€“119."}]},{"#name":"bib-reference","$":{"id":"b30"},"$$":[{"#name":"label","_":"[30]"},{"#name":"reference","$":{"id":"sb30","refId":"30"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Rahimiyan"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Mashhadi"},{"#name":"given-name","_":"H.R."}]}]},{"#name":"title","$$":[{"#name":"maintitle","$$":[{"#name":"__text__","_":"An adaptive "},{"#name":"math","$":{"altimg":"si12.svg","display":"inline","id":"d1e9938"},"$$":[{"#name":"mi","_":"q"}]},{"#name":"__text__","_":"-learning algorithm developed for agent-based computational modeling of electricity market"}]}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Syst. Man Cybern. C (Appl. Rev.)"}]},{"#name":"volume-nr","_":"40"}]},{"#name":"issue-nr","_":"5"},{"#name":"date","_":"2010"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"547"},{"#name":"last-page","_":"556"}]}]}]},{"#name":"source-text","$":{"id":"afs31"},"_":"M. Rahimiyan, H. R. Mashhadi, An adaptive q-learning algorithm developed for agent-based computational modeling of electricity market, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 40 (5) (2010) 547â€“556."}]},{"#name":"bib-reference","$":{"id":"b31"},"$$":[{"#name":"label","_":"[31]"},{"#name":"reference","$":{"id":"sb31","refId":"31"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hwang"},{"#name":"given-name","_":"K.-S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Tan"},{"#name":"given-name","_":"S.-W."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"C.-C."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Cooperative strategy based on adaptive q-learning for robot soccer systems"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Fuzzy Syst."}]},{"#name":"volume-nr","_":"12"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2004"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"569"},{"#name":"last-page","_":"576"}]}]}]},{"#name":"source-text","$":{"id":"afs27"},"_":"K.-S. Hwang, S.-W. Tan, C.-C. Chen, Cooperative strategy based on adaptive q-learning for robot soccer systems, IEEE Transactions on Fuzzy Systems 12 (4) (2004) 569â€“576."}]},{"#name":"bib-reference","$":{"id":"b32"},"$$":[{"#name":"label","_":"[32]"},{"#name":"reference","$":{"id":"sb32","refId":"32"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhou"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhou"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"R.Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Subcarrier assignment schemes based on q-learning in wideband cognitive radio networks"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Veh. Technol."}]},{"#name":"volume-nr","_":"69"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1168"},{"#name":"last-page","_":"1172"}]}]}]},{"#name":"source-text","$":{"id":"afs32"},"_":"Y. Zhou, F. Zhou, Y. Wu, R. Q. Hu, Y. Wang, Subcarrier assignment schemes based on q-learning in wideband cognitive radio networks, IEEE Transactions on Vehicular Technology 69 (1) (2019) 1168â€“1172."}]},{"#name":"bib-reference","$":{"id":"b33"},"$$":[{"#name":"label","_":"[33]"},{"#name":"reference","$":{"id":"sb33","refId":"33"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Chung"},{"#name":"given-name","_":"W.-C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chang"},{"#name":"given-name","_":"C.-J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Feng"},{"#name":"given-name","_":"K.-T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"Y.-Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","$$":[{"#name":"__text__","_":"An mimo configuration mode and mcs level selection scheme by fuzzy q-learning for hspa"},{"#name":"math","$":{"altimg":"si257.svg","display":"inline","id":"d1e10097"},"$$":[{"#name":"msup","$$":[{"#name":"mrow"},{"#name":"mrow","$$":[{"#name":"mo","_":"+"}]}]}]},{"#name":"__text__","_":" systems"}]}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Mob. Comput."}]},{"#name":"volume-nr","_":"11"}]},{"#name":"issue-nr","_":"7"},{"#name":"date","_":"2012"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1151"},{"#name":"last-page","_":"1162"}]}]}]},{"#name":"source-text","$":{"id":"afs33"},"_":"W.-C. Chung, C.-J. Chang, K.-T. Feng, Y.-Y. Chen, An mimo configuration mode and mcs level selection scheme by fuzzy q-learning for hspa+ systems, IEEE Transactions on Mobile Computing 11 (7) (2012) 1151â€“1162."}]},{"#name":"bib-reference","$":{"id":"b34"},"$$":[{"#name":"label","_":"[34]"},{"#name":"reference","$":{"id":"sb34","refId":"34"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Shams"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Bacci"},{"#name":"given-name","_":"G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Luise"},{"#name":"given-name","_":"M."}]}]},{"#name":"title","$$":[{"#name":"maintitle","$$":[{"#name":"__text__","_":"Energy-efficient power control for multiple-relay cooperative networks using "},{"#name":"math","$":{"altimg":"si12.svg","display":"inline","id":"d1e10152"},"$$":[{"#name":"mi","_":"q"}]},{"#name":"__text__","_":"-learning"}]}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Wireless Commun."}]},{"#name":"volume-nr","_":"14"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2014"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1567"},{"#name":"last-page","_":"1580"}]}]}]},{"#name":"source-text","$":{"id":"afs34"},"_":"F. Shams, G. Bacci, M. Luise, Energy-efficient power control for multiple-relay cooperative networks using q-learning, IEEE Transactions on Wireless Communications 14 (3) (2014) 1567â€“1580."}]},{"#name":"bib-reference","$":{"id":"b35"},"$$":[{"#name":"label","_":"[35]"},{"#name":"reference","$":{"id":"sb35","refId":"35"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Peng"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Liu"},{"#name":"given-name","_":"W."}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A cooperative-learning path planning algorithm for originâ€“destination pairs in urban road networks"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Math. Probl. Eng."}]},{"#name":"volume-nr","_":"2015"}]},{"#name":"date","_":"2015"}]}]}]},{"#name":"source-text","$":{"id":"afs35"},"_":"X. Zhang, H. Li, J. Peng, W. Liu, others, A cooperative-learning path planning algorithm for originâ€“destination pairs in urban road networks, Mathematical Problems in Engineering 2015 (2015) 0."}]},{"#name":"bib-reference","$":{"id":"b36"},"$$":[{"#name":"label","_":"[36]"},{"#name":"reference","$":{"id":"sb36","refId":"36"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhu"},{"#name":"given-name","_":"P."}]},{"#name":"author","$$":[{"#name":"surname","_":"Fang"},{"#name":"given-name","_":"X."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-uav cooperative task assignment based on half random q-learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Symmetry"}]},{"#name":"volume-nr","_":"13"}]},{"#name":"issue-nr","_":"12"},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"2417"}]}]}]},{"#name":"source-text","$":{"id":"afs36"},"_":"P. Zhu, X. Fang, Multi-uav cooperative task assignment based on half random q-learning, Symmetry 13 (12) (2021) 2417."}]},{"#name":"bib-reference","$":{"id":"b37"},"$$":[{"#name":"label","_":"[37]"},{"#name":"reference","$":{"id":"sb37","refId":"37"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Fang"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Liu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Nallanathan"},{"#name":"given-name","_":"A."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Stochastic game based cooperative alternating q-learning caching in dynamic d2d networks"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Veh. Technol."}]},{"#name":"volume-nr","_":"70"}]},{"#name":"issue-nr","_":"12"},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"13255"},{"#name":"last-page","_":"13269"}]}]}]},{"#name":"source-text","$":{"id":"afs37"},"_":"T. Zhang, X. Fang, Z. Wang, Y. Liu, A. Nallanathan, Stochastic game based cooperative alternating q-learning caching in dynamic d2d networks, IEEE Transactions on Vehicular Technology 70 (12) (2021) 13255â€“13269."}]},{"#name":"bib-reference","$":{"id":"b38"},"$$":[{"#name":"label","_":"[38]"},{"#name":"reference","$":{"id":"sb38","refId":"38"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wiering"},{"#name":"given-name","_":"M.A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Van Hasselt"},{"#name":"given-name","_":"H."}]}]},{"#name":"title","$$":[{"#name":"maintitle","$$":[{"#name":"__text__","_":"Two novel on-policy reinforcement learning algorithms based on td ("},{"#name":"math","$":{"altimg":"si21.svg","display":"inline","id":"d1e10339"},"$$":[{"#name":"mi","_":"Î»"}]},{"#name":"__text__","_":")-methods"}]}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning"}]},{"#name":"date","_":"2007"},{"#name":"publisher","$$":[{"#name":"name","_":"IEEE"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"280"},{"#name":"last-page","_":"287"}]}]}]},{"#name":"source-text","$":{"id":"afs38"},"_":"M. A. Wiering, H. Van Hasselt, Two novel on-policy reinforcement learning algorithms based on td (Î»)-methods, in: 2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, IEEE, 0 (2007) 280â€“287."}]},{"#name":"bib-reference","$":{"id":"b39"},"$$":[{"#name":"label","_":"[39]"},{"#name":"reference","$":{"id":"sb39","refId":"39"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Aissani"},{"#name":"given-name","_":"N."}]},{"#name":"author","$$":[{"#name":"surname","_":"Beldjilali"},{"#name":"given-name","_":"B."}]},{"#name":"author","$$":[{"#name":"surname","_":"Trentesaux"},{"#name":"given-name","_":"D."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Dynamic scheduling of maintenance tasks in the petroleum industry: A reinforcement approach"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Eng. Appl. Artif. Intell."}]},{"#name":"volume-nr","_":"22"}]},{"#name":"issue-nr","_":"7"},{"#name":"date","_":"2009"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1089"},{"#name":"last-page","_":"1103"}]}]}]},{"#name":"source-text","$":{"id":"afs39"},"_":"N. Aissani, B. Beldjilali, D. Trentesaux, Dynamic scheduling of maintenance tasks in the petroleum industry: A reinforcement approach, Engineering Applications of Artificial Intelligence 22 (7) (2009) 1089â€“1103."}]},{"#name":"bib-reference","$":{"id":"b40"},"$$":[{"#name":"label","_":"[40]"},{"#name":"reference","$":{"id":"sb40","refId":"40"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Derhami"},{"#name":"given-name","_":"V."}]},{"#name":"author","$$":[{"#name":"surname","_":"Majd"},{"#name":"given-name","_":"V.J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Ahmadabadi"},{"#name":"given-name","_":"M.N."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Exploration and exploitation balance management in fuzzy reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Fuzzy Sets and Systems"}]},{"#name":"volume-nr","_":"161"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2010"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"578"},{"#name":"last-page","_":"595"}]}]}]},{"#name":"source-text","$":{"id":"afs40"},"_":"V. Derhami, V. J. Majd, M. N. Ahmadabadi, Exploration and exploitation balance management in fuzzy reinforcement learning, Fuzzy sets and systems 161 (4) (2010) 578â€“595."}]},{"#name":"bib-reference","$":{"id":"b41"},"$$":[{"#name":"label","_":"[41]"},{"#name":"reference","$":{"id":"sb41","refId":"41"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Andrecut"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Ali"},{"#name":"given-name","_":"M."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Deep-sarsa: A reinforcement learning algorithm for autonomous navigation"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Internat. J. Modern Phys. C"}]},{"#name":"volume-nr","_":"12"}]},{"#name":"issue-nr","_":"10"},{"#name":"date","_":"2001"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1513"},{"#name":"last-page","_":"1523"}]}]}]},{"#name":"source-text","$":{"id":"afs41"},"_":"M. Andrecut, M. Ali, Deep-sarsa: A reinforcement learning algorithm for autonomous navigation, International Journal of Modern Physics C 12 (10) (2001) 1513â€“1523."}]},{"#name":"bib-reference","$":{"id":"b42"},"$$":[{"#name":"label","_":"[42]"},{"#name":"reference","$":{"id":"sb42","refId":"42"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Olyaei"},{"#name":"given-name","_":"M.H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jalali"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Olyaei"},{"#name":"given-name","_":"A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Noori"},{"#name":"given-name","_":"A."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Implement deep sarsa in grid world with changing obstacles and testing against new environment"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Fundamental Research in Electrical Engineering: The Selected Papers of the First International Conference on Fundamental Research in Electrical Engineering"}]},{"#name":"date","_":"2019"},{"#name":"publisher","$$":[{"#name":"name","_":"Springer"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"267"},{"#name":"last-page","_":"279"}]}]}]},{"#name":"source-text","$":{"id":"afs42"},"_":"M. H. Olyaei, H. Jalali, A. Olyaei, A. Noori, Implement deep sarsa in grid world with changing obstacles and testing against new environment, in: Fundamental Research in Electrical Engineering: The Selected Papers of The First International Conference on Fundamental Research in Electrical Engineering, Springer, 0 (2019) 267â€“279."}]},{"#name":"bib-reference","$":{"id":"b43"},"$$":[{"#name":"label","_":"[43]"},{"#name":"reference","$":{"id":"sb43","refId":"43"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Luo"},{"#name":"given-name","_":"W."}]},{"#name":"author","$$":[{"#name":"surname","_":"Tang"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Fu"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Eberhard"},{"#name":"given-name","_":"P."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Deep-sarsa based multi-uav path planning and obstacle avoidance in a dynamic environment"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Advances in Swarm Intelligence: 9th International Conference, ICSI 2018, Shanghai, China, June (2018) 17-22, Proceedings, Part II 9"}]},{"#name":"date","_":"2018"},{"#name":"publisher","$$":[{"#name":"name","_":"Springer"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"102"},{"#name":"last-page","_":"111"}]}]}]},{"#name":"source-text","$":{"id":"afs43"},"_":"W. Luo, Q. Tang, C. Fu, P. Eberhard, Deep-sarsa based multi-uav path planning and obstacle avoidance in a dynamic environment, in: Advances in Swarm Intelligence: 9th International Conference, ICSI 2018, Shanghai, China, June (2018) 17-22, Proceedings, Part II 9, Springer, 0 (2018) 102â€“111."}]},{"#name":"bib-reference","$":{"id":"b44"},"$$":[{"#name":"label","_":"[44]"},{"#name":"reference","$":{"id":"sb44","refId":"44"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"Z.-x."}]},{"#name":"author","$$":[{"#name":"surname","_":"Cao"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"X.-l."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"C.-x."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Y.-l."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lai"},{"#name":"given-name","_":"J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Deep reinforcement learning with sarsa and q-learning: A hybrid approach"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEICE Trans. Inf. Syst."}]},{"#name":"volume-nr","_":"101"}]},{"#name":"issue-nr","_":"9"},{"#name":"date","_":"2018"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"2315"},{"#name":"last-page","_":"2322"}]}]}]},{"#name":"source-text","$":{"id":"afs44"},"_":"Z.-x. Xu, L. Cao, X.-l. Chen, C.-x. Li, Y.-l. Zhang, J. Lai, Deep reinforcement learning with sarsa and q-learning: A hybrid approach, IEICE TRANSACTIONS on Information and Systems 101 (9) (2018) 2315â€“2322."}]},{"#name":"bib-reference","$":{"id":"b45"},"$$":[{"#name":"label","_":"[45]"},{"#name":"reference","$":{"id":"sb45","refId":"45"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Bellman"},{"#name":"given-name","_":"R."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Dynamic programming"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Science"}]},{"#name":"volume-nr","_":"153"}]},{"#name":"issue-nr","_":"3731"},{"#name":"date","_":"1966"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"34"},{"#name":"last-page","_":"37"}]}]}]},{"#name":"source-text","$":{"id":"afs46"},"_":"R. Bellman, Dynamic programming, Science 153 (3731) (1966) 34â€“37."}]},{"#name":"bib-reference","$":{"id":"b46"},"$$":[{"#name":"label","_":"[46]"},{"#name":"reference","$":{"id":"sb46","refId":"46"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Amini"},{"#name":"given-name","_":"A.A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Weymouth"},{"#name":"given-name","_":"T.E."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jain"},{"#name":"given-name","_":"R.C."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Using dynamic programming for solving variational problems in vision"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Pattern Anal. Mach. Intell."}]},{"#name":"volume-nr","_":"12"}]},{"#name":"issue-nr","_":"9"},{"#name":"date","_":"1990"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"855"},{"#name":"last-page","_":"867"}]}]}]},{"#name":"source-text","$":{"id":"afs47"},"_":"A. A. Amini, T. E. Weymouth, R. C. Jain, Using dynamic programming for solving variational problems in vision, IEEE Transactions on pattern analysis and machine intelligence 12 (9) (1990) 855â€“867."}]},{"#name":"bib-reference","$":{"id":"b47"},"$$":[{"#name":"label","_":"[47]"},{"#name":"reference","$":{"id":"sb47","refId":"47"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Merlet"},{"#name":"given-name","_":"N."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zerubia"},{"#name":"given-name","_":"J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"New prospects in line detection by dynamic programming"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Pattern Anal. Mach. Intell."}]},{"#name":"volume-nr","_":"18"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"1996"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"426"},{"#name":"last-page","_":"431"}]}]}]},{"#name":"source-text","$":{"id":"afs48"},"_":"N. Merlet, J. Zerubia, New prospects in line detection by dynamic programming, IEEE Transactions on Pattern Analysis and Machine Intelligence 18 (4) (1996) 426â€“431."}]},{"#name":"bib-reference","$":{"id":"b48"},"$$":[{"#name":"label","_":"[48]"},{"#name":"reference","$":{"id":"sb48","refId":"48"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Buckley"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Regularised shortest-path extraction"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Pattern Recognit. Lett."}]},{"#name":"volume-nr","_":"18"}]},{"#name":"issue-nr","_":"7"},{"#name":"date","_":"1997"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"621"},{"#name":"last-page","_":"629"}]}]}]},{"#name":"source-text","$":{"id":"afs49"},"_":"M. Buckley, J. Yang, Regularised shortest-path extraction, Pattern Recognition Letters 18 (7) (1997) 621â€“629."}]},{"#name":"bib-reference","$":{"id":"b49"},"$$":[{"#name":"label","_":"[49]"},{"#name":"reference","$":{"id":"sb49","refId":"49"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Jiang"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"S."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Neural network-based intelligent computing algorithms for discrete-time optimal control with the application to a cyberphysical power system"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Complexity"}]},{"#name":"volume-nr","_":"2021"}]},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"10"}]}]}]},{"#name":"source-text","$":{"id":"afs45"},"_":"F. Jiang, K. Zhang, J. Hu, S. Wang, Neural network-based intelligent computing algorithms for discrete-time optimal control with the application to a cyberphysical power system, Complexity 2021 (2021) 1â€“10."}]},{"#name":"bib-reference","$":{"id":"b50"},"$$":[{"#name":"label","_":"[50]"},{"#name":"reference","$":{"id":"sb50","refId":"50"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Werbos"},{"#name":"given-name","_":"P."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Advanced forecasting methods for global crisis warning and models of intelligence"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Gen. Syst. Yearb."}]}]},{"#name":"date","_":"1977"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"25"},{"#name":"last-page","_":"38"}]}]}]},{"#name":"source-text","$":{"id":"afs50"},"_":"P. Werbos, Advanced forecasting methods for global crisis warning and models of intelligence, General System Yearbook 0 (1977) 25â€“38."}]},{"#name":"bib-reference","$":{"id":"b51"},"$$":[{"#name":"label","_":"[51]"},{"#name":"reference","$":{"id":"sb51","refId":"51"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Miller"},{"#name":"given-name","_":"W.T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Sutton"},{"#name":"given-name","_":"R.S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Werbos"},{"#name":"given-name","_":"P.J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A menu of designs for reinforcement learning over time"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"1995"}]}]}]},{"#name":"source-text","$":{"id":"afs51"},"_":"W. T. Miller, R. S. Sutton, P. J. Werbos, A menu of designs for reinforcement learning over time (1995)."}]},{"#name":"bib-reference","$":{"id":"b52"},"$$":[{"#name":"label","_":"[52]"},{"#name":"reference","$":{"id":"sb52","refId":"52"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Powell"},{"#name":"given-name","_":"W.B."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Approximate Dynamic Programming: Solving the Curses of Dimensionality"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2007"},{"#name":"publisher","$$":[{"#name":"name","_":"John Wiley & Sons"}]}]}]}]},{"#name":"source-text","$":{"id":"afs52"},"_":"W. B. Powell, Approximate Dynamic Programming: Solving the curses of dimensionality, John Wiley & Sons, 703 (2007) 0"}]},{"#name":"bib-reference","$":{"id":"b53"},"$$":[{"#name":"label","_":"[53]"},{"#name":"reference","$":{"id":"sb53","refId":"53"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Joy"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Kaisare"},{"#name":"given-name","_":"N.S."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Approximate dynamic programming-based control of distributed parameter systems"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Asia-Pac. J. Chem. Eng."}]},{"#name":"volume-nr","_":"6"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2011"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"452"},{"#name":"last-page","_":"459"}]}]}]},{"#name":"source-text","$":{"id":"afs53"},"_":"M. Joy, N. S. Kaisare, Approximate dynamic programming-based control of distributed parameter systems, Asia-Pacific Journal of Chemical Engineering 6 (3) (2011) 452â€“459."}]},{"#name":"bib-reference","$":{"id":"b54"},"$$":[{"#name":"label","_":"[54]"},{"#name":"reference","$":{"id":"sb54","refId":"54"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"F.-Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wei"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zheng"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Pdp: Parallel dynamic programming"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE/CAA J. Autom. Sin."}]},{"#name":"volume-nr","_":"4"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2017"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"5"}]}]}]},{"#name":"source-text","$":{"id":"afs54"},"_":"F.-Y. Wang, J. Zhang, Q. Wei, X. Zheng, L. Li, Pdp: parallel dynamic programming, IEEE/CAA Journal of Automatica Sinica 4 (1) (2017) 1â€“5."}]},{"#name":"bib-reference","$":{"id":"b55"},"$$":[{"#name":"label","_":"[55]"},{"#name":"reference","$":{"id":"sb55","refId":"55"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Luo"},{"#name":"given-name","_":"B."}]},{"#name":"author","$$":[{"#name":"surname","_":"Liu"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Huang"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Ma"},{"#name":"given-name","_":"H."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-step heuristic dynamic programming for optimal control of nonlinear discrete-time systems"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Inform. Sci."}]},{"#name":"volume-nr","_":"411"}]},{"#name":"date","_":"2017"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"66"},{"#name":"last-page","_":"83"}]}]}]},{"#name":"source-text","$":{"id":"afs55"},"_":"B. Luo, D. Liu, T. Huang, X. Yang, H. Ma, Multi-step heuristic dynamic programming for optimal control of nonlinear discrete-time systems, Information Sciences 411 (2017) 66â€“83."}]},{"#name":"bib-reference","$":{"id":"b56"},"$$":[{"#name":"label","_":"[56]"},{"#name":"reference","$":{"id":"sb56","refId":"56"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Voelkel"},{"#name":"given-name","_":"M.A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Sachs"},{"#name":"given-name","_":"A.-L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Thonemann"},{"#name":"given-name","_":"U.W."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"An aggregation-based approximate dynamic programming approach for the periodic review model with random yield"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"European J. Oper. Res."}]},{"#name":"volume-nr","_":"281"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"286"},{"#name":"last-page","_":"298"}]}]}]},{"#name":"source-text","$":{"id":"afs56"},"_":"M. A. Voelkel, A.-L. Sachs, U. W. Thonemann, An aggregation-based approximate dynamic programming approach for the periodic review model with random yield, European Journal of Operational Research 281 (2) (2020) 286â€“298."}]},{"#name":"bib-reference","$":{"id":"b57"},"$$":[{"#name":"label","_":"[57]"},{"#name":"reference","$":{"id":"sb57","refId":"57"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Puterman"},{"#name":"given-name","_":"M.L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Markov decision processes: Discrete stochastic dynamic programming"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"1994"}]}]}]},{"#name":"source-text","$":{"id":"afs57"},"_":"M. L. Puterman, Markov decision processes: Discrete stochastic dynamic programming (1994)."}]},{"#name":"bib-reference","$":{"id":"b58"},"$$":[{"#name":"label","_":"[58]"},{"#name":"reference","$":{"id":"sb58","refId":"58"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Liu"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhao"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wei"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jin"},{"#name":"given-name","_":"N."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Neural-network-based optimal control for a class of unknown discrete-time nonlinear systems using globalized dual heuristic programming"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Autom. Sci. Eng."}]},{"#name":"volume-nr","_":"9"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2012"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"628"},{"#name":"last-page","_":"634"}]}]}]},{"#name":"source-text","$":{"id":"afs58"},"_":"D. Liu, D. Wang, D. Zhao, Q. Wei, N. Jin, Neural-network-based optimal control for a class of unknown discrete-time nonlinear systems using globalized dual heuristic programming, IEEE Transactions on Automation Science and Engineering 9 (3) (2012) 628â€“634."}]},{"#name":"bib-reference","$":{"id":"b59"},"$$":[{"#name":"label","_":"[59]"},{"#name":"reference","$":{"id":"sb59","refId":"59"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Vrabie"},{"#name":"given-name","_":"D.L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Vamvoudakis"},{"#name":"given-name","_":"K.G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lewis"},{"#name":"given-name","_":"F.L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Optimal adaptive control and differential games by reinforcement learning principles"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Inst. Electr. Eng."}]}]},{"#name":"date","_":"2013"}]}]}]},{"#name":"source-text","$":{"id":"afs59"},"_":"D. L. Vrabie, K. G. Vamvoudakis, F. L. Lewis, Optimal adaptive control and differential games by reinforcement learning principles, Institution of Electrical Engineers, 0 (2013) 0."}]},{"#name":"bib-reference","$":{"id":"b60"},"$$":[{"#name":"label","_":"[60]"},{"#name":"reference","$":{"id":"sb60","refId":"60"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Bian"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jiang"},{"#name":"given-name","_":"Z.-P."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Value iteration and adaptive dynamic programming for data-driven adaptive optimal control design"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Automatica"}]},{"#name":"volume-nr","_":"71"}]},{"#name":"date","_":"2016"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"348"},{"#name":"last-page","_":"360"}]}]}]},{"#name":"source-text","$":{"id":"afs60"},"_":"T. Bian, Z.-P. Jiang, Value iteration and adaptive dynamic programming for data-driven adaptive optimal control design, Automatica 71 (2016) 348â€“360."}]},{"#name":"bib-reference","$":{"id":"b61"},"$$":[{"#name":"label","_":"[61]"},{"#name":"reference","$":{"id":"sb61","refId":"61"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Yuan"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hua"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Cheng"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Sang"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wei"},{"#name":"given-name","_":"W."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A novel model-based reinforcement learning algorithm for solving the problem of unbalanced reward"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"J. Intell. Fuzzy Systems"}]},{"#name":"volume-nr","_":"44"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"3233"},{"#name":"last-page","_":"3243"}]}]}]},{"#name":"source-text","$":{"id":"afs61"},"_":"Y. Yuan, L. Hua, Y. Cheng, J. Li, X. Sang, L. Zhang, W. Wei, A novel model-based reinforcement learning algorithm for solving the problem of unbalanced reward, Journal of Intelligent & Fuzzy Systems 44 (2) (2023) 3233â€“3243."}]},{"#name":"bib-reference","$":{"id":"b62"},"$$":[{"#name":"label","_":"[62]"},{"#name":"reference","$":{"id":"sb62","refId":"62"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Kleinman"},{"#name":"given-name","_":"D."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"On an iterative technique for riccati equation computations"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Autom. Control"}]},{"#name":"volume-nr","_":"13"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"1968"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"114"},{"#name":"last-page","_":"115"}]}]}]},{"#name":"source-text","$":{"id":"afs62"},"_":"D. Kleinman, On an iterative technique for riccati equation computations, IEEE Transactions on Automatic Control 13 (1) (1968) 114â€“115."}]},{"#name":"bib-reference","$":{"id":"b63"},"$$":[{"#name":"label","_":"[63]"},{"#name":"reference","$":{"id":"sb63","refId":"63"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Lewis"},{"#name":"given-name","_":"F.L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Vrabie"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Syrmos"},{"#name":"given-name","_":"V.L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Optimal Control"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2012"}]}]}]},{"#name":"source-text","$":{"id":"afs63"},"_":"F. L. Lewis, D. Vrabie, V. L. Syrmos, Optimal control, John Wiley & Sons, 0 (2012) 0."}]},{"#name":"bib-reference","$":{"id":"b64"},"$$":[{"#name":"label","_":"[64]"},{"#name":"reference","$":{"id":"sb64","refId":"64"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Sutton"},{"#name":"given-name","_":"R.S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Barto"},{"#name":"given-name","_":"A.G."}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Introduction To Reinforcement Learning"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"1998"},{"#name":"publisher","$$":[{"#name":"name","_":"MIT press Cambridge"}]}]}]}]},{"#name":"source-text","$":{"id":"afs64"},"_":"R. S. Sutton, A. G. Barto, others, Introduction to reinforcement learning, MIT press Cambridge, 135 (1998) 0."}]},{"#name":"bib-reference","$":{"id":"b65"},"$$":[{"#name":"label","_":"[65]"},{"#name":"reference","$":{"id":"sb65","refId":"65"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wei"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Luo"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A novel infinite-time optimal tracking control scheme for a class of discrete-time nonlinear systems via the greedy hdp iteration algorithm"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Syst. Man Cybern. B"}]},{"#name":"volume-nr","_":"38"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2008"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"937"},{"#name":"last-page","_":"942"}]}]}]},{"#name":"source-text","$":{"id":"afs65"},"_":"H. Zhang, Q. Wei, Y. Luo, A novel infinite-time optimal tracking control scheme for a class of discrete-time nonlinear systems via the greedy hdp iteration algorithm, IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 38 (4) (2008) 937â€“942."}]},{"#name":"bib-reference","$":{"id":"b66"},"$$":[{"#name":"label","_":"[66]"},{"#name":"reference","$":{"id":"sb66","refId":"66"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Lewis"},{"#name":"given-name","_":"F.L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Vrabie"},{"#name":"given-name","_":"D."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Reinforcement learning and adaptive dynamic programming for feedback control"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Circuits Syst. Mag."}]},{"#name":"volume-nr","_":"9"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2009"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"32"},{"#name":"last-page","_":"50"}]}]}]},{"#name":"source-text","$":{"id":"afs66"},"_":"F. L. Lewis, D. Vrabie, Reinforcement learning and adaptive dynamic programming for feedback control, IEEE circuits and systems magazine 9 (3) (2009) 32â€“50."}]},{"#name":"bib-reference","$":{"id":"b67"},"$$":[{"#name":"label","_":"[67]"},{"#name":"reference","$":{"id":"sb67","refId":"67"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Vamvoudakis"},{"#name":"given-name","_":"K.G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lewis"},{"#name":"given-name","_":"F.L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hudas"},{"#name":"given-name","_":"G.R."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent differential graphical games: Online adaptive learning solution for synchronization with optimality"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Automatica"}]},{"#name":"volume-nr","_":"48"}]},{"#name":"issue-nr","_":"8"},{"#name":"date","_":"2012"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1598"},{"#name":"last-page","_":"1611"}]}]}]},{"#name":"source-text","$":{"id":"afs67"},"_":"K. G. Vamvoudakis, F. L. Lewis, G. R. Hudas, Multi-agent differential graphical games: Online adaptive learning solution for synchronization with optimality, Automatica 48 (8) (2012) 1598â€“1611."}]},{"#name":"bib-reference","$":{"id":"b68"},"$$":[{"#name":"label","_":"[68]"},{"#name":"reference","$":{"id":"sb68","refId":"68"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"G.-H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Luo"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Leader-based optimal coordination control for the consensus problem of multiagent differential games via fuzzy adaptive dynamic programming"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Fuzzy Syst."}]},{"#name":"volume-nr","_":"23"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2014"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"152"},{"#name":"last-page","_":"163"}]}]}]},{"#name":"source-text","$":{"id":"afs68"},"_":"H. Zhang, J. Zhang, G.-H. Yang, Y. Luo, Leader-based optimal coordination control for the consensus problem of multiagent differential games via fuzzy adaptive dynamic programming, IEEE Transactions on Fuzzy Systems 23 (1) (2014) 152â€“163."}]},{"#name":"bib-reference","$":{"id":"b69"},"$$":[{"#name":"label","_":"[69]"},{"#name":"reference","$":{"id":"sb69","refId":"69"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Cui"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Pan"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xue"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Tan"},{"#name":"given-name","_":"L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Simplified optimized finite-time containment control for a class of multi-agent systems with actuator faults"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Nonlinear Dynam."}]},{"#name":"volume-nr","_":"109"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"2799"},{"#name":"last-page","_":"2816"}]}]}]},{"#name":"source-text","$":{"id":"afs69"},"_":"J. Cui, Y. Pan, H. Xue, L. Tan, Simplified optimized finite-time containment control for a class of multi-agent systems with actuator faults, Nonlinear Dynamics 109 (4) (2022) 2799â€“2816."}]},{"#name":"bib-reference","$":{"id":"b70"},"$$":[{"#name":"label","_":"[70]"},{"#name":"reference","$":{"id":"sb70","refId":"70"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"B."}]},{"#name":"author","$$":[{"#name":"surname","_":"Proietti"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhu"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yoo"},{"#name":"given-name","_":"S.B."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep reinforcement learning in cognitive inter-domain networking with multi-broker orchestration"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"2019 Optical Fiber Communications Conference and Exhibition"}]},{"#name":"conference","_":"OFC"},{"#name":"date","_":"2019"},{"#name":"publisher","$$":[{"#name":"name","_":"IEEE"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"3"}]}]}]},{"#name":"source-text","$":{"id":"afs70"},"_":"X. Chen, B. Li, R. Proietti, Z. Zhu, S. B. Yoo, Multi-agent deep reinforcement learning in cognitive inter-domain networking with multi-broker orchestration, in: 2019 Optical Fiber Communications Conference and Exhibition (OFC), IEEE, 0 (2019) 1â€“3."}]},{"#name":"bib-reference","$":{"id":"b71"},"$$":[{"#name":"label","_":"[71]"},{"#name":"reference","$":{"id":"sb71","refId":"71"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Guo"},{"#name":"given-name","_":"C."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep reinforcement learning based spectrum allocation for d2d underlay communications"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Veh. Technol."}]},{"#name":"volume-nr","_":"69"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1828"},{"#name":"last-page","_":"1840"}]}]}]},{"#name":"source-text","$":{"id":"afs71"},"_":"Z. Li, C. Guo, Multi-agent deep reinforcement learning based spectrum allocation for d2d underlay communications, IEEE Transactions on Vehicular Technology 69 (2) (2019) 1828â€“1840."}]},{"#name":"bib-reference","$":{"id":"b72"},"$$":[{"#name":"label","_":"[72]"},{"#name":"reference","$":{"id":"sb72","refId":"72"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"You"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Feng"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhao"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yan"},{"#name":"given-name","_":"H."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Toward packet routing with fully distributed multiagent deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Syst. Man Cybern.: Syst."}]},{"#name":"volume-nr","_":"52"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"855"},{"#name":"last-page","_":"868"}]}]}]},{"#name":"source-text","$":{"id":"afs72"},"_":"X. You, X. Li, Y. Xu, H. Feng, J. Zhao, H. Yan, Toward packet routing with fully distributed multiagent deep reinforcement learning, IEEE Transactions on Systems, Man, and Cybernetics: Systems 52 (2) (2020) 855â€“868."}]},{"#name":"bib-reference","$":{"id":"b73"},"$$":[{"#name":"label","_":"[73]"},{"#name":"reference","$":{"id":"sb73","refId":"73"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Shen"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhong"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wen"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"An"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zheng"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhao"},{"#name":"given-name","_":"J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep reinforcement learning optimization framework for building energy system with renewable energy"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Appl. Energy"}]},{"#name":"volume-nr","_":"312"}]},{"#name":"date","_":"2022"}]},{"#name":"article-number","_":"118724"}]}]},{"#name":"source-text","$":{"id":"afs73"},"_":"R. Shen, S. Zhong, X. Wen, Q. An, R. Zheng, Y. Li, J. Zhao, Multi-agent deep reinforcement learning optimization framework for building energy system with renewable energy, Applied Energy 312 (2022) 118724."}]},{"#name":"bib-reference","$":{"id":"b74"},"$$":[{"#name":"label","_":"[74]"},{"#name":"reference","$":{"id":"sb74","refId":"74"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Qiu"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Strbac"},{"#name":"given-name","_":"G."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep reinforcement learning for resilience-driven routing and scheduling of mobile energy storage systems"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Appl. Energy"}]},{"#name":"volume-nr","_":"310"}]},{"#name":"date","_":"2022"}]},{"#name":"article-number","_":"118575"}]}]},{"#name":"source-text","$":{"id":"afs74"},"_":"Y. Wang, D. Qiu, G. Strbac, Multi-agent deep reinforcement learning for resilience-driven routing and scheduling of mobile energy storage systems, Applied Energy 310 (2022) 118575."}]},{"#name":"bib-reference","$":{"id":"b75"},"$$":[{"#name":"label","_":"[75]"},{"#name":"reference","$":{"id":"sb75","refId":"75"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Tan"},{"#name":"given-name","_":"A.H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Bejarano"},{"#name":"given-name","_":"F.P."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Ren"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Nejat"},{"#name":"given-name","_":"G."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Deep reinforcement learning for decentralized multi-robot exploration with macro actions"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Robotics and Automation Letters"}]},{"#name":"volume-nr","_":"8"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"272"},{"#name":"last-page","_":"279"}]}]}]},{"#name":"source-text","$":{"id":"afs75"},"_":"H. Allioui, M. A. Mohammed, N. Benameur, B. Al-Khateeb, K. H. Abdulkareem, B. Garcia-Zapirain, R. DamaÅ¡eviÄius, R. MaskeliÅ«nas, A multi-agent deep reinforcement learning approach for enhancement of covid-19 ct image segmentation, Journal of personalized medicine 12 (2) (2022) 309."}]},{"#name":"bib-reference","$":{"id":"b76"},"$$":[{"#name":"label","_":"[76]"},{"#name":"reference","$":{"id":"sb76","refId":"76"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Xie"},{"#name":"given-name","_":"N."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A distributed multi-agent formation control method based on deep q learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Front. Neurorobot."}]},{"#name":"volume-nr","_":"16"}]},{"#name":"date","_":"2022"}]},{"#name":"article-number","_":"817168"}]}]},{"#name":"source-text","$":{"id":"afs76"},"_":"N. Xie, Y. Hu, L. Chen, A distributed multi-agent formation control method based on deep q learning, Frontiers in Neurorobotics 16 (2022) 817168."}]},{"#name":"bib-reference","$":{"id":"b77"},"$$":[{"#name":"label","_":"[77]"},{"#name":"reference","$":{"id":"sb77","refId":"77"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhu"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Song"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Ma"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhai"},{"#name":"given-name","_":"L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Path planning of multi-uavs based on deep q-network for energy-efficient data collection in uavs-assisted iot"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Veh. Commun."}]},{"#name":"volume-nr","_":"36"}]},{"#name":"date","_":"2022"}]},{"#name":"article-number","_":"100491"}]}]},{"#name":"source-text","$":{"id":"afs77"},"_":"X. Zhu, L. Wang, Y. Li, S. Song, S. Ma, F. Yang, L. Zhai, Path planning of multi-uavs based on deep q-network for energy-efficient data collection in uavs-assisted iot, Vehicular Communications 36 (2022) 100491."}]},{"#name":"bib-reference","$":{"id":"b78"},"$$":[{"#name":"label","_":"[78]"},{"#name":"reference","$":{"id":"sb78","refId":"78"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Kim"},{"#name":"given-name","_":"K."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep q network to enhance the reinforcement learning for delayed reward system"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Appl. Sci."}]},{"#name":"volume-nr","_":"12"}]},{"#name":"issue-nr","_":"7"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"3520"}]}]}]},{"#name":"source-text","$":{"id":"afs78"},"_":"K. Kim, Multi-agent deep q network to enhance the reinforcement learning for delayed reward system, Applied Sciences 12 (7) (2022) 3520."}]},{"#name":"bib-reference","$":{"id":"b79"},"$$":[{"#name":"label","_":"[79]"},{"#name":"reference","$":{"id":"sb79","refId":"79"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Huang"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Mo"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Behavioral control task supervisor with memory based on reinforcement learning for humanâ€”multi-robot coordination systems"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Front. Inf. Technol. Electron. Eng."}]},{"#name":"volume-nr","_":"23"}]},{"#name":"issue-nr","_":"8"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1174"},{"#name":"last-page","_":"1188"}]}]}]},{"#name":"source-text","$":{"id":"afs79"},"_":"J. Huang, Z. Mo, Z. Zhang, Y. Chen, Behavioral control task supervisor with memory based on reinforcement learning for humanâ€”multi-robot coordination systems, Frontiers of Information Technology & Electronic Engineering 23 (8) (2022) 1174â€“1188."}]},{"#name":"bib-reference","$":{"id":"b80"},"$$":[{"#name":"label","_":"[80]"},{"#name":"reference","$":{"id":"sb80","refId":"80"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xue"},{"#name":"given-name","_":"C."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Resource optimization for multi-unmanned aerial vehicle formation communication based on an improved deep q-network"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Sensors"}]},{"#name":"volume-nr","_":"23"}]},{"#name":"issue-nr","_":"5"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"2667"}]}]}]},{"#name":"source-text","$":{"id":"afs80"},"_":"J. Li, S. Li, C. Xue, Resource optimization for multi-unmanned aerial vehicle formation communication based on an improved deep q-network, Sensors 23 (5) (2023) 2667."}]},{"#name":"bib-reference","$":{"id":"b81"},"$$":[{"#name":"label","_":"[81]"},{"#name":"reference","$":{"id":"sb81","refId":"81"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Bai"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lv"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Smart mobile robot fleet management based on hierarchical multi-agent deep q network towards intelligent manufacturing"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Eng. Appl. Artif. Intell."}]},{"#name":"volume-nr","_":"124"}]},{"#name":"date","_":"2023"}]},{"#name":"article-number","_":"106534"}]}]},{"#name":"source-text","$":{"id":"afs81"},"_":"Y. Bai, Y. Lv, J. Zhang, Smart mobile robot fleet management based on hierarchical multi-agent deep q network towards intelligent manufacturing, Engineering Applications of Artificial Intelligence 124 (2023) 106534."}]},{"#name":"bib-reference","$":{"id":"b82"},"$$":[{"#name":"label","_":"[82]"},{"#name":"reference","$":{"id":"sb82","refId":"82"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Yuan"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Huang"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Pei"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Gu"},{"#name":"given-name","_":"W."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A multi-agent double deep-q-network based on state machine and event stream for flexible job shop scheduling problem"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Adv. Eng. Inform."}]},{"#name":"volume-nr","_":"58"}]},{"#name":"date","_":"2023"}]},{"#name":"article-number","_":"102230"}]}]},{"#name":"source-text","$":{"id":"afs82"},"_":"M. Yuan, H. Huang, Z. Li, C. Zhang, F. Pei, W. Gu, A multi-agent double deep-q-network based on state machine and event stream for flexible job shop scheduling problem, Advanced Engineering Informatics 58 (2023) 102230."}]},{"#name":"bib-reference","$":{"id":"b83"},"$$":[{"#name":"label","_":"[83]"},{"#name":"reference","$":{"id":"sb83","refId":"83"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jia"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chai"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lai"},{"#name":"given-name","_":"C.S."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A multi-agent reinforcement learning-based data-driven method for home energy management"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Smart Grid"}]},{"#name":"volume-nr","_":"11"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"3201"},{"#name":"last-page","_":"3211"}]}]}]},{"#name":"source-text","$":{"id":"afs83"},"_":"X. Xu, Y. Jia, Y. Xu, Z. Xu, S. Chai, C. S. Lai, A multi-agent reinforcement learning-based data-driven method for home energy management, IEEE Transactions on Smart Grid 11 (4) (2020) 3201â€“3211."}]},{"#name":"bib-reference","$":{"id":"b84"},"$$":[{"#name":"label","_":"[84]"},{"#name":"reference","$":{"id":"sb84","refId":"84"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jiang"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Luo"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xiao"},{"#name":"given-name","_":"G."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Data-driven optimal consensus control for discrete-time multi-agent systems with unknown dynamics using reinforcement learning method"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Ind. Electron."}]},{"#name":"volume-nr","_":"64"}]},{"#name":"issue-nr","_":"5"},{"#name":"date","_":"2016"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"4091"},{"#name":"last-page","_":"4100"}]}]}]},{"#name":"source-text","$":{"id":"afs84"},"_":"A. H. Tan, F. P. Bejarano, Y. Zhu, R. Ren, G. Nejat, Deep reinforcement learning for decentralized multi-robot exploration with macro actions, IEEE Robotics and Automation Letters 8 (1) (2022) 272â€“279."}]},{"#name":"bib-reference","$":{"id":"b85"},"$$":[{"#name":"label","_":"[85]"},{"#name":"reference","$":{"id":"sb85","refId":"85"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Yu"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Sun"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Shen"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yue"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jiang"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Guan"},{"#name":"given-name","_":"X."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep reinforcement learning for hvac control in commercial buildings"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Smart Grid"}]},{"#name":"volume-nr","_":"12"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"407"},{"#name":"last-page","_":"419"}]}]}]},{"#name":"source-text","$":{"id":"afs85"},"_":"C. Jiang, Z. Wang, S. Chen, J. Li, H. Wang, J. Xiang, W. Xiao, Attention-shared multi-agent actorâ€“critic-based deep reinforcement learning approach for mobile charging dynamic scheduling in wireless rechargeable sensor networks, Entropy 24 (7) (2022) 965."}]},{"#name":"bib-reference","$":{"id":"b86"},"$$":[{"#name":"label","_":"[86]"},{"#name":"reference","$":{"id":"sb86","refId":"86"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"You"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Feng"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhao"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yan"},{"#name":"given-name","_":"H."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Toward packet routing with fully distributed multiagent deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Syst. Man Cybern.: Syst."}]},{"#name":"volume-nr","_":"52"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2015"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"855"},{"#name":"last-page","_":"868"}]}]}]},{"#name":"source-text","$":{"id":"afs86"},"_":"X. Tan, L. Zhou, H. Wang, Y. Sun, H. Zhao, B.-C. Seet, J. Wei, V. C. Leung, Cooperative multi-agent reinforcement-learning-based distributed dynamic spectrum access in cognitive radio networks, IEEE Internet of Things Journal 9 (19) (2022) 19477â€“19488."}]},{"#name":"bib-reference","$":{"id":"b87"},"$$":[{"#name":"label","_":"[87]"},{"#name":"reference","$":{"id":"sb87","refId":"87"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Guo"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yuan"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhao"},{"#name":"given-name","_":"P."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Admission-based reinforcement-learning algorithm in sequential social dilemmas"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Appl. Sci."}]},{"#name":"volume-nr","_":"13"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1807"}]}]}]},{"#name":"source-text","$":{"id":"afs87"},"_":"Y. Yuan, P. Zhao, T. Guo, H. Jiang, Counterfactual-based action evaluation algorithm in multi-agent reinforcement learning, Applied Sciences 12 (7) (2022) 3439."}]},{"#name":"bib-reference","$":{"id":"b88"},"$$":[{"#name":"label","_":"[88]"},{"#name":"reference","$":{"id":"sb88","refId":"88"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Liang"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Dai"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lyu"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lin"},{"#name":"given-name","_":"B."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Adaptive data collection and offloading in multi-uav-assisted maritime iot systems: A deep reinforcement learning approach"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Remote Sens."}]},{"#name":"volume-nr","_":"15"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"292"}]}]}]},{"#name":"source-text","$":{"id":"afs88"},"_":"O. Naparstek, K. Cohen, Deep multi-user reinforcement learning for distributed dynamic spectrum access, IEEE transactions on wireless communications 18 (1) (2019) 310â€“323."}]},{"#name":"bib-reference","$":{"id":"b89"},"$$":[{"#name":"label","_":"[89]"},{"#name":"reference","$":{"id":"sb89","refId":"89"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhu"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wong"},{"#name":"given-name","_":"Y.D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhu"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Coor-plt: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Transp. Res. C"}]},{"#name":"volume-nr","_":"146"}]},{"#name":"date","_":"2023"}]},{"#name":"article-number","_":"103933"}]}]},{"#name":"source-text","$":{"id":"afs89"},"_":"N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, Y. Jiang, Deep reinforcement learning for user association and resource allocation in heterogeneous cellular networks, IEEE Transactions on Wireless Communications 18 (11) (2019) 5141â€“5152."}]},{"#name":"bib-reference","$":{"id":"b90"},"$$":[{"#name":"label","_":"[90]"},{"#name":"reference","$":{"id":"sb90","refId":"90"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"She"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"J."}]},{"#name":"author","$$":[{"#name":"surname","_":"Bo"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zeng"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Maca: Multi-agent with credit assignment for computation offloading in smart parks monitoring"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Mathematics"}]},{"#name":"volume-nr","_":"10"}]},{"#name":"issue-nr","_":"23"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"4616"}]}]}]},{"#name":"source-text","$":{"id":"afs90"},"_":"Z. Ding, Q. Fu, J. Chen, Y. Lu, H. Wu, N. Fang, B. Xing, Maqmc: Multi-agent deep q-network for multi-zone residential hvac control., CMES-Computer Modeling in Engineering & Sciences 136 (3) (2023) 0."}]},{"#name":"bib-reference","$":{"id":"b91"},"$$":[{"#name":"label","_":"[91]"},{"#name":"reference","$":{"id":"sb91","refId":"91"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhan"},{"#name":"given-name","_":"G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhou"},{"#name":"given-name","_":"D."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"Z."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multiple-uav reinforcement learning algorithm based on improved ppo in ray framework"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Drones"}]},{"#name":"volume-nr","_":"6"}]},{"#name":"issue-nr","_":"7"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"166"}]}]}]},{"#name":"source-text","$":{"id":"afs91"},"_":"W. Jiang, W. Yu, W. Wang, T. Huang, Multi-agent reinforcement learning for joint cooperative spectrum sensing and channel access in cognitive uav networks, Sensors 22 (4) (2022) 1651."}]},{"#name":"bib-reference","$":{"id":"b92"},"$$":[{"#name":"label","_":"[92]"},{"#name":"reference","$":{"id":"sb92","refId":"92"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Aslam"},{"#name":"given-name","_":"N."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xia"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hadi"},{"#name":"given-name","_":"M.U."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Optimal wireless charging inclusive of intellectual routing based on sarsa learning in renewable wireless sensor networks"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Sens. J."}]},{"#name":"volume-nr","_":"19"}]},{"#name":"issue-nr","_":"18"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"8340"},{"#name":"last-page","_":"8351"}]}]}]},{"#name":"source-text","$":{"id":"afs92"},"_":"Y. S. Nasir, D. Guo, Multi-agent deep reinforcement learning for dynamic power allocation in wireless networks, IEEE Journal on Selected Areas in Communications 37 (10) (2019) 2239â€“2250."}]},{"#name":"bib-reference","$":{"id":"b93"},"$$":[{"#name":"label","_":"[93]"},{"#name":"reference","$":{"id":"sb93","refId":"93"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Alfakih"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hassan"},{"#name":"given-name","_":"M.M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Gumaei"},{"#name":"given-name","_":"A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Savaglio"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Fortino"},{"#name":"given-name","_":"G."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Task offloading and resource allocation for mobile edge computing by deep reinforcement learning based on sarsa"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Access"}]},{"#name":"volume-nr","_":"8"}]},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"54074"},{"#name":"last-page","_":"54084"}]}]}]},{"#name":"source-text","$":{"id":"afs93"},"_":"K. Hu, J. Jin, F. Zheng, L. Weng, Y. Ding, Overview of behavior recognition based on deep learning, Artificial Intelligence Review 56 (3) (2023) 1833â€“1865."}]},{"#name":"bib-reference","$":{"id":"b94"},"$$":[{"#name":"label","_":"[94]"},{"#name":"reference","$":{"id":"sb94","refId":"94"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Yu"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Gu"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Liu"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lai"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"5 g multi-slices bi-level resource allocation by reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Mathematics"}]},{"#name":"volume-nr","_":"11"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"760"}]}]}]},{"#name":"source-text","$":{"id":"afs94"},"_":"H. Zhang, H. Jiang, Y. Luo, G. Xiao, Data-driven optimal consensus control for discrete-time multi-agent systems with unknown dynamics using reinforcement learning method, IEEE Transactions on Industrial Electronics 64 (5) (2016) 4091â€“4100."}]},{"#name":"bib-reference","$":{"id":"b95"},"$$":[{"#name":"label","_":"[95]"},{"#name":"reference","$":{"id":"sb95","refId":"95"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"G."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wei"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jiang"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhao"},{"#name":"given-name","_":"M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Qi"},{"#name":"given-name","_":"H."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A multi-auv maritime target search method for moving and invisible objects based on multi-agent deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Sensors"}]},{"#name":"volume-nr","_":"22"}]},{"#name":"issue-nr","_":"21"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"8562"}]}]}]},{"#name":"source-text","$":{"id":"afs95"},"_":"L. Yu, Y. Sun, Z. Xu, C. Shen, D. Yue, T. Jiang, X. Guan, Multi-agent deep reinforcement learning for hvac control in commercial buildings, IEEE Transactions on Smart Grid 12 (1) (2020) 407â€“419."}]},{"#name":"bib-reference","$":{"id":"b96"},"$$":[{"#name":"label","_":"[96]"},{"#name":"reference","$":{"id":"sb96","refId":"96"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Yan"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"B."}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"C."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A networked multi-agent reinforcement learning approach for cooperative femtocaching assisted wireless heterogeneous networks"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Comput. Netw."}]},{"#name":"volume-nr","_":"220"}]},{"#name":"date","_":"2023"}]},{"#name":"article-number","_":"109513"}]}]},{"#name":"source-text","$":{"id":"afs96"},"_":"Y. Yan, B. Zhang, C. Li, A networked multi-agent reinforcement learning approach for cooperative femtocaching assisted wireless heterogeneous networks, Computer Networks 220 (2023) 109513."}]},{"#name":"bib-reference","$":{"id":"b97"},"$$":[{"#name":"label","_":"[97]"},{"#name":"reference","$":{"id":"sb97","refId":"97"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Z."}]},{"#name":"author","$$":[{"#name":"surname","_":"Sui"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Qin"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lu"},{"#name":"given-name","_":"H."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"State super sampling soft actor-critic algorithm for multi-auv hunting in 3d underwater environment"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"J. Mar. Sci. Eng."}]},{"#name":"volume-nr","_":"11"}]},{"#name":"issue-nr","_":"7"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1257"}]}]}]},{"#name":"source-text","$":{"id":"afs97"},"_":"Z. Wang, Y. Sui, H. Qin, H. Lu, State super sampling soft actor-critic algorithm for multi-auv hunting in 3d underwater environment, Journal of Marine Science and Engineering 11 (7) (2023) 1257."}]},{"#name":"bib-reference","$":{"id":"b98"},"$$":[{"#name":"label","_":"[98]"},{"#name":"reference","$":{"id":"sb98","refId":"98"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"S."}]},{"#name":"author","$$":[{"#name":"surname","_":"Jia"},{"#name":"given-name","_":"Y."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"F."}]},{"#name":"author","$$":[{"#name":"surname","_":"Qin"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Gao"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhou"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Collaborative decision-making method for multi-uav based on multiagent reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Access"}]},{"#name":"volume-nr","_":"10"}]},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"91385"},{"#name":"last-page","_":"91396"}]}]}]},{"#name":"source-text","$":{"id":"afs98"},"_":"S. Li, Y. Jia, F. Yang, Q. Qin, H. Gao, Y. Zhou, Collaborative decision-making method for multi-uav based on multiagent reinforcement learning, IEEE Access 10 (2022) 91385â€“91396."}]},{"#name":"bib-reference","$":{"id":"b99"},"$$":[{"#name":"label","_":"[99]"},{"#name":"reference","$":{"id":"sb99","refId":"99"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"R."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"X."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xiong"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Ma"},{"#name":"given-name","_":"Q."}]},{"#name":"author","$$":[{"#name":"surname","_":"Peng"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Kernel-based multiagent reinforcement learning for near-optimal formation control of mobile robots"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Appl. Intell."}]},{"#name":"volume-nr","_":"53"}]},{"#name":"issue-nr","_":"10"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"12736"},{"#name":"last-page","_":"12748"}]}]}]},{"#name":"source-text","$":{"id":"afs99"},"_":"R. Zhang, X. Xu, X. Zhang, Q. Xiong, Q. Ma, Y. Peng, Kernel-based multiagent reinforcement learning for near-optimal formation control of mobile robots, Applied Intelligence 53 (10) (2023) 12736â€“12748."}]},{"#name":"bib-reference","$":{"id":"b100"},"$$":[{"#name":"label","_":"[100]"},{"#name":"reference","$":{"id":"sb100","refId":"100"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Ying"},{"#name":"given-name","_":"C.-s."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chow"},{"#name":"given-name","_":"A.H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Nguyen"},{"#name":"given-name","_":"H.T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chin"},{"#name":"given-name","_":"K.-S."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep reinforcement learning for adaptive coordinated metro service operations with flexible train composition"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Transp. Res. B"}]},{"#name":"volume-nr","_":"161"}]},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"36"},{"#name":"last-page","_":"59"}]}]}]},{"#name":"source-text","$":{"id":"afs100"},"_":"C.-s. Ying, A. H. Chow, H. T. Nguyen, K.-S. Chin, Multi-agent deep reinforcement learning for adaptive coordinated metro service operations with flexible train composition, Transportation Research Part B: Methodological 161 (2022) 36â€“59."}]},{"#name":"bib-reference","$":{"id":"b101"},"$$":[{"#name":"label","_":"[101]"},{"#name":"reference","$":{"id":"sb101","refId":"101"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"L."}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"K."}]},{"#name":"author","$$":[{"#name":"surname","_":"Pan"},{"#name":"given-name","_":"C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"W."}]},{"#name":"author","$$":[{"#name":"surname","_":"Aslam"},{"#name":"given-name","_":"N."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hanzo"},{"#name":"given-name","_":"L."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multi-agent deep reinforcement learning-based trajectory planning for multi-uav assisted mobile edge computing"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Trans. Cogn. Commun. Netw."}]},{"#name":"volume-nr","_":"7"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"73"},{"#name":"last-page","_":"84"}]}]}]},{"#name":"source-text","$":{"id":"afs101"},"_":"K. G. Vamvoudakis, F. L. Lewis, G. R. Hudas, Multi-agent differential graphical games: Online adaptive learning solution for synchronization with optimality, Automatica 48 (8) (2012) 1598â€“1611."}]},{"#name":"bib-reference","$":{"id":"b102"},"$$":[{"#name":"label","_":"[102]"},{"#name":"reference","$":{"id":"sb102","refId":"102"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"Kai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"Jiasheng"}]},{"#name":"author","$$":[{"#name":"surname","_":"Weng"},{"#name":"given-name","_":"Liguo"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Yanwen"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zheng"},{"#name":"given-name","_":"Fei"}]},{"#name":"author","$$":[{"#name":"surname","_":"Pang"},{"#name":"given-name","_":"Zichao"}]},{"#name":"author","$$":[{"#name":"surname","_":"Xia"},{"#name":"given-name","_":"Min"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A novel federated learning approach based on the confidence of federated kalman filters"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"International Journal of Machine Learning and Cybernetics"}]},{"#name":"volume-nr","_":"12"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"3607"},{"#name":"last-page","_":"3627"}]}]}]},{"#name":"source-text","$":{"id":"afs102"},"_":"X. You, X. Li, Y. Xu, H. Feng, J. Zhao, H. Yan, Toward packet routing with fully distributed multiagent deep reinforcement learning, IEEE Transactions on Systems, Man, and Cybernetics: Systems 52 (2) (2015) 855â€“868."}]},{"#name":"bib-reference","$":{"id":"b103"},"$$":[{"#name":"label","_":"[103]"},{"#name":"reference","$":{"id":"sb103","refId":"103"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"Kai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"Jiasheng"}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Yaogen"}]},{"#name":"author","$$":[{"#name":"surname","_":"Lu"},{"#name":"given-name","_":"Meixia"}]},{"#name":"author","$$":[{"#name":"surname","_":"Weng"},{"#name":"given-name","_":"Liguo"}]},{"#name":"author","$$":[{"#name":"surname","_":"Xia"},{"#name":"given-name","_":"Min"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Fedgcn: federated learning-based graph convolutional networks for non-euclidean spatial data"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Mathematics"}]},{"#name":"volume-nr","_":"10"}]},{"#name":"issue-nr","_":"6"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1000"}]}]}]},{"#name":"source-text","$":{"id":"afs103"},"_":"T. Guo, Y. Yuan, P. Zhao, Admission-based reinforcement-learning algorithm in sequential social dilemmas, Applied Sciences 13 (3) (2023) 1807."}]},{"#name":"bib-reference","$":{"id":"b104"},"$$":[{"#name":"label","_":"[104]"},{"#name":"reference","$":{"id":"sb104","refId":"104"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"WY"}]},{"#name":"author","$$":[{"#name":"surname","_":"Bai"},{"#name":"given-name","_":"CJ"}]},{"#name":"author","$$":[{"#name":"surname","_":"Cai"},{"#name":"given-name","_":"C"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Review on sparse reward in deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Computer Science"}]},{"#name":"volume-nr","_":"47"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"183"},{"#name":"last-page","_":"191"}]}]}]},{"#name":"source-text","$":{"id":"afs104"},"_":"Z. Liang, Y. Dai, L. Lyu, B. Lin, Adaptive data collection and offloading in multi-uav-assisted maritime iot systems: A deep reinforcement learning approach, Remote Sensing 15 (2) (2023) 292."}]},{"#name":"bib-reference","$":{"id":"b105"},"$$":[{"#name":"label","_":"[105]"},{"#name":"reference","$":{"id":"sb105","refId":"105"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Salimans"},{"#name":"given-name","_":"Tim"}]},{"#name":"author","$$":[{"#name":"surname","_":"Ho"},{"#name":"given-name","_":"Jonathan"}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"Xi"}]},{"#name":"author","$$":[{"#name":"surname","_":"Sidor"},{"#name":"given-name","_":"Szymon"}]},{"#name":"author","$$":[{"#name":"surname","_":"Sutskever"},{"#name":"given-name","_":"Ilya"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Evolution strategies as a scalable alternative to reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2017"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref4","href":"arxiv:1703.03864","role":"http://www.elsevier.com/xml/linking-roles/preprint","type":"simple"},"_":"arXiv:1703.03864"}]}]},{"#name":"source-text","$":{"id":"afs105"},"_":"D. Li, F. Zhu, T. Chen, Y. D. Wong, C. Zhu, J. Wu, Coor-plt: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning, Transportation Research Part C: Emerging Technologies 146 (2023) 103933."}]},{"#name":"bib-reference","$":{"id":"b106"},"$$":[{"#name":"label","_":"[106]"},{"#name":"reference","$":{"id":"sb106","refId":"106"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"LÃ¼"},{"#name":"given-name","_":"Shuai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Han"},{"#name":"given-name","_":"Shuai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhou"},{"#name":"given-name","_":"Wenbo"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Junwei"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Recruitment-imitation mechanism for evolutionary reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Information Sciences"}]},{"#name":"volume-nr","_":"553"}]},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"172"},{"#name":"last-page","_":"188"}]}]}]},{"#name":"source-text","$":{"id":"afs106"},"_":"L. She, J. Wang, Y. Bo, Y. Zeng, Maca: Multi-agent with credit assignment for computation offloading in smart parks monitoring, Mathematics 10 (23) (2022) 4616."}]},{"#name":"bib-reference","$":{"id":"b107"},"$$":[{"#name":"label","_":"[107]"},{"#name":"reference","$":{"id":"sb107","refId":"107"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Holland"},{"#name":"given-name","_":"JohnH"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Adaptation in natural and artificial systems, univ. of mich. press"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Ann Arbor"}]},{"#name":"volume-nr","_":"7"}]},{"#name":"date","_":"1975"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"390"},{"#name":"last-page","_":"401"}]}]}]},{"#name":"source-text","$":{"id":"afs107"},"_":"G. Zhan, X. Zhang, Z. Li, L. Xu, D. Zhou, Z. Yang, Multiple-uav reinforcement learning algorithm based on improved ppo in ray framework, Drones 6 (7) (2022) 166."}]},{"#name":"bib-reference","$":{"id":"b108"},"$$":[{"#name":"label","_":"[108]"},{"#name":"reference","$":{"id":"sb108","refId":"108"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Koza"},{"#name":"given-name","_":"John R"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Genetic programming as a means for programming computers by natural selection"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Statistics and computing"}]},{"#name":"volume-nr","_":"4"}]},{"#name":"date","_":"1994"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"87"},{"#name":"last-page","_":"112"}]}]}]},{"#name":"source-text","$":{"id":"afs108"},"_":"N. Aslam, K. Xia, M. U. Hadi, Optimal wireless charging inclusive of intellectual routing based on sarsa learning in renewable wireless sensor networks, IEEE Sensors Journal 19 (18) (2019) 8340â€“8351."}]},{"#name":"bib-reference","$":{"id":"b109"},"$$":[{"#name":"label","_":"[109]"},{"#name":"reference","$":{"id":"sb109","refId":"109"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Storn"},{"#name":"given-name","_":"Rainer"}]},{"#name":"author","$$":[{"#name":"surname","_":"Price"},{"#name":"given-name","_":"Kenneth"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Differential evolutionâ€“a simple and efficient heuristic for global optimization over continuous spaces"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Journal of global optimization"}]},{"#name":"volume-nr","_":"11"}]},{"#name":"date","_":"1997"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"341"},{"#name":"last-page","_":"359"}]}]}]},{"#name":"source-text","$":{"id":"afs109"},"_":"T. Alfakih, M. M. Hassan, A. Gumaei, C. Savaglio, G. Fortino, Task offloading and resource allocation for mobile edge computing by deep reinforcement learning based on sarsa, IEEE Access 8 (2020) 54074â€“54084."}]},{"#name":"bib-reference","$":{"id":"b110"},"$$":[{"#name":"label","_":"[110]"},{"#name":"other-ref","$":{"id":"sb110","refId":"110"},"$$":[{"#name":"textref","_":"Ingo Rechenberg, Evolutionsstrategien, in: Simulationsmethoden in der Medizin und Biologie: Workshop, Hannover, 29. Sept.â€“1. Okt. 1977, 1978, pp. 83â€“114."}]}]},{"#name":"bib-reference","$":{"id":"b111"},"$$":[{"#name":"label","_":"[111]"},{"#name":"reference","$":{"id":"sb111","refId":"111"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Fogel"},{"#name":"given-name","_":"David B"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Artificial intelligence through simulated evolution"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"1998"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"227"},{"#name":"last-page","_":"296"}]}]}]},{"#name":"source-text","$":{"id":"afs111"},"_":"G. Wang, F. Wei, Y. Jiang, M. Zhao, K. Wang, H. Qi, A multi-auv maritime target search method for moving and invisible objects based on multi-agent deep reinforcement learning, Sensors 22 (21) (2022) 8562."}]},{"#name":"bib-reference","$":{"id":"b112"},"$$":[{"#name":"label","_":"[112]"},{"#name":"reference","$":{"id":"sb112","refId":"112"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Mao"},{"#name":"given-name","_":"Yubing"}]},{"#name":"author","$$":[{"#name":"surname","_":"Gao"},{"#name":"given-name","_":"Farong"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Qizhong"}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"Zhangyi"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"An auv target-tracking method combining imitation learning and deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Journal of Marine Science and Engineering"}]},{"#name":"volume-nr","_":"10"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2022"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"383"}]}]}]},{"#name":"source-text","$":{"id":"afs112"},"_":"J. Yao, L. Yan, Z. Xu, P. Wang, X. Zhao, Collaborative decision-making method of emergency response for highway incidents, Sustainability 15 (3) (2023) 2099."}]},{"#name":"bib-reference","$":{"id":"b113"},"$$":[{"#name":"label","_":"[113]"},{"#name":"reference","$":{"id":"sb113","refId":"113"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Torabi"},{"#name":"given-name","_":"Faraz"}]},{"#name":"author","$$":[{"#name":"surname","_":"Warnell"},{"#name":"given-name","_":"Garrett"}]},{"#name":"author","$$":[{"#name":"surname","_":"Stone"},{"#name":"given-name","_":"Peter"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Behavioral cloning from observation"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2018"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref5","href":"arxiv:1805.01954","role":"http://www.elsevier.com/xml/linking-roles/preprint","type":"simple"},"_":"arXiv:1805.01954"}]}]},{"#name":"source-text","$":{"id":"afs113"},"_":"Y. Zhou, Z. Liu, H. Shi, S. Li, N. Ning, F. Liu, X. Gao, Cooperative multi-agent target searching: a deep reinforcement learning approach based on parallel hindsight experience replay, Complex & Intelligent Systems 0 (2023) 1â€“12."}]},{"#name":"bib-reference","$":{"id":"b114"},"$$":[{"#name":"label","_":"[114]"},{"#name":"reference","$":{"id":"sb114","refId":"114"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Ng"},{"#name":"given-name","_":"Andrew Y"}]},{"#name":"author","$$":[{"#name":"surname","_":"Russell"},{"#name":"given-name","_":"Stuart"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Algorithms for inverse reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Icml"}]},{"#name":"volume-nr","_":"1"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2000"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"2"}]}]}]},{"#name":"source-text","$":{"id":"afs114"},"_":"Z. Lu, C. Zhong, M. C. Gursoy, Dynamic channel access and power control in wireless interference networks via multi-agent deep reinforcement learning, IEEE Transactions on Vehicular Technology 71 (2) (2021) 1588â€“1601."}]},{"#name":"bib-reference","$":{"id":"b115"},"$$":[{"#name":"label","_":"[115]"},{"#name":"reference","$":{"id":"sb115","refId":"115"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Ho"},{"#name":"given-name","_":"Jonathan"}]},{"#name":"author","$$":[{"#name":"surname","_":"Ermon"},{"#name":"given-name","_":"Stefano"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Generative adversarial imitation learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Advances in neural information processing systems"}]},{"#name":"volume-nr","_":"29"}]},{"#name":"date","_":"2016"}]}]}]},{"#name":"source-text","$":{"id":"afs115"},"_":"P. Zhao, Y. Yuan, T. Guo, Extensible hierarchical multi-agent reinforcement-learning algorithm in traffic signal control, Applied Sciences 12 (24) (2022) 12783."}]},{"#name":"bib-reference","$":{"id":"b116"},"$$":[{"#name":"label","_":"[116]"},{"#name":"reference","$":{"id":"sb116","refId":"116"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"Kai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Yaogen"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Shuai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"Jiasheng"}]},{"#name":"author","$$":[{"#name":"surname","_":"Gong"},{"#name":"given-name","_":"Sheng"}]},{"#name":"author","$$":[{"#name":"surname","_":"Jiang"},{"#name":"given-name","_":"Shanshan"}]},{"#name":"author","$$":[{"#name":"surname","_":"Weng"},{"#name":"given-name","_":"Liguo"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Fedmmd: a federated weighting algorithm considering non-iid and local model deviation"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Expert Systems with Applications"}]},{"#name":"volume-nr","_":"237"}]},{"#name":"date","_":"2024"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"121463"}]}]}]},{"#name":"source-text","$":{"id":"afs116"},"_":"H. Xu, Q. Fang, C. Hu, Y. Hu, Q. Yin, Mira: Model-based imagined rollouts augmentation for non-stationarity in multi-agent systems, Mathematics 10 (17) (2022) 3059."}]},{"#name":"bib-reference","$":{"id":"b117"},"$$":[{"#name":"label","_":"[117]"},{"#name":"reference","$":{"id":"sb117","refId":"117"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hu"},{"#name":"given-name","_":"Kai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Yaogen"}]},{"#name":"author","$$":[{"#name":"surname","_":"Xia"},{"#name":"given-name","_":"Min"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"Jiasheng"}]},{"#name":"author","$$":[{"#name":"surname","_":"Lu"},{"#name":"given-name","_":"Meixia"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Shuai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Weng"},{"#name":"given-name","_":"Liguo"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Federated learning: a distributed shared machine learning method"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Complexity"}]},{"#name":"volume-nr","_":"2021"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"8261663"}]}]}]},{"#name":"source-text","$":{"id":"afs117"},"_":"Y. Tian, K.-R. Kladny, Q. Wang, Z. Huang, O. Fink, Multi-agent actor-critic with time dynamical opponent model, Neurocomputing 517 (2023) 165â€“172."}]},{"#name":"bib-reference","$":{"id":"b118"},"$$":[{"#name":"label","_":"[118]"},{"#name":"reference","$":{"id":"sb118","refId":"118"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Liang"},{"#name":"given-name","_":"Zhiuxan"}]},{"#name":"author","$$":[{"#name":"surname","_":"Cao"},{"#name":"given-name","_":"Jiannong"}]},{"#name":"author","$$":[{"#name":"surname","_":"Jiang"},{"#name":"given-name","_":"Shan"}]},{"#name":"author","$$":[{"#name":"surname","_":"Saxena"},{"#name":"given-name","_":"Divya"}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"Jinlin"}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"Huafeng"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"From multi-agent to multi-robot: a scalable training and evaluation platform for multi-robot reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2022"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref6","href":"arxiv:2206.09590","role":"http://www.elsevier.com/xml/linking-roles/preprint","type":"simple"},"_":"arXiv:2206.09590"}]}]},{"#name":"source-text","$":{"id":"afs118"},"_":"L. Wang, K. Wang, C. Pan, W. Xu, N. Aslam, L. Hanzo, Multi-agent deep reinforcement learning-based trajectory planning for multi-uav assisted mobile edge computing, IEEE Transactions on Cognitive Communications and Networking 7 (1) (2020) 73â€“84."}]},{"#name":"bib-reference","$":{"id":"b119"},"$$":[{"#name":"label","_":"[119]"},{"#name":"reference","$":{"id":"sb119","refId":"119"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Oroojlooy"},{"#name":"given-name","_":"A."}]},{"#name":"author","$$":[{"#name":"surname","_":"Hajinezhad"},{"#name":"given-name","_":"D."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A review of cooperative multi-agent deep reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Appl. Intell."}]},{"#name":"volume-nr","_":"53"}]},{"#name":"issue-nr","_":"11"},{"#name":"date","_":"2023"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"13677"},{"#name":"last-page","_":"13722"}]}]}]},{"#name":"source-text","$":{"id":"afs119"},"_":"A. Oroojlooy, D. Hajinezhad, A review of cooperative multi-agent deep reinforcement learning, Applied Intelligence 53 (11) (2023) 13677â€“13722."}]},{"#name":"bib-reference","$":{"id":"b120"},"$$":[{"#name":"label","_":"[120]"},{"#name":"reference","$":{"id":"sb120","refId":"120"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Luo"},{"#name":"given-name","_":"F.-M."}]},{"#name":"author","$$":[{"#name":"surname","_":"Xu"},{"#name":"given-name","_":"T."}]},{"#name":"author","$$":[{"#name":"surname","_":"Lai"},{"#name":"given-name","_":"H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"X.-H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"W."}]},{"#name":"author","$$":[{"#name":"surname","_":"Yu"},{"#name":"given-name","_":"Y."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A survey on model-based reinforcement learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Sci. China Inf. Sci."}]},{"#name":"volume-nr","_":"67"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2024"}]},{"#name":"article-number","_":"121101"}]}]},{"#name":"source-text","$":{"id":"afs120"},"_":"F.-M. Luo, T. Xu, H. Lai, X.-H. Chen, W. Zhang, Y. Yu, A survey on model-based reinforcement learning, Science China Information Sciences 67 (2) (2024) 121101."}]}]}]}],"floats":[],"footnotes":[],"attachments":[],"sourceTextMap":{"1":"G. Weiss, Multiagent systems: a modern approach to distributed artificial intelligence, MIT press, 0 (1999) 0.","2":"S. Gronauer, K. Diepold, Multi-agent deep reinforcement learning: a survey, Artificial Intelligence Review 0 (2022) 1â€“49.","3":"K. Hu, E. Zhang, M. Xia, L. Weng, H. Lin, Mcanet: a multi-branch network for cloud/snow segmentation in high-resolution remote sensing images, Remote Sensing 15 (4) (2023) 1055.","4":"K. Hu, T. Wang, C. Shen, C. Weng, F. Zhou, M. Xia, L. Weng, Overview of underwater 3d reconstruction technology based on optical images, Journal of Marine Science and Engineering 11 (5) (2023) 949.","6":"C. Claus, C. Boutilier, The dynamics of reinforcement learning in cooperative multiagent systems, AAAI/IAAI 1998 (746-752) (1998) 2.","7":"P. Stone, M. Veloso, Multiagent systems: A survey from a machine learning perspective, Autonomous Robots 8 (2000) 345â€“383.","8":"Y. Shoham, R. Powers, T. Grenager, Multi-agent reinforcement learning: a critical survey, Tech. rep., Citeseer (2003).","9":"P. J. Hoen, K. Tuyls, L. Panait, S. Luke, J. A. La Poutre, An overview of cooperative and competitive multiagent learning, in: Learning and Adaption in Multi-Agent Systems: First International Workshop, LAMAS 2005, Utrecht, The Netherlands, July 25 2005, Revised Selected Papers, Springer, 0 (2006) 1â€“46.","10":"L. Busoniu, R. Babuska, B. De Schutter, A comprehensive survey of multiagent reinforcement learning, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 38 (2) (2008) 156â€“172.","11":"L. Matignon, G. J. Laurent, N. Le Fort-Piat, Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems, The Knowledge Engineering Review 27 (1) (2012) 1â€“31.","12":"K. Tuyls, G. Weiss, Multiagent learning: Basics, challenges, and prospects, Ai Magazine 33 (3) (2012) 41â€“41.","13":"B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, I. Mordatch, Emergent tool use from multi-agent autocurricula, arXiv preprint arXiv:1909.07528 (2019).","14":"C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, others, Dota 2 with large scale deep reinforcement learning, arXiv preprint arXiv:1912.06680 (2019).","15":"M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, others, Human-level performance in 3d multiplayer games with population-based reinforcement learning, Science 364 (6443) (2019) 859â€“865.","16":"O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, others, Grandmaster level in starcraft ii using multi-agent reinforcement learning, Nature 575 (7782) (2019) 350â€“354.","17":"T. T. Nguyen, N. D. Nguyen, S. Nahavandi, Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications, IEEE transactions on cybernetics 50 (9) (2020) 3826â€“3839.","18":"P. Hernandez-Leal, B. Kartal, M. E. Taylor, A survey and critique of multiagent deep reinforcement learning, Autonomous Agents and Multi-Agent Systems 33 (6) (2019) 750â€“797.","19":"A. Oroojlooy, D. Hajinezhad, A review of cooperative multi-agent deep reinforcement learning, Applied Intelligence 53 (11) (2023) 13677â€“13722.","20":"F. L. Da Silva, A. H. R. Costa, A survey on transfer learning for multiagent reinforcement learning systems, Journal of Artificial Intelligence Research 64 (2019) 645â€“703.","21":"F. L. Da Silva, G. Warnell, A. H. R. Costa, P. Stone, Agents teaching agents: a survey on inter-agent transfer learning, Autonomous Agents and Multi-Agent Systems 34 (2020) 1â€“17.","22":"A. Lazaridou, M. Baroni, Emergent multi-agent communication in the deep learning era, arXiv preprint arXiv:2006.02419 (2020).","23":"K. Zhang, Z. Yang, T. BaÅŸar, Multi-agent reinforcement learning: A selective overview of theories and algorithms, Handbook of reinforcement learning and control 0 (2021) 321â€“384.","24":"M. Guojun, G. Shimin, Improved q-learning algorithm and its application in path planning, Journal of Taiyuan University of Technology 52 (1) (2021) 91â€“97.","25":"B. J. A. KrÃ¶se, Learning from delayed rewards, Robotics Auton. Syst. 15 (1995) 233â€“235.","26":"M. Guo, Y. Liu, J. Malec, A new q-learning algorithm based on the metropolis criterion, IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 34 (5) (2004) 2140â€“2143.","27":"Y.-P. Lin, X.-Y. Li, Reinforcement learning based on local state feature learning and policy adjustment, Information Sciences 154 (1-2) (2003) 59â€“70.","28":"R. Sharma, M. Gopal, A markov game-adaptive fuzzy controller for robot manipulators, IEEE Transactions on Fuzzy Systems 16 (1) (2008) 171â€“186.","29":"H. Boubertakh, M. Tadjine, P.-Y. Glorennec, A new mobile robot navigation method using fuzzy logic and a modified q-learning algorithm, Journal of Intelligent & Fuzzy Systems 21 (1, 2) (2010) 113â€“119.","30":"M. Rahimiyan, H. R. Mashhadi, An adaptive q-learning algorithm developed for agent-based computational modeling of electricity market, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 40 (5) (2010) 547â€“556.","31":"K.-S. Hwang, S.-W. Tan, C.-C. Chen, Cooperative strategy based on adaptive q-learning for robot soccer systems, IEEE Transactions on Fuzzy Systems 12 (4) (2004) 569â€“576.","32":"Y. Zhou, F. Zhou, Y. Wu, R. Q. Hu, Y. Wang, Subcarrier assignment schemes based on q-learning in wideband cognitive radio networks, IEEE Transactions on Vehicular Technology 69 (1) (2019) 1168â€“1172.","33":"W.-C. Chung, C.-J. Chang, K.-T. Feng, Y.-Y. Chen, An mimo configuration mode and mcs level selection scheme by fuzzy q-learning for hspa+ systems, IEEE Transactions on Mobile Computing 11 (7) (2012) 1151â€“1162.","34":"F. Shams, G. Bacci, M. Luise, Energy-efficient power control for multiple-relay cooperative networks using q-learning, IEEE Transactions on Wireless Communications 14 (3) (2014) 1567â€“1580.","35":"X. Zhang, H. Li, J. Peng, W. Liu, others, A cooperative-learning path planning algorithm for originâ€“destination pairs in urban road networks, Mathematical Problems in Engineering 2015 (2015) 0.","36":"P. Zhu, X. Fang, Multi-uav cooperative task assignment based on half random q-learning, Symmetry 13 (12) (2021) 2417.","37":"T. Zhang, X. Fang, Z. Wang, Y. Liu, A. Nallanathan, Stochastic game based cooperative alternating q-learning caching in dynamic d2d networks, IEEE Transactions on Vehicular Technology 70 (12) (2021) 13255â€“13269.","38":"M. A. Wiering, H. Van Hasselt, Two novel on-policy reinforcement learning algorithms based on td (Î»)-methods, in: 2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, IEEE, 0 (2007) 280â€“287.","39":"N. Aissani, B. Beldjilali, D. Trentesaux, Dynamic scheduling of maintenance tasks in the petroleum industry: A reinforcement approach, Engineering Applications of Artificial Intelligence 22 (7) (2009) 1089â€“1103.","40":"V. Derhami, V. J. Majd, M. N. Ahmadabadi, Exploration and exploitation balance management in fuzzy reinforcement learning, Fuzzy sets and systems 161 (4) (2010) 578â€“595.","41":"M. Andrecut, M. Ali, Deep-sarsa: A reinforcement learning algorithm for autonomous navigation, International Journal of Modern Physics C 12 (10) (2001) 1513â€“1523.","42":"M. H. Olyaei, H. Jalali, A. Olyaei, A. Noori, Implement deep sarsa in grid world with changing obstacles and testing against new environment, in: Fundamental Research in Electrical Engineering: The Selected Papers of The First International Conference on Fundamental Research in Electrical Engineering, Springer, 0 (2019) 267â€“279.","43":"W. Luo, Q. Tang, C. Fu, P. Eberhard, Deep-sarsa based multi-uav path planning and obstacle avoidance in a dynamic environment, in: Advances in Swarm Intelligence: 9th International Conference, ICSI 2018, Shanghai, China, June (2018) 17-22, Proceedings, Part II 9, Springer, 0 (2018) 102â€“111.","44":"Z.-x. Xu, L. Cao, X.-l. Chen, C.-x. Li, Y.-l. Zhang, J. Lai, Deep reinforcement learning with sarsa and q-learning: A hybrid approach, IEICE TRANSACTIONS on Information and Systems 101 (9) (2018) 2315â€“2322.","45":"R. Bellman, Dynamic programming, Science 153 (3731) (1966) 34â€“37.","46":"A. A. Amini, T. E. Weymouth, R. C. Jain, Using dynamic programming for solving variational problems in vision, IEEE Transactions on pattern analysis and machine intelligence 12 (9) (1990) 855â€“867.","47":"N. Merlet, J. Zerubia, New prospects in line detection by dynamic programming, IEEE Transactions on Pattern Analysis and Machine Intelligence 18 (4) (1996) 426â€“431.","48":"M. Buckley, J. Yang, Regularised shortest-path extraction, Pattern Recognition Letters 18 (7) (1997) 621â€“629.","49":"F. Jiang, K. Zhang, J. Hu, S. Wang, Neural network-based intelligent computing algorithms for discrete-time optimal control with the application to a cyberphysical power system, Complexity 2021 (2021) 1â€“10.","50":"P. Werbos, Advanced forecasting methods for global crisis warning and models of intelligence, General System Yearbook 0 (1977) 25â€“38.","51":"W. T. Miller, R. S. Sutton, P. J. Werbos, A menu of designs for reinforcement learning over time (1995).","52":"W. B. Powell, Approximate Dynamic Programming: Solving the curses of dimensionality, John Wiley & Sons, 703 (2007) 0","53":"M. Joy, N. S. Kaisare, Approximate dynamic programming-based control of distributed parameter systems, Asia-Pacific Journal of Chemical Engineering 6 (3) (2011) 452â€“459.","54":"F.-Y. Wang, J. Zhang, Q. Wei, X. Zheng, L. Li, Pdp: parallel dynamic programming, IEEE/CAA Journal of Automatica Sinica 4 (1) (2017) 1â€“5.","55":"B. Luo, D. Liu, T. Huang, X. Yang, H. Ma, Multi-step heuristic dynamic programming for optimal control of nonlinear discrete-time systems, Information Sciences 411 (2017) 66â€“83.","56":"M. A. Voelkel, A.-L. Sachs, U. W. Thonemann, An aggregation-based approximate dynamic programming approach for the periodic review model with random yield, European Journal of Operational Research 281 (2) (2020) 286â€“298.","57":"M. L. Puterman, Markov decision processes: Discrete stochastic dynamic programming (1994).","58":"D. Liu, D. Wang, D. Zhao, Q. Wei, N. Jin, Neural-network-based optimal control for a class of unknown discrete-time nonlinear systems using globalized dual heuristic programming, IEEE Transactions on Automation Science and Engineering 9 (3) (2012) 628â€“634.","59":"D. L. Vrabie, K. G. Vamvoudakis, F. L. Lewis, Optimal adaptive control and differential games by reinforcement learning principles, Institution of Electrical Engineers, 0 (2013) 0.","60":"T. Bian, Z.-P. Jiang, Value iteration and adaptive dynamic programming for data-driven adaptive optimal control design, Automatica 71 (2016) 348â€“360.","61":"Y. Yuan, L. Hua, Y. Cheng, J. Li, X. Sang, L. Zhang, W. Wei, A novel model-based reinforcement learning algorithm for solving the problem of unbalanced reward, Journal of Intelligent & Fuzzy Systems 44 (2) (2023) 3233â€“3243.","62":"D. Kleinman, On an iterative technique for riccati equation computations, IEEE Transactions on Automatic Control 13 (1) (1968) 114â€“115.","63":"F. L. Lewis, D. Vrabie, V. L. Syrmos, Optimal control, John Wiley & Sons, 0 (2012) 0.","64":"R. S. Sutton, A. G. Barto, others, Introduction to reinforcement learning, MIT press Cambridge, 135 (1998) 0.","65":"H. Zhang, Q. Wei, Y. Luo, A novel infinite-time optimal tracking control scheme for a class of discrete-time nonlinear systems via the greedy hdp iteration algorithm, IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 38 (4) (2008) 937â€“942.","66":"F. L. Lewis, D. Vrabie, Reinforcement learning and adaptive dynamic programming for feedback control, IEEE circuits and systems magazine 9 (3) (2009) 32â€“50.","67":"K. G. Vamvoudakis, F. L. Lewis, G. R. Hudas, Multi-agent differential graphical games: Online adaptive learning solution for synchronization with optimality, Automatica 48 (8) (2012) 1598â€“1611.","68":"H. Zhang, J. Zhang, G.-H. Yang, Y. Luo, Leader-based optimal coordination control for the consensus problem of multiagent differential games via fuzzy adaptive dynamic programming, IEEE Transactions on Fuzzy Systems 23 (1) (2014) 152â€“163.","69":"J. Cui, Y. Pan, H. Xue, L. Tan, Simplified optimized finite-time containment control for a class of multi-agent systems with actuator faults, Nonlinear Dynamics 109 (4) (2022) 2799â€“2816.","70":"X. Chen, B. Li, R. Proietti, Z. Zhu, S. B. Yoo, Multi-agent deep reinforcement learning in cognitive inter-domain networking with multi-broker orchestration, in: 2019 Optical Fiber Communications Conference and Exhibition (OFC), IEEE, 0 (2019) 1â€“3.","71":"Z. Li, C. Guo, Multi-agent deep reinforcement learning based spectrum allocation for d2d underlay communications, IEEE Transactions on Vehicular Technology 69 (2) (2019) 1828â€“1840.","72":"X. You, X. Li, Y. Xu, H. Feng, J. Zhao, H. Yan, Toward packet routing with fully distributed multiagent deep reinforcement learning, IEEE Transactions on Systems, Man, and Cybernetics: Systems 52 (2) (2020) 855â€“868.","73":"R. Shen, S. Zhong, X. Wen, Q. An, R. Zheng, Y. Li, J. Zhao, Multi-agent deep reinforcement learning optimization framework for building energy system with renewable energy, Applied Energy 312 (2022) 118724.","74":"Y. Wang, D. Qiu, G. Strbac, Multi-agent deep reinforcement learning for resilience-driven routing and scheduling of mobile energy storage systems, Applied Energy 310 (2022) 118575.","75":"H. Allioui, M. A. Mohammed, N. Benameur, B. Al-Khateeb, K. H. Abdulkareem, B. Garcia-Zapirain, R. DamaÅ¡eviÄius, R. MaskeliÅ«nas, A multi-agent deep reinforcement learning approach for enhancement of covid-19 ct image segmentation, Journal of personalized medicine 12 (2) (2022) 309.","76":"N. Xie, Y. Hu, L. Chen, A distributed multi-agent formation control method based on deep q learning, Frontiers in Neurorobotics 16 (2022) 817168.","77":"X. Zhu, L. Wang, Y. Li, S. Song, S. Ma, F. Yang, L. Zhai, Path planning of multi-uavs based on deep q-network for energy-efficient data collection in uavs-assisted iot, Vehicular Communications 36 (2022) 100491.","78":"K. Kim, Multi-agent deep q network to enhance the reinforcement learning for delayed reward system, Applied Sciences 12 (7) (2022) 3520.","79":"J. Huang, Z. Mo, Z. Zhang, Y. Chen, Behavioral control task supervisor with memory based on reinforcement learning for humanâ€”multi-robot coordination systems, Frontiers of Information Technology & Electronic Engineering 23 (8) (2022) 1174â€“1188.","80":"J. Li, S. Li, C. Xue, Resource optimization for multi-unmanned aerial vehicle formation communication based on an improved deep q-network, Sensors 23 (5) (2023) 2667.","81":"Y. Bai, Y. Lv, J. Zhang, Smart mobile robot fleet management based on hierarchical multi-agent deep q network towards intelligent manufacturing, Engineering Applications of Artificial Intelligence 124 (2023) 106534.","82":"M. Yuan, H. Huang, Z. Li, C. Zhang, F. Pei, W. Gu, A multi-agent double deep-q-network based on state machine and event stream for flexible job shop scheduling problem, Advanced Engineering Informatics 58 (2023) 102230.","83":"X. Xu, Y. Jia, Y. Xu, Z. Xu, S. Chai, C. S. Lai, A multi-agent reinforcement learning-based data-driven method for home energy management, IEEE Transactions on Smart Grid 11 (4) (2020) 3201â€“3211.","84":"A. H. Tan, F. P. Bejarano, Y. Zhu, R. Ren, G. Nejat, Deep reinforcement learning for decentralized multi-robot exploration with macro actions, IEEE Robotics and Automation Letters 8 (1) (2022) 272â€“279.","85":"C. Jiang, Z. Wang, S. Chen, J. Li, H. Wang, J. Xiang, W. Xiao, Attention-shared multi-agent actorâ€“critic-based deep reinforcement learning approach for mobile charging dynamic scheduling in wireless rechargeable sensor networks, Entropy 24 (7) (2022) 965.","86":"X. Tan, L. Zhou, H. Wang, Y. Sun, H. Zhao, B.-C. Seet, J. Wei, V. C. Leung, Cooperative multi-agent reinforcement-learning-based distributed dynamic spectrum access in cognitive radio networks, IEEE Internet of Things Journal 9 (19) (2022) 19477â€“19488.","87":"Y. Yuan, P. Zhao, T. Guo, H. Jiang, Counterfactual-based action evaluation algorithm in multi-agent reinforcement learning, Applied Sciences 12 (7) (2022) 3439.","88":"O. Naparstek, K. Cohen, Deep multi-user reinforcement learning for distributed dynamic spectrum access, IEEE transactions on wireless communications 18 (1) (2019) 310â€“323.","89":"N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, Y. Jiang, Deep reinforcement learning for user association and resource allocation in heterogeneous cellular networks, IEEE Transactions on Wireless Communications 18 (11) (2019) 5141â€“5152.","90":"Z. Ding, Q. Fu, J. Chen, Y. Lu, H. Wu, N. Fang, B. Xing, Maqmc: Multi-agent deep q-network for multi-zone residential hvac control., CMES-Computer Modeling in Engineering & Sciences 136 (3) (2023) 0.","91":"W. Jiang, W. Yu, W. Wang, T. Huang, Multi-agent reinforcement learning for joint cooperative spectrum sensing and channel access in cognitive uav networks, Sensors 22 (4) (2022) 1651.","92":"Y. S. Nasir, D. Guo, Multi-agent deep reinforcement learning for dynamic power allocation in wireless networks, IEEE Journal on Selected Areas in Communications 37 (10) (2019) 2239â€“2250.","93":"K. Hu, J. Jin, F. Zheng, L. Weng, Y. Ding, Overview of behavior recognition based on deep learning, Artificial Intelligence Review 56 (3) (2023) 1833â€“1865.","94":"H. Zhang, H. Jiang, Y. Luo, G. Xiao, Data-driven optimal consensus control for discrete-time multi-agent systems with unknown dynamics using reinforcement learning method, IEEE Transactions on Industrial Electronics 64 (5) (2016) 4091â€“4100.","95":"L. Yu, Y. Sun, Z. Xu, C. Shen, D. Yue, T. Jiang, X. Guan, Multi-agent deep reinforcement learning for hvac control in commercial buildings, IEEE Transactions on Smart Grid 12 (1) (2020) 407â€“419.","96":"Y. Yan, B. Zhang, C. Li, A networked multi-agent reinforcement learning approach for cooperative femtocaching assisted wireless heterogeneous networks, Computer Networks 220 (2023) 109513.","97":"Z. Wang, Y. Sui, H. Qin, H. Lu, State super sampling soft actor-critic algorithm for multi-auv hunting in 3d underwater environment, Journal of Marine Science and Engineering 11 (7) (2023) 1257.","98":"S. Li, Y. Jia, F. Yang, Q. Qin, H. Gao, Y. Zhou, Collaborative decision-making method for multi-uav based on multiagent reinforcement learning, IEEE Access 10 (2022) 91385â€“91396.","99":"R. Zhang, X. Xu, X. Zhang, Q. Xiong, Q. Ma, Y. Peng, Kernel-based multiagent reinforcement learning for near-optimal formation control of mobile robots, Applied Intelligence 53 (10) (2023) 12736â€“12748.","100":"C.-s. Ying, A. H. Chow, H. T. Nguyen, K.-S. Chin, Multi-agent deep reinforcement learning for adaptive coordinated metro service operations with flexible train composition, Transportation Research Part B: Methodological 161 (2022) 36â€“59.","101":"K. G. Vamvoudakis, F. L. Lewis, G. R. Hudas, Multi-agent differential graphical games: Online adaptive learning solution for synchronization with optimality, Automatica 48 (8) (2012) 1598â€“1611.","102":"X. You, X. Li, Y. Xu, H. Feng, J. Zhao, H. Yan, Toward packet routing with fully distributed multiagent deep reinforcement learning, IEEE Transactions on Systems, Man, and Cybernetics: Systems 52 (2) (2015) 855â€“868.","103":"T. Guo, Y. Yuan, P. Zhao, Admission-based reinforcement-learning algorithm in sequential social dilemmas, Applied Sciences 13 (3) (2023) 1807.","104":"Z. Liang, Y. Dai, L. Lyu, B. Lin, Adaptive data collection and offloading in multi-uav-assisted maritime iot systems: A deep reinforcement learning approach, Remote Sensing 15 (2) (2023) 292.","105":"D. Li, F. Zhu, T. Chen, Y. D. Wong, C. Zhu, J. Wu, Coor-plt: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning, Transportation Research Part C: Emerging Technologies 146 (2023) 103933.","106":"L. She, J. Wang, Y. Bo, Y. Zeng, Maca: Multi-agent with credit assignment for computation offloading in smart parks monitoring, Mathematics 10 (23) (2022) 4616.","107":"G. Zhan, X. Zhang, Z. Li, L. Xu, D. Zhou, Z. Yang, Multiple-uav reinforcement learning algorithm based on improved ppo in ray framework, Drones 6 (7) (2022) 166.","108":"N. Aslam, K. Xia, M. U. Hadi, Optimal wireless charging inclusive of intellectual routing based on sarsa learning in renewable wireless sensor networks, IEEE Sensors Journal 19 (18) (2019) 8340â€“8351.","109":"T. Alfakih, M. M. Hassan, A. Gumaei, C. Savaglio, G. Fortino, Task offloading and resource allocation for mobile edge computing by deep reinforcement learning based on sarsa, IEEE Access 8 (2020) 54074â€“54084.","111":"G. Wang, F. Wei, Y. Jiang, M. Zhao, K. Wang, H. Qi, A multi-auv maritime target search method for moving and invisible objects based on multi-agent deep reinforcement learning, Sensors 22 (21) (2022) 8562.","112":"J. Yao, L. Yan, Z. Xu, P. Wang, X. Zhao, Collaborative decision-making method of emergency response for highway incidents, Sustainability 15 (3) (2023) 2099.","113":"Y. Zhou, Z. Liu, H. Shi, S. Li, N. Ning, F. Liu, X. Gao, Cooperative multi-agent target searching: a deep reinforcement learning approach based on parallel hindsight experience replay, Complex & Intelligent Systems 0 (2023) 1â€“12.","114":"Z. Lu, C. Zhong, M. C. Gursoy, Dynamic channel access and power control in wireless interference networks via multi-agent deep reinforcement learning, IEEE Transactions on Vehicular Technology 71 (2) (2021) 1588â€“1601.","115":"P. Zhao, Y. Yuan, T. Guo, Extensible hierarchical multi-agent reinforcement-learning algorithm in traffic signal control, Applied Sciences 12 (24) (2022) 12783.","116":"H. Xu, Q. Fang, C. Hu, Y. Hu, Q. Yin, Mira: Model-based imagined rollouts augmentation for non-stationarity in multi-agent systems, Mathematics 10 (17) (2022) 3059.","117":"Y. Tian, K.-R. Kladny, Q. Wang, Z. Huang, O. Fink, Multi-agent actor-critic with time dynamical opponent model, Neurocomputing 517 (2023) 165â€“172.","118":"L. Wang, K. Wang, C. Pan, W. Xu, N. Aslam, L. Hanzo, Multi-agent deep reinforcement learning-based trajectory planning for multi-uav assisted mobile edge computing, IEEE Transactions on Cognitive Communications and Networking 7 (1) (2020) 73â€“84.","119":"A. Oroojlooy, D. Hajinezhad, A review of cooperative multi-agent deep reinforcement learning, Applied Intelligence 53 (11) (2023) 13677â€“13722.","120":"F.-M. Luo, T. Xu, H. Lai, X.-H. Chen, W. Zhang, Y. Yu, A survey on model-based reinforcement learning, Science China Information Sciences 67 (2) (2024) 121101."}},"referenceLinks":{"external":[{"refId":3,"scopusHubEid":"2-s2.0-85148877076","crossRefDoi":"10.3390/rs15041055"},{"refId":4,"scopusHubEid":"2-s2.0-85160855470","crossRefDoi":"10.3390/jmse11050949"},{"refId":7,"scopusHubEid":"2-s2.0-0034205975"},{"refId":9,"crossRefDoi":"10.1007/11691839_1"},{"refId":10,"scopusHubEid":"2-s2.0-40949147745"},{"refId":11,"scopusHubEid":"2-s2.0-84857861863"},{"refId":12,"scopusHubEid":"2-s2.0-84861480021","crossRefDoi":"10.1609/aimag.v33i3.2426"},{"refId":15,"scopusHubEid":"2-s2.0-85066950970","crossRefDoi":"10.1126/science.aau6249"},{"refId":16,"scopusHubEid":"2-s2.0-85074707314","crossRefDoi":"10.1038/s41586-019-1724-z"},{"refId":17,"scopusHubEid":"2-s2.0-85088318959","crossRefDoi":"10.1109/tcyb.2020.2977374"},{"refId":18,"scopusHubEid":"2-s2.0-85073618168","crossRefDoi":"10.1007/s10458-019-09421-1"},{"refId":19,"scopusHubEid":"2-s2.0-85139824666","crossRefDoi":"10.1007/s10489-022-04105-y"},{"refId":20,"scopusHubEid":"2-s2.0-85065251299"},{"refId":23,"scopusHubEid":"2-s2.0-85111796839","crossRefDoi":"10.1007/978-3-030-60990-0_12"},{"refId":24,"scopusHubEid":"2-s2.0-85108415536"},{"refId":25,"scopusHubEid":"2-s2.0-0011669655"},{"refId":26,"scopusHubEid":"2-s2.0-4844223639"},{"refId":27,"scopusHubEid":"2-s2.0-0038683261"},{"refId":28,"scopusHubEid":"2-s2.0-40549092708"},{"refId":29,"scopusHubEid":"2-s2.0-76149130076","crossRefDoi":"10.3233/ifs-2010-0440"},{"refId":30,"scopusHubEid":"2-s2.0-77955828918"},{"refId":31,"scopusHubEid":"2-s2.0-4344624645"},{"refId":32,"scopusHubEid":"2-s2.0-85062766434"},{"refId":33,"scopusHubEid":"2-s2.0-84861443243"},{"refId":36,"scopusHubEid":"2-s2.0-85121548320","crossRefDoi":"10.3390/sym13122417"},{"refId":37,"scopusHubEid":"2-s2.0-85117852182","crossRefDoi":"10.1109/tvt.2021.3120292"},{"refId":38,"scopusHubEid":"2-s2.0-34548771972"},{"refId":39,"scopusHubEid":"2-s2.0-70149084778"},{"refId":40,"scopusHubEid":"2-s2.0-72149133902"},{"refId":41,"scopusHubEid":"2-s2.0-0035567541"},{"refId":42,"scopusHubEid":"2-s2.0-85054307344","crossRefDoi":"10.1007/978-981-10-8672-4_20"},{"refId":43,"scopusHubEid":"2-s2.0-85049076275","crossRefDoi":"10.1007/978-3-319-93818-9_10"},{"refId":44,"scopusHubEid":"2-s2.0-85053833094","crossRefDoi":"10.1587/transinf.2017edp7278"},{"refId":45,"scopusHubEid":"2-s2.0-37049252093","crossRefDoi":"10.1126/science.153.3731.34"},{"refId":46,"scopusHubEid":"2-s2.0-0025483204"},{"refId":47,"scopusHubEid":"2-s2.0-0030125669"},{"refId":48,"scopusHubEid":"2-s2.0-0031177640"},{"refId":53,"scopusHubEid":"2-s2.0-79958227077","crossRefDoi":"10.1002/apj.568"},{"refId":55,"scopusHubEid":"2-s2.0-85019446583"},{"refId":56,"scopusHubEid":"2-s2.0-85071560764"},{"refId":58,"scopusHubEid":"2-s2.0-84863467146","crossRefDoi":"10.1109/TASE.2012.2198057"},{"refId":60,"scopusHubEid":"2-s2.0-84975755115"},{"refId":61,"scopusHubEid":"2-s2.0-85148085684","crossRefDoi":"10.3233/jifs-210956"},{"refId":62,"scopusHubEid":"2-s2.0-84914965022"},{"refId":65,"crossRefDoi":"10.1109/ICCS.2008.4737322"},{"refId":66,"scopusHubEid":"2-s2.0-70349116541"},{"refId":67,"scopusHubEid":"2-s2.0-84864491417"},{"refId":69,"scopusHubEid":"2-s2.0-85132120928","crossRefDoi":"10.1007/s11071-022-07586-1"},{"refId":73,"scopusHubEid":"2-s2.0-85125181130"},{"refId":74,"scopusHubEid":"2-s2.0-85123950931"},{"refId":75,"scopusHubEid":"2-s2.0-85144782892"},{"refId":76,"scopusHubEid":"2-s2.0-85128467658"},{"refId":77,"scopusHubEid":"2-s2.0-85132508413"},{"refId":78,"scopusHubEid":"2-s2.0-85128219671","crossRefDoi":"10.3390/app12073520"},{"refId":79,"scopusHubEid":"2-s2.0-85137033414","crossRefDoi":"10.1631/fitee.2100280"},{"refId":80,"scopusHubEid":"2-s2.0-85149724796","crossRefDoi":"10.3390/s23052667"},{"refId":81,"scopusHubEid":"2-s2.0-85161283821"},{"refId":82,"scopusHubEid":"2-s2.0-85175630980"},{"refId":83,"scopusHubEid":"2-s2.0-85087407637","crossRefDoi":"10.1109/tsg.2020.2971427"},{"refId":85,"crossRefDoi":"10.3390/d12100407"},{"refId":87,"scopusHubEid":"2-s2.0-85148016231","crossRefDoi":"10.3390/app13031807"},{"refId":88,"scopusHubEid":"2-s2.0-85146586085","crossRefDoi":"10.3390/rs15020292"},{"refId":89,"scopusHubEid":"2-s2.0-85145553651"},{"refId":90,"scopusHubEid":"2-s2.0-85143642919","crossRefDoi":"10.3390/math10234616"},{"refId":91,"scopusHubEid":"2-s2.0-85133692904","crossRefDoi":"10.3390/drones6070166"},{"refId":92,"scopusHubEid":"2-s2.0-85071108175","crossRefDoi":"10.1109/jsen.2019.2918865"},{"refId":93,"scopusHubEid":"2-s2.0-85082617639","crossRefDoi":"10.1109/access.2020.2981434"},{"refId":94,"scopusHubEid":"2-s2.0-85147886938","crossRefDoi":"10.3390/math11030760"},{"refId":95,"scopusHubEid":"2-s2.0-85141566091","crossRefDoi":"10.3390/s22218562"},{"refId":96,"scopusHubEid":"2-s2.0-85143870270"},{"refId":97,"scopusHubEid":"2-s2.0-85166273178","crossRefDoi":"10.3390/jmse11071257"},{"refId":98,"scopusHubEid":"2-s2.0-85136632374","crossRefDoi":"10.1109/access.2022.3199070"},{"refId":99,"scopusHubEid":"2-s2.0-85139243916","crossRefDoi":"10.1007/s10489-022-04086-y"},{"refId":100,"scopusHubEid":"2-s2.0-85133925443"}],"internal":[{"refId":25,"pii":"092188909500026C","filesize":"260KB","pdf":{"urlType":"download","url":"/science/article/pii/092188909500026C/pdfft?md5=4eb85c4757558c23bdf78369ff8620d7&pid=1-s2.0-092188909500026C-main.pdf"}},{"refId":27,"pii":"S0020025503000069","filesize":"223KB","pdf":{"urlType":"download","url":"/science/article/pii/S0020025503000069/pdfft?md5=252bce9545244ec034c56cc8a81e45a9&pid=1-s2.0-S0020025503000069-main.pdf"}},{"refId":39,"pii":"S0952197609000384","filesize":"943KB","pdf":{"urlType":"download","url":"/science/article/pii/S0952197609000384/pdfft?md5=386d0bcaff2c04881f06222421bbd8fc&pid=1-s2.0-S0952197609000384-main.pdf"}},{"refId":40,"pii":"S0165011409002450","filesize":"1016KB","pdf":{"urlType":"download","url":"/science/article/pii/S0165011409002450/pdf?md5=9574f401790d7a064e4145b668094c1f&pid=1-s2.0-S0165011409002450-main.pdf"}},{"refId":48,"pii":"S0167865597000767","filesize":"949KB","pdf":{"urlType":"download","url":"/science/article/pii/S0167865597000767/pdf?md5=67f38778ab2be5b7d47b348deb746a8d&pid=1-s2.0-S0167865597000767-main.pdf"}},{"refId":55,"pii":"S0020025516312853","filesize":"2MB","pdf":{"urlType":"download","url":"/science/article/pii/S0020025516312853/pdfft?md5=77a283536d4afe323b121f05b539b2dd&pid=1-s2.0-S0020025516312853-main.pdf"}},{"refId":56,"pii":"S0377221719307052","filesize":"762KB","pdf":{"urlType":"download","url":"/science/article/pii/S0377221719307052/pdfft?md5=aab5929f1ff9699f51bac5528a5a7410&pid=1-s2.0-S0377221719307052-main.pdf"}},{"refId":60,"pii":"S000510981630187X","filesize":"836KB","pdf":{"urlType":"download","url":"/science/article/pii/S000510981630187X/pdfft?md5=6c365dfe1883af188b70e59df96366e5&pid=1-s2.0-S000510981630187X-main.pdf"}},{"refId":67,"pii":"S0005109812002476","filesize":"716KB","pdf":{"urlType":"download","url":"/science/article/pii/S0005109812002476/pdfft?md5=1a160077fb49dc687669538d7b25e17d&pid=1-s2.0-S0005109812002476-main.pdf"}},{"refId":73,"pii":"S0306261922001829","filesize":"4MB","pdf":{"urlType":"download","url":"/science/article/pii/S0306261922001829/pdfft?md5=e07a76bd4b213c7b5c9f7dc144e69d85&pid=1-s2.0-S0306261922001829-main.pdf"}},{"refId":74,"pii":"S0306261922000563","filesize":"3MB","pdf":{"urlType":"download","url":"/science/article/pii/S0306261922000563/pdfft?md5=48c306ee91fdbbd1fdca51d7bc909c3f&pid=1-s2.0-S0306261922000563-main.pdf"}},{"refId":77,"pii":"S2214209622000389","filesize":"944KB","pdf":{"urlType":"download","url":"/science/article/pii/S2214209622000389/pdfft?md5=0440509cac1ed27fabfceefb102bab39&pid=1-s2.0-S2214209622000389-main.pdf"}},{"refId":81,"pii":"S0952197623007182","filesize":"3MB","pdf":{"urlType":"download","url":"/science/article/pii/S0952197623007182/pdfft?md5=e07d0a72659104b5a957a06dd1deb9b0&pid=1-s2.0-S0952197623007182-main.pdf"}},{"refId":82,"pii":"S1474034623003580","filesize":"2MB","pdf":{"urlType":"download","url":"/science/article/pii/S1474034623003580/pdfft?md5=9041cfcddc3992bfe7a08f3571744197&pid=1-s2.0-S1474034623003580-main.pdf"}},{"refId":89,"pii":"S0968090X22003461","filesize":"7MB","pdf":{"urlType":"download","url":"/science/article/pii/S0968090X22003461/pdfft?md5=e210ab7e5fe7ff64a589f20c6243c2d2&pid=1-s2.0-S0968090X22003461-main.pdf"}},{"refId":96,"pii":"S1389128622005473","filesize":"3MB","pdf":{"urlType":"download","url":"/science/article/pii/S1389128622005473/pdfft?md5=5e0daa4a315c4af26faab929d72136e0&pid=1-s2.0-S1389128622005473-main.pdf"}},{"refId":100,"pii":"S0191261522000728","filesize":"2MB","pdf":{"urlType":"download","url":"/science/article/pii/S0191261522000728/pdfft?md5=3fb2ed51ac65c6fcd393e90641e73092&pid=1-s2.0-S0191261522000728-main.pdf"}}]},"refersTo":{},"referredToBy":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenVirtualSpecialIssueLink":false,"isOpenRecommendations":true,"isOpenSubstances":true,"citingArticles":[false,false,false,false,false,false],"recommendations":[false,false,false,false,false,false]},"seamlessAccess":{},"specialIssueArticles":{},"substances":{},"supplementaryFilesData":[],"tableOfContents":{"outlineTitle":"Outline","outline":[{"#name":"entry","$":{"class":"author","depth":"1","id":"d1e4530","type":"abstract"},"$$":[{"#name":"title","$":{"id":"d1e4531"},"_":"Abstract"}]},{"#name":"entry","$":{"class":"keyword","depth":"1","id":"d1e4536","type":"keywords"},"$$":[{"#name":"title","$":{"id":"d1e4537"},"_":"Keywords"}]},{"#name":"entry","$":{"depth":"1","id":"sec1","type":"sections"},"$$":[{"#name":"label","_":"1"},{"#name":"title","$":{"id":"d1e4562"},"_":"Introduction"},{"#name":"entry","$":{"depth":"2","id":"sec1.1"},"$$":[{"#name":"label","_":"1.1"},{"#name":"title","$":{"id":"d1e4567"},"_":"Background"}]},{"#name":"entry","$":{"depth":"2","id":"sec1.2"},"$$":[{"#name":"label","_":"1.2"},{"#name":"title","$":{"id":"d1e4610"},"_":"Development history"}]},{"#name":"entry","$":{"depth":"2","id":"sec1.3"},"$$":[{"#name":"label","_":"1.3"},{"#name":"title","$":{"id":"d1e4655"},"_":"Survey and contributions"}]},{"#name":"entry","$":{"depth":"2","id":"sec1.4"},"$$":[{"#name":"label","_":"1.4"},{"#name":"title","$":{"id":"d1e4780"},"_":"Broad structure of the thesis"}]}]},{"#name":"entry","$":{"depth":"1","id":"sec2","type":"sections"},"$$":[{"#name":"label","_":"2"},{"#name":"title","$":{"id":"d1e4813"},"_":"Citespace statistic"}]},{"#name":"entry","$":{"depth":"1","id":"sec3","type":"sections"},"$$":[{"#name":"label","_":"3"},{"#name":"title","$":{"id":"d1e4856"},"_":"RL conventional algorithm"},{"#name":"entry","$":{"depth":"2","id":"sec3.1"},"$$":[{"#name":"label","_":"3.1"},{"#name":"title","$":{"id":"d1e4863"},"_":"Model-free reinforcement learning"},{"#name":"entry","$":{"depth":"3","id":"sec3.1.1"},"$$":[{"#name":"label","_":"3.1.1"},{"#name":"title","$":{"id":"d1e4870"},"_":"Q-learning algorithm"}]},{"#name":"entry","$":{"depth":"3","id":"sec3.1.2"},"$$":[{"#name":"label","_":"3.1.2"},{"#name":"title","$":{"id":"d1e5861"},"_":"SARSA classical algorithm"}]}]},{"#name":"entry","$":{"depth":"2","id":"sec3.2"},"$$":[{"#name":"label","_":"3.2"},{"#name":"title","$":{"id":"d1e6232"},"_":"Model-based reinforcement learning"},{"#name":"entry","$":{"depth":"3","id":"sec3.2.1"},"$$":[{"#name":"label","_":"3.2.1"},{"#name":"title","$":{"id":"d1e6250"},"_":"Dynamic planning"}]},{"#name":"entry","$":{"depth":"3","id":"sec3.2.2"},"$$":[{"#name":"label","_":"3.2.2"},{"#name":"title","$":{"id":"d1e6459"},"_":"Model-based value iteration"}]},{"#name":"entry","$":{"depth":"3","id":"sec3.2.3"},"$$":[{"#name":"label","_":"3.2.3"},{"#name":"title","$":{"id":"d1e6682"},"_":"Model-based policy iteration"}]}]}]},{"#name":"entry","$":{"depth":"1","id":"sec4","type":"sections"},"$$":[{"#name":"label","_":"4"},{"#name":"title","$":{"id":"d1e6789"},"_":"MARL algorithm"},{"#name":"entry","$":{"depth":"2","id":"sec4.1"},"$$":[{"#name":"label","_":"4.1"},{"#name":"title","$":{"id":"d1e6796"},"_":"Algorithms performance measures"}]},{"#name":"entry","$":{"depth":"2","id":"sec4.2"},"$$":[{"#name":"label","_":"4.2"},{"#name":"title","$":{"id":"d1e6857"},"_":"Classification of algorithms"},{"#name":"entry","$":{"depth":"3","id":"sec4.2.1"},"$$":[{"#name":"label","_":"4.2.1"},{"#name":"title","$":{"id":"d1e6862"},"_":"Introduction to value-based approaches"}]},{"#name":"entry","$":{"depth":"3","id":"sec4.2.2"},"$$":[{"#name":"label","_":"4.2.2"},{"#name":"title","$":{"id":"d1e6883"},"_":"Analysis and application of value algorithms"}]},{"#name":"entry","$":{"depth":"3","id":"sec4.2.3"},"$$":[{"#name":"label","_":"4.2.3"},{"#name":"title","$":{"id":"d1e6999"},"_":"Introduction to policy-based methods"}]},{"#name":"entry","$":{"depth":"3","id":"sec4.2.4"},"$$":[{"#name":"label","_":"4.2.4"},{"#name":"title","$":{"id":"d1e7022"},"_":"Analysis and application of policy algorithms"}]},{"#name":"entry","$":{"depth":"3","id":"sec4.2.5"},"$$":[{"#name":"label","_":"4.2.5"},{"#name":"title","$":{"id":"d1e8117"},"_":"Introduction to the actorâ€“critic based approach"}]},{"#name":"entry","$":{"depth":"3","id":"sec4.2.6"},"$$":[{"#name":"label","_":"4.2.6"},{"#name":"title","$":{"id":"d1e8184"},"_":"Actorâ€“critic algorithm analysis and application"}]}]}]},{"#name":"entry","$":{"depth":"1","id":"sec5","type":"sections"},"$$":[{"#name":"label","_":"5"},{"#name":"title","$":{"id":"d1e8316"},"_":"Discussion"},{"#name":"entry","$":{"depth":"2","id":"sec5.1"},"$$":[{"#name":"label","_":"5.1"},{"#name":"title","$":{"id":"d1e8333"},"_":"Methods based on evolutionary algorithms"}]},{"#name":"entry","$":{"depth":"2","id":"sec5.2"},"$$":[{"#name":"label","_":"5.2"},{"#name":"title","$":{"id":"d1e8373"},"_":"Methods based on imitation learning"}]}]},{"#name":"entry","$":{"depth":"1","id":"sec6","type":"sections"},"$$":[{"#name":"label","_":"6"},{"#name":"title","$":{"id":"d1e8410"},"_":"Summary and expectation"},{"#name":"entry","$":{"depth":"2","id":"sec6.1"},"$$":[{"#name":"label","_":"6.1"},{"#name":"title","$":{"id":"d1e8415"},"_":"Summary"}]},{"#name":"entry","$":{"depth":"2","id":"sec6.2"},"$$":[{"#name":"label","_":"6.2"},{"#name":"title","$":{"id":"d1e8442"},"_":"Future challenges and expectation"}]}]},{"#name":"entry","$":{"depth":"1","id":"d1e8509","type":"sections"},"$$":[{"#name":"title","$":{"id":"d1e8510"},"_":"CRediT authorship contribution statement"}]},{"#name":"entry","$":{"depth":"1","id":"d1e8538","type":"sections"},"$$":[{"#name":"title","$":{"id":"d1e8539"},"_":"Declaration of Generative AI and AI-assisted technologies in the writing process"}]},{"#name":"entry","$":{"depth":"1","id":"coi1","type":"conflict-of-interest"},"$$":[{"#name":"title","$":{"id":"d1e8544"},"_":"Declaration of competing interest"}]},{"#name":"entry","$":{"depth":"1","id":"d1e8548","type":"acknowledgment"},"$$":[{"#name":"title","$":{"id":"d1e8549"},"_":"Acknowledgments"},{"#name":"entry","$":{"depth":"2","id":"d1e8560"},"$$":[{"#name":"title","$":{"id":"d1e8561"},"_":"Funding"}]}]},{"#name":"entry","$":{"depth":"1","id":"appendix","type":"appendices"},"$$":[{"#name":"label","_":"Appendix"},{"#name":"title","$":{"id":"d1e8569"},"_":"List of abbreviations and parameters"}]},{"#name":"entry","$":{"depth":"1","id":"da1","type":"data-availability"},"$$":[{"#name":"title","$":{"id":"aep-das-001"},"_":"Data availability"}]},{"#name":"entry","$":{"depth":"1","id":"bib1","type":"bibliography"},"$$":[{"#name":"title","$":{"id":"d1e8583"},"_":"References"}]},{"#name":"entry","$":{"depth":"1","id":"bio1","type":"biography"},"$$":[{"#name":"title","_":"Vitae"}]}],"figures":[],"tables":[{"#name":"entry","$":{"id":"tbl1","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 1"},{"#name":"caption","$":{"truncated":"false"},"_":"A Q-Learning algorithm-based approach to solving the single-agent RL problem."}]},{"#name":"entry","$":{"id":"tbl2","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 2"},{"#name":"caption","$":{"truncated":"false"},"_":"Q-Learning algorithm based approach for solving the MAS collaboration problem."}]},{"#name":"entry","$":{"id":"tbl3","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 3"},{"#name":"caption","$":{"truncated":"false"},"_":"Q-Learning algorithm based approach to solving the efficiency problem of MAS."}]},{"#name":"entry","$":{"id":"tbl4","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 4"},{"#name":"caption","$":{"truncated":"false"},"_":"SARSA algorithm for solving single-agent problems."}]},{"#name":"entry","$":{"id":"tbl5","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 5"},{"#name":"caption","$":{"truncated":"false"},"_":"DP-based approach to the single-agent reinforcement learning problem."}]},{"#name":"entry","$":{"id":"tbl6","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 6"},{"#name":"caption","$":{"truncated":"false"},"_":"PD-based approaches to multi-agent collaboration and efficiency problems."}]},{"#name":"entry","$":{"id":"tbl7","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 7"},{"#name":"caption","$":{"truncated":"false"},"_":"MBVI algorithm-based approach for solving single-agent reinforcement learning problems."}]},{"#name":"entry","$":{"id":"tbl8","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 8"},{"#name":"caption","$":{"truncated":"false"},"_":"MBPI algorithm-based approach for solving single-agent reinforcement learning problems."}]},{"#name":"entry","$":{"id":"tbl9","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 9"},{"#name":"caption","$":{"truncated":"false"},"_":"Application of the value function algorithm."}]},{"#name":"entry","$":{"id":"tbl10","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 10"},{"#name":"caption","$":{"truncated":"false"},"_":"Application of the value function algorithm."}]},{"#name":"entry","$":{"id":"tbl11","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 11"},{"#name":"caption","$":{"truncated":"false"},"_":"SARSA late improvement algorithm applications."}]},{"#name":"entry","$":{"id":"tbl12","local-type":"table","type":"sections"},"$$":[{"#name":"label","_":"Table 12"},{"#name":"caption","$":{"truncated":"false"},"_":"Actorâ€“critic algorithm applications."}]},{"#name":"entry","$":{"id":"tbl13","local-type":"table","type":"appendices"},"$$":[{"#name":"label","_":"Table 13"},{"#name":"caption","$":{"truncated":"false"},"_":"List of abbreviations and parameters."}]},{"#name":"entry","$":{"id":"tbl14","local-type":"table","type":"appendices"},"$$":[{"#name":"label","_":"Table 14"},{"#name":"caption","$":{"truncated":"false"},"_":"List of parameters."}]}],"extras":[],"attachments":[],"showEntitledTocLinks":false},"tail":{},"transientError":{"isOpen":false},"sidePanel":{"openState":1},"viewConfig":{"articleFeature":{"rightsAndContentLink":true,"sdAnswersButton":false},"pathPrefix":""},"virtualSpecialIssue":{"showVirtualSpecialIssueLink":false},"usageProps":{"itemStage":"S300","isAip":false,"tombAip":"0","sample":false},"userCookiePreferences":{"STRICTLY_NECESSARY":true,"PERFORMANCE":true,"FUNCTIONAL":true,"TARGETING":true}};
      </script>
      <noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics.elsevier.com/b/ss/elsevier-sd-prod/1/G.4--NS/1736709675074?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AANON_GUEST&c1=ae%3A228598&c12=ae%3A12975512 />
    </noscript>
      <div id="elementForFocusReset" tabindex="-1"></div><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-content"><span class="anchor-text-container"><span class="anchor-text">Skip to main content</span></span></a><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-title"><span class="anchor-text-container"><span class="anchor-text">Skip to article</span></span></a>
      <div id="root"><div class="App" id="app" data-aa-name="root"><div class="page"><div class="sd-flex-container"><div class="sd-flex-content"><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-l-hor-from-xl"><a id="gh-branding" class="u-flex-center-ver" href="/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"/><svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="15" viewBox="0 0 190 23" role="img" class="gh-wordmark u-margin-s-left" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path fill="#EB6500" d="M3.81 6.9c0-1.48 0.86-3.04 3.7-3.04 1.42 0 3.1 0.43 4.65 1.32l0.13-2.64c-1.42-0.63-2.97-0.96-4.78-0.96 -4.62 0-6.6 2.44-6.6 5.45 0 5.61 8.78 6.14 8.78 9.93 0 1.48-1.15 3.04-3.86 3.04 -1.72 0-3.4-0.56-4.72-1.39l-0.36 2.64c1.55 0.76 3.57 1.06 5.15 1.06 4.26 0 6.7-2.48 6.7-5.51C12.59 11.49 3.81 10.76 3.81 6.9M20.27 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.14 0-4.55-1.71-4.55-5.91C17.93 10.2 20.01 9.18 20.27 9.01"></path><rect x="29.42" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M30.67 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.68-0.96 1.68-1.88C32.35 1.55 31.56 0.7 30.67 0.7M48.06 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H48.06M39.91 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C38.56 10.27 39.71 9.37 39.91 9.18zM58.82 6.57c-2.24 0-3.63 1.12-4.85 2.61l-0.4-2.21h-2.34l0.13 1.19c0.1 0.76 0.13 1.78 0.13 2.97v10.79h2.54V11.88c0.69-0.96 2.15-2.48 2.48-2.64 0.23-0.13 1.29-0.4 2.08-0.4 2.28 0 2.48 1.15 2.54 3.43 0.03 1.19 0.03 3.17 0.03 3.17 0.03 3-0.1 6.47-0.1 6.47h2.54c0 0 0.07-4.49 0.07-6.96 0-1.48 0.03-2.97-0.1-4.46C63.31 7.43 61.49 6.57 58.82 6.57M72.12 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C69.77 10.2 71.85 9.18 72.12 9.01M92.74 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H92.74M84.59 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C83.24 10.27 84.39 9.37 84.59 9.18zM103.9 1.98h-7.13v19.93h6.83c7.26 0 9.77-5.68 9.77-10.03C113.37 7.33 110.93 1.98 103.9 1.98M103.14 19.8h-3.76V4.1h4.09c5.38 0 6.96 4.39 6.96 7.79C110.43 16.87 108.19 19.8 103.14 19.8zM118.38 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.69-0.96 1.69-1.88C120.07 1.55 119.28 0.7 118.38 0.7"></path><rect x="117.13" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M130.2 6.6c-1.62 0-2.87 1.45-3.4 2.74l-0.43-2.37h-2.34l0.13 1.19c0.1 0.76 0.13 1.75 0.13 2.9v10.86h2.54v-9.51c0.53-1.29 1.72-3.7 3.17-3.7 0.96 0 1.06 0.99 1.06 1.22l2.08-0.6V9.18c0-0.03-0.03-0.17-0.06-0.4C132.8 7.36 131.91 6.6 130.2 6.6M145.87 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.89-1.95-4.89-5.51v-0.49H145.87M137.72 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C136.37 10.27 137.52 9.37 137.72 9.18zM153.23 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.14-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.69 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C150.89 10.2 152.97 9.18 153.23 9.01M170 19.44c-0.92 0.36-1.72 0.69-2.51 0.69 -1.16 0-1.58-0.66-1.58-2.34V8.95h3.93V6.97h-3.93V2.97h-2.48v3.99h-2.71v1.98h2.71v9.67c0 2.64 1.39 3.73 3.33 3.73 1.15 0 2.54-0.39 3.43-0.79L170 19.44M173.68 5.96c-1.09 0-2-0.87-2-1.97 0-1.1 0.91-1.97 2-1.97s1.98 0.88 1.98 1.98C175.66 5.09 174.77 5.96 173.68 5.96zM173.67 2.46c-0.85 0-1.54 0.67-1.54 1.52 0 0.85 0.69 1.54 1.54 1.54 0.85 0 1.54-0.69 1.54-1.54C175.21 3.13 174.52 2.46 173.67 2.46zM174.17 5.05c-0.09-0.09-0.17-0.19-0.25-0.3l-0.41-0.56h-0.16v0.87h-0.39V2.92c0.22-0.01 0.47-0.03 0.66-0.03 0.41 0 0.82 0.16 0.82 0.64 0 0.29-0.21 0.55-0.49 0.63 0.23 0.32 0.45 0.62 0.73 0.91H174.17zM173.56 3.22l-0.22 0.01v0.63h0.22c0.26 0 0.43-0.05 0.43-0.34C174 3.28 173.83 3.21 173.56 3.22z"></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action text-s anchor-secondary anchor-medium" href="/browse/journals-and-books" id="gh-journals-books-link" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text-container"><span class="anchor-text">Journals &amp; Books</span></span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-help text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-move-to-spine gh-help-button gh-help-icon gh-nav-item"><div class="popover" id="gh-help-icon-popover"><div id="popover-trigger-gh-help-icon-popover"><input type="hidden"/><button class="button-link button-link-secondary gh-icon-btn button-link-medium button-link-icon-left" title="Help" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" width="20" class="icon icon-help gh-icon"><path d="M57 8C35.69 7.69 15.11 21.17 6.68 40.71c-8.81 19.38-4.91 43.67 9.63 59.25 13.81 15.59 36.85 21.93 56.71 15.68 21.49-6.26 37.84-26.81 38.88-49.21 1.59-21.15-10.47-42.41-29.29-52.1C74.76 10.17 65.88 7.99 57 8zm0 10c20.38-.37 39.57 14.94 43.85 34.85 4.59 18.53-4.25 39.23-20.76 48.79-17.05 10.59-40.96 7.62-54.9-6.83-14.45-13.94-17.42-37.85-6.83-54.9C26.28 26.5 41.39 17.83 57 18zm-.14 14C45.31 32.26 40 40.43 40 50v2h10v-2c0-4.22 2.22-9.66 8-9.24 5.5.4 6.32 5.14 5.78 8.14C62.68 55.06 52 58.4 52 69.4V76h10v-5.56c0-8.16 11.22-11.52 12-21.7.74-9.86-5.56-16.52-16-16.74-.39-.01-.76-.01-1.14 0zM52 82v10h10V82H52z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Help</span></span></button></div></div></div></li><li class="gh-nav-search text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-secondary anchor-secondary u-margin-l-left gh-nav-action gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/search" id="gh-search-link" title="Search" data-aa-button="search-in-header-opened-from-article" role="button"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search gh-icon"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg><span class="anchor-text-container"><span class="anchor-text">Search</span></span></a></div></li></ul></nav></div></div><div class="gh-profile-container gh-move-to-spine u-hide-from-print"><a class="anchor text-s u-clr-grey8 u-margin-l-left gh-icon-btn anchor-primary anchor-medium anchor-icon-left anchor-with-icon" href="/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0925231224008397&amp;from=globalheader" id="gh-myaccount-btn" data-aa-region="header" data-aa-name="personalsignin"><svg focusable="false" viewBox="0 0 106 128" height="20" aria-hidden="true" class="icon icon-person gh-cta-btn-icon"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><span class="anchor-text-container"><span class="anchor-text">My account</span></span></a></div><a class="anchor text-s u-clr-grey8 gh-move-to-spine u-hide-from-print u-margin-l-left anchor-secondary gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0925231224008397" id="gh-institutionalsignin-btn" data-aa-region="header" data-aa-name="institutionalsignin"><svg focusable="false" viewBox="0 0 106 128" height="20" aria-hidden="true" class="icon icon-institution gh-cta-btn-icon"><path d="M84 98h10v10H12V98h10V52h14v46h10V52h14v46h10V52h14v46zM12 36.86l41-20.84 41 20.84V42H12v-5.14zM104 52V30.74L53 4.8 2 30.74V52h10v36H2v30h102V88H94V52h10z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Sign in</span></span></a><div id="gh-mobile-menu" class="mobile-menu u-hide-from-print"><div class="gh-hamburger u-fill-grey7"><button class="button-link u-flex-center-ver button-link-primary button-link-icon-left" aria-label="Toggle mobile menu" aria-expanded="false" type="button"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="20" width="20"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class="" role="navigation"></div></div></div></header><div class="Article Preview" id="mathjax-container" role="main"><div class="accessbar-sticky"><div id="screen-reader-main-content"></div><div role="region" aria-label="Download options and search"><div class="accessbar"><div class="accessbar-label"></div><ul aria-label="PDF Options"><li class="RemoteAccess"><a class="link-button RemoteAccessButton accessbar-utility-component accessbar-utility-link link-button-primary link-button-icon-left" href="/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0925231224008397" aria-label="Access through your organization"><svg focusable="false" viewBox="0 0 106 128" height="20" aria-label="Seamless access" role="img" class="icon icon-institution"><path d="M84 98h10v10H12V98h10V52h14v46h10V52h14v46h10V52h14v46zM12 36.86l41-20.84 41 20.84V42H12v-5.14zM104 52V30.74L53 4.8 2 30.74V52h10v36H2v30h102V88H94V52h10z"></path></svg><span class="link-button-text-container"><span class="link-button-text"><span>Access through&nbsp;<strong>your organization</strong></span></span></span></a></li><li class="PurchasePDF"><a class="anchor accessbar-utility-component accessbar-utility-link anchor-primary" href="/getaccess/pii/S0925231224008397/purchase" target="_blank" aria-label="Purchase PDF" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text"><span>Purchase PDF</span></span></span></a></li></ul><form class="QuickSearch" action="/search#submit" method="get" aria-label="form"><div class="search-input"><div class="search-input-container search-input-container-no-label"><label class="search-input-label u-hide-visually" for="article-quick-search">Search ScienceDirect</label><input type="text" id="article-quick-search" name="qs" class="search-input-field" aria-describedby="article-quick-search-description-message" aria-invalid="false" aria-label="Search ScienceDirect" placeholder="Search ScienceDirect" value=""/></div><div class="search-input-message-container"><div class="search-input-validation-error" aria-live="polite"></div><div id="article-quick-search-description-message"></div></div></div><button type="submit" class="button u-margin-xs-left button-primary small button-icon-only" aria-disabled="false" aria-label="Submit search"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></button><input type="hidden" name="origin" value="article"/><input type="hidden" name="zone" value="qSearch"/></form></div></div></div><div class="article-wrapper u-padding-s-top grid row"><div role="navigation" aria-label="Table of Contents" class="preview-sidebar u-display-block-from-lg col-lg-6"><div class="PreviewTableOfContents"><h2 class="u-h4 preview-table-of-contents-title">Article preview</h2><ul class="preview-table-of-contents-list"><li id="preview-section-abstract-item" class=""><a class="anchor anchor-primary" href="#preview-section-abstract"><span class="anchor-text-container"><span class="anchor-text">Abstract</span></span></a></li><li id="preview-section-introduction-item" class=""><a class="anchor anchor-primary" href="#preview-section-introduction"><span class="anchor-text-container"><span class="anchor-text">Introduction</span></span></a></li><li id="preview-section-snippets-item" class=""><a class="anchor anchor-primary" href="#preview-section-snippets"><span class="anchor-text-container"><span class="anchor-text">Section snippets</span></span></a></li><li id="preview-section-references-item" class=""><a class="anchor anchor-primary" href="#preview-section-references"><span class="anchor-text-container"><span class="anchor-text">References (120)</span></span></a></li><li id="preview-section-cited-by-item" class=""><a class="anchor anchor-primary" href="#preview-section-cited-by"><span class="anchor-text-container"><span class="anchor-text">Cited by (5)</span></span></a></li></ul></div></div><article class="col-lg-12 col-md-16 pad-left pad-right" lang="en"><div class="Publication" id="publication"><div class="publication-brand u-display-block-from-sm"><a class="anchor u-display-flex anchor-primary" href="/journal/neurocomputing" title="Go to Neurocomputing on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-brand-image" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/e444bc27e06cd87e0d0139060246bdab684588aa/image/elsevier-non-solus.png" alt="Elsevier"/></span></span></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="anchor anchor-secondary publication-title-link" href="/journal/neurocomputing" title="Go to Neurocomputing on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text">Neurocomputing</span></span></a></h2><div class="text-xs"><a class="anchor anchor-primary" href="/journal/neurocomputing/vol/599/suppl/C" title="Go to table of contents for this volume/issue"><span class="anchor-text-container"><span class="anchor-text">Volume 599</span></span></a>, <!-- -->28 September 2024<!-- -->, 128068</div></div><div class="publication-cover u-display-block-from-sm"><a class="anchor u-display-flex anchor-primary" href="/journal/neurocomputing/vol/599/suppl/C"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-cover-image" src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224X00296-cov150h.gif" alt="Neurocomputing"/></span></span></a></div></div><div class="PageDivider"></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><div class="article-dochead"><span>Survey Paper</span></div><span class="title-text">A review of research on reinforcement learning algorithms for multi-agents</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div aria-live="polite"></div><div class="AuthorGroups"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000001" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Kai</span> <span class="text surname">Hu</span> </span><span class="author-ref" id="baff1"><sup>a</sup></span> <span class="author-ref" id="baff2"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000002" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Mingyang</span> <span class="text surname">Li</span> </span><span class="author-ref" id="baff1"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000003" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Zhiqiang</span> <span class="text surname">Song</span> </span><span class="author-ref" id="baff3"><sup>c</sup></span><svg focusable="false" viewBox="0 0 106 128" height="20" title="Correspondence author icon" class="icon icon-person react-xocs-author-icon u-fill-grey8"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000004" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Keer</span> <span class="text surname">Xu</span> </span><span class="author-ref" id="baff1"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000005" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Qingfeng</span> <span class="text surname">Xia</span> </span><span class="author-ref" id="baff3"><sup>c</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000006" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Ning</span> <span class="text surname">Sun</span> </span><span class="author-ref" id="baff3"><sup>c</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000007" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Peng</span> <span class="text surname">Zhou</span> </span><span class="author-ref" id="baff1"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000008" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Min</span> <span class="text surname">Xia</span> </span><span class="author-ref" id="baff1"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button></div></div></div><button class="button-link u-margin-s-ver button-link-primary button-link-icon-right" id="show-more-btn" type="button" data-aa-button="icon-expand"><span class="button-link-text-container"><span class="button-link-text">Show more</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><button class="button-link AddToMendeley button-link-secondary u-margin-s-right u-display-inline-flex-from-md button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 86 128" height="20" class="icon icon-plus"><path d="M48 58V20H38v38H0v10h38v38h10V68h38V58z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Add to Mendeley</span></span></button><div class="Social u-display-inline-block" id="social"><div class="popover social-popover" id="social-popover"><div id="popover-trigger-social-popover"><button class="button-link button-link-secondary u-margin-s-right button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" class="icon icon-share"><path d="M90 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zM24 76c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-60c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48L45.1 70.2c.58-1.98.9-4.04.9-6.2s-.32-4.22-.9-6.2l28.42-15.28C77.56 47.1 83.44 50 90 50c12.14 0 22-9.86 22-22S102.14 6 90 6s-22 9.86-22 22c0 1.98.28 3.9.78 5.72L40.14 49.1C36.12 44.76 30.38 42 24 42 11.86 42 2 51.86 2 64s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-.5 1.84-.78 3.76-.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Share</span></span></button></div></div></div><div class="ExportCitation u-display-inline-block" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 104 128" height="20" class="icon icon-cited-by-66"><path d="M2 58.78V106h44V64H12v-5.22C12 40.28 29.08 32 46 32V22C20.1 22 2 37.12 2 58.78zM102 32V22c-25.9 0-44 15.12-44 36.78V106h44V64H68v-5.22C68 40.28 85.08 32 102 32z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Cite</span></span></button></div></div></div></div></div><div class="ArticleIdentifierLinks u-margin-xs-bottom text-xs" id="article-identifier-links"><a class="anchor doi anchor-primary" href="https://doi.org/10.1016/j.neucom.2024.128068" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span class="anchor-text-container"><span class="anchor-text">https://doi.org/10.1016/j.neucom.2024.128068</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor rights-and-content anchor-primary" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0925231224008397&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">Get rights and content</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div id="preview-section-abstract"><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author" id="d1e4530"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="d1e4533"><div class="u-margin-s-bottom" id="d1e4534"><span>In recent years, multi-agent <a href="/topics/engineering/reinforcement-learning-technique" title="Learn more about reinforcement learning techniques from ScienceDirect&#x27;s AI-generated Topic Pages" class="topic-link">reinforcement learning techniques</a> have been widely used and evolved in the field of </span><a href="/topics/computer-science/artificial-intelligence" title="Learn more about artificial intelligence from ScienceDirect&#x27;s AI-generated Topic Pages" class="topic-link">artificial intelligence</a><span>. However, traditional <a href="/topics/computer-science/reinforcement-learning" title="Learn more about reinforcement learning from ScienceDirect&#x27;s AI-generated Topic Pages" class="topic-link">reinforcement learning</a> methods have limitations such as long training time, large sample data requirements, and highly delayed rewards. Therefore, this paper systematically and specifically studies the MARL algorithm. Firstly, this paper uses Citespace software to visually analyze the existing literature on multi-agent reinforcement learning and briefly indicates the research hotspots and key research directions in this field. Secondly, the applications of traditional reinforcement learning algorithms under two task objects, namely single-agent and multi-agent systems, are described in detail. Then, the paper highlights the diverse applications, challenges, and corresponding solutions of MARL algorithmic techniques in the field of MAS. Finally, the paper points out future research directions based on the existing limitations of the algorithm. Through this paper, readers will gain a systematic and in-depth understanding of MARL algorithms and how they can be utilized to better address the various challenges posed by MAS.</span></div></div></div></div></div><div id="preview-section-introduction"><div class="PageDivider"></div><div class="Introduction u-font-serif u-margin-l-ver"><h2 class="u-h4 u-margin-s-bottom">Introduction</h2><section id="sec1"><section id="sec1.1"><div class="u-margin-s-bottom" id="d1e4569">With the continuous development of society and economy, many problems in modern society have become intricate and complex, and there is an urgent need for more efficient, flexible and intelligent solutions. Multi-agent Reinforcement Learning (MARL) helps to better simulate and understand complex social systems and interactions and provides smarter and more efficient solutions to these problems. Using MARL algorithms, multi-agent can work together to complete complex tasks, thus improving the efficiency and accuracy of the whole system. In addition, it can be used for AI decision-making, assisting it to make smarter decisions in diverse and complex environments.</div><div class="u-margin-s-bottom" id="d1e4571">Multi-agent systems (MAS) encompass multiple distributed entities, i.e., agents, that make decisions independently and interact with each other in a shared environment [1]. With the diversity of tasks, complex interactions between agents may occur to decide whether to collaborate or adopt competitive strategies to outperform competitors. Determining intelligent behavior in a pre-programmed manner in complex systems is a challenging task [2].</div><div class="u-margin-s-bottom" id="d1e4581">MARL is an application of Reinforcement Learning (RL) in MAS, which focuses on the problem of multi-agent completing a task with interaction and collaboration to promote interaction and collaboration among the agents. In MAS, each agent possesses its own state, action, and reward signals, and they reach a common goal by interacting with each other [3]. Unlike single-agent RL, it needs to consider the interactions and collaborations between agents, which increases the complexity and challenge of the problem. In MARL, the agents can choose different strategies, such as cooperation, competition, coordination, etc., which can be rule-based or learned. Usually, the goal of the above methods is to achieve some kind of globally optimal solution through collaboration between the agents.</div><div class="u-margin-s-bottom" id="d1e4587">MARL combines Collaborative Learning (CL) and RL to address the collaboration and competition problem in MAS, whereby each agent interacts with other agents by sensing the environment to learn the best behavioral strategies to optimize overall performance.CL focuses on cooperation and coordination between multi-agent, while RL explores how agents can derive rewards from the environment and optimize strategies.</div><div class="u-margin-s-bottom" id="d1e4589">CL is an approach in which multi-agent cooperate to achieve a common goal, which usually occurs in the field of artificial agents in scenarios such as MAS and robot collaboration. In this context, agents can collaborate by communicating, sharing information, and coordinating decisions, to improve the performance of the whole system through the interoperability of the agents. Fig. 1 shows a generic model diagram for RL.</div><div class="u-margin-s-bottom" id="d1e4595">RL is a branch of machine learning (ML) that focuses on how agents can achieve predefined goals through interaction with the environment in a trial-and-error learning process. In RL, an agent interacts with the environment, obtains rewards or penalties based on the state of the environment and the behavior of the agent, and adjusts its behavioral strategies intending to maximize long-term cumulative rewards [4]. It allows the agent to learn the optimal policy by interacting with the environment to maximize rewards in future environments.</div><div class="u-margin-s-bottom" id="d1e4601">In MARL, each agent is an independent RL agent, and the value of each state and action is assessed through a value function, which allows the agents to continuously adjust their behavior based on the feedback signals from the environment to maximize the expected long-term rewards. However, in a multi-agent environment, the value function needs to take into account the effects of the actions of other agents on the current one, considering that the actions of the agents interact with each other. To address this issue, CL is introduced into MARL.CL enables collaboration between agents by sharing information such as states, rewards, and actions.</div><div class="u-margin-s-bottom" id="d1e4605">MARL is a product of the organic combination of CL and RL, which makes full use of the decision-making ability of RL and the coordination ability of CL to achieve collaboration and optimal decision-making among multi-agent.</div></section><section id="sec1.2"><div class="u-margin-s-bottom" id="d1e4612">With the continuous development of AI technology, MARL has received more and more attention as an important part of it. It applies CL to solve the learning problem of MAS, aiming to achieve optimal decision-making of the whole system by exploiting the collaboration between agents. During the past decades, researchers have proposed diverse MARL algorithms. This paper aims to analyze the development of MARL in recent years and provide a systematic introduction.</div><div class="u-margin-s-bottom" id="d1e4614">Before 2000, the field of MARL was in its infancy, with research focusing primarily on theoretical exploration and the initial establishment of methodologies. Researchers began to realize the potential value of applying reinforcement learning in MAS, even though significant progress has not yet been made in terms of algorithms and frameworks, during this period some researchers have started to investigate MARL methods in both independent and collaborative environments, e.g., Tan [5], Caroline Claus and Craig Boutilier [6] addressed MARL related issues are discussed.</div><div class="u-margin-s-bottom" id="d1e4624">During the initial development phase from 2000 to 2010, MARL gradually became an independent research direction and made some progress. Researchers began to enrich the theoretical foundation of MARL and proposed a series of new algorithms and methods, including game theory-based methods, centralized agent RL methods, and distributed control algorithms. The focus of research in this stage is centered on issues such as how multi-agent collaborate to complete tasks and how to resolve conflicts between different agents. In this phase, MARL emerged with representative algorithms such as Joint action learning. These works laid the foundation for subsequent research, but challenges still exist in dealing with complex multi-agent interaction problems.</div><div class="u-margin-s-bottom" id="d1e4626">Between 2010 and 2017, MARL entered its mid-development phase, and the field of artificial agents witnessed it making more significant progress. Researchers continue to propose new algorithms and methods, including elements such as distributed agent reinforcement learning, deep reinforcement learning (DRL), and meta-learning. The problems of how to improve the efficiency, robustness, and scalability of MAS are the focus of research in this stage, and algorithms such as Deep Q-Network (DQN) and DDPG play a great role in solving these problems.</div><div class="u-margin-s-bottom" id="d1e4628">Since 2017, MARL has been further developed and researchers have proposed more advanced algorithms and methods, including multi-agent evolutionary algorithms as well as hierarchical RL, and some new application areas, such as humanâ€“computer collaboration, robot control, and the Internet of Things. New issues have also emerged from this phase of research, such as the robustness problem of MAS, how to improve the efficiency and scalability of MAS, and the realization of more complex multi-agent tasks. In this field, many algorithms such as MAAC, QMIX, PPO, etc. have achieved remarkable success.</div><div class="u-margin-s-bottom" id="d1e4630">Due to the rise of Deep Learning (DL) technology and its remarkable achievements in many fields, the fusion of Deep Neural Networks (DQNs) and RL into DRL has become the focus of extensive research. Significant breakthroughs in this approach have been achieved in areas such as computer vision, robot control, and large-scale real-time policy games. The great success of DRL has sparked more researchers to focus on the field of multi-agent. Researchers have boldly attempted to introduce DRL methods into MAS to solve complex tasks in multi-agent environments, and this approach has laid the foundation for the rise of multi-agent deep reinforcement learning (MADRL).</div><div class="u-margin-s-bottom" id="d1e4632">This paper provides an extensive review of recent advances in the field of MADRL.</div><div class="u-margin-s-bottom" id="d1e4634">Firstly, MADRL involves agents (or decision makers) interacting with each other and with the environment to achieve a common goal cooperatively or competitively. In traditional RL, agents learn how to make optimal decisions by interacting with the environment to obtain reward signals. However, in multi-agent CL, as the behaviors of multi-agent interact with each other, a single dimension of reward signals often fails to adequately reflect this complex collaborative relationship. To consider the collaborative relationship between multi-agent more comprehensively, the MADRL method introduces multi-dimensional reward signals. These reward signals can cover multiple dimensions, including individual reward, team reward, and collaborative reward, to comprehensively consider the collaborative effects among multi-agent.</div><div class="u-margin-s-bottom" id="d1e4636">In addition, the MADRL method can also apply techniques such as Hierarchical Reinforcement Learning and Evolutionary Algorithms to optimize the decision-making of the agents, as a way to further improve the learning effect.MADRL not only extends the traditional RL but also integrates MARL with DL, which represents the latest advances in the field of artificial agents, of which the MADDPG algorithm is a typical example.</div><div class="u-margin-s-bottom" id="d1e4639">The MADRL method has a broad application prospect in the field of MARL. It can solve the problem of collaboration among multi-agent more effectively under the CL framework, improve learning efficiency, and provide useful references and lessons for a wider range of MAS applications in the future.</div><div class="u-margin-s-bottom" id="d1e4641">After years of innovation and development, MADRL has spawned a variety of algorithms, rules, and frameworks that have been widely applied in various real-world domains. The evolutionary trajectory from single to multi-agent, from simple to complex tasks, and from low-dimensional to high-dimensional environments suggests that MADRL technology has emerged and gradually become a high-profile research and application direction in the field of ML and even artificial agent, which is of great research value and practical significance. Fig. 2 provides a summary of the stages of MARL development.</div><div class="u-margin-s-bottom" id="d1e4647">However, in MAS, the optimization of interactions and collaboration between agents, the non-stationarity of the system, uncertainty, and how to improve the learning efficiency are all issues that researchers need to address urgently. Therefore, it is of great significance to classify and review MARL algorithms in this paper, and, it helps researchers to better understand the current state of research and future research directions in this field.</div><div class="u-margin-s-bottom" id="d1e4649"></div></section><section id="sec1.3"><div class="u-margin-s-bottom" id="d1e4657">Nowadays, MAS has a wide range of applications in various fields, including disaster rescue, environmental monitoring, traffic management, and so on. However, the applicability of multi-agent systems is affected by realistic factors, such as communication delays, environmental changes, and even mission requirements and constraints. Due to the uncertainty between the collaboration of agents, how to flexibly build a MAS to effectively integrate and coordinate the heterogeneity and maximize the collaboration capability between agents becomes a huge challenge to be solved nowadays.</div><div class="u-margin-s-bottom" id="d1e4659">Therefore, researchers continue to improve and optimize MAS algorithms, aiming to promote the application of MAS in complex environments, improve the adaptability, synergy, and performance of the system, and promote the achievement of MAS technology in practical engineering applications. However, there is a wide variety of MAS algorithms, and the theory needs to be transformed into practical solutions in practical applications. By systematically analyzing the challenges involved in MAS, this paper can provide a reference framework for relevant researchers as well as engineers to help them better apply MAS algorithms to practical scenarios and solve practical problems.</div><div class="u-margin-s-bottom" id="d1e4661">In this paper, the Science Citation Index-Expanded (SCIE) database was searched by the keywords of â€œMASâ€, â€œMARLâ€, â€œMARL algorithmsâ€ and other keywords, a total of 120 related articles were obtained, including reviews as well as specific algorithms. In addition, this paper summarizes some evaluation metrics as performance measures of MARL algorithms.</div><div class="u-margin-s-bottom" id="d1e4663">In this paper, existing MARL algorithms are carefully categorized and summarized to facilitate further exploration by researchers. Before this, excellent review articles have been published on the field of MARL.</div><div class="u-margin-s-bottom" id="d1e4665">Although the concept of MARL was first proposed in the 1980s, unfortunately, MARL did not receive much attention until 2000, and therefore no systematic review articles appeared. However, some researchers at that time had already started to study MARL-related issues in the areas of MAS, RL, and game theory. For example, the first formalization of IQL and its application to the field of MARL were presented in Tan [5]; Caroline Claus and Craig Boutilier [6] investigated Q-Learning in collaborative MAS from different perspectives. The early research results laid a theoretical foundation for the field of MARL and provided important references and inspirations for future research.</div><div class="u-margin-s-bottom" id="d1e4675">Over time, the combination of MAS and RL has been in a permanent research boom. As one of the most representative first surveys in the field, Stone and Veloso [7] analyzed MAS from an ML perspective, categorizing all the literature reviewed by them according to the structure of the agent as well as the perspective of learning algorithms. The authors address research issues and challenges in MAS related to system dynamics, cooperative and competitive games, social norms, etc. in some detail. Shoham et al. [8], Pieter Jan â€™t Hoen et al. [9], Busoniu et al. [10], in emphasizing the complexity as well as the importance of MAS premise, not only explored different forms and applications of MARL but also evaluated multiple approaches. Although these review articles have made a great contribution in outlining and summarizing the current state of research, issues and methods in MARL, there are still some inevitable limitations, such as the lack of a comprehensive focus and the fact that the research only covers some specific types of MAS and tasks.</div><div class="u-margin-s-bottom" id="d1e4694">The booming development of MARL has attracted more and more researchers to explore it in depth, and this phase can be regarded as the maturity phase of MARL, with numerous review articles exploring the various issues involved. Matignon et al. [11] identified several challenges in coordinating independent learners in a fully collaborative MG: Pareto selection, non-smoothness, stochasticity, altered exploration, and shaded equilibrium. Furthermore, the researchers analyzed the conditions under which algorithms can solve such coordination problems. Another work by Tuyls and Weiss [12] explains the historical development of MARL and raises non-technical challenges. The above studies provide a more systematic and in-depth look at the research directions and focus of MARL and suggest viable options for future research directions.</div><div class="u-margin-s-bottom" id="d1e4704">With the emergence of DL methods and breakthroughs, the field of MARL has attracted new attention from the community and a large body of related literature has emerged during the past few years. Researchers have challenged real-world complexity problems by overcoming historical limitation issues through DL approaches (Baker et al. [13]; Berner et al. [14]; Jaderberg et al. [15]; Vinyals et al. [16]). Nguyen et al. [17] presented five technical challenges including non-smoothness, partial observability, continuous space, training schemes, and transfer learning, while also discussing possible solutions and their practical applications. Hernandez-Leal et al. [18] focus on four main categories: emergency behavior, learning communication, learning cooperation, and analysis of agent modeling. In addition, they provide an in-depth survey of further research on MADRL in specific sub-domains. Oroojlooyjadid and Hajinezhad [19] review recent work in collaborative environments, whereas the studies of Da Silva and Costa [20] and Da Silva et al. [21] are more focussed on knowledge reuse. Lazaridou and Baroni [22], while reviewing the emergence of language, linked two perspectives, including the conditions under which language develops in a community and the ability to solve problems through dynamic communication. Based on theoretical analyses, Zhang et al. [23] focused on MARL algorithms and presented challenges from a mathematical perspective.</div><div class="u-margin-s-bottom" id="d1e4752">The above review articles provide a comprehensive and exhaustive summary and evaluation of the history, current status, and future of MARL while advancing important research results in the field of MARL to a deeper stage of development.</div><div class="u-margin-s-bottom" id="d1e4755">Over time, more innovative approaches to MARL have emerged, especially algorithms in the field of MADRL, and as a result, there is a need for the emergence of more systematic and comprehensive reviews to fill in the gaps in this area and to update the classification of past algorithms. In this paper, we classify the algorithms in relative detail according to their functional characteristics and show the performance of different methods</div><div class="u-margin-s-bottom" id="d1e4757">The contribution of this paper can be summarized as follows.</div><div class="u-margin-s-bottom" id="d1e4759"><ul class="list"><li class="react-xocs-list-item"><span class="list-label">(1)</span><span><div class="u-margin-s-bottom" id="d1e4765">Agent collaboration algorithms are categorized from the perspective of task objects. This paper details the application of RL algorithms to both single agents and MAS with two task objects, highlights specific existing algorithms, and compares their strengths and weaknesses to help readers better understand the characteristics and development of certain types of algorithms.</div></span></li><li class="react-xocs-list-item"><span class="list-label">(2)</span><span><div class="u-margin-s-bottom" id="d1e4770">The diverse applications of MARL algorithms in subtasks are explored. Focusing on an overview of the current phase of MARL algorithms and describing the challenges and their solutions in the field of MAS collaboration, this paper aims to help readers gain insights into the characteristics and future trends of various MARL algorithms.</div></span></li><li class="react-xocs-list-item"><span class="list-label">(3)</span><span><div class="u-margin-s-bottom" id="d1e4775">The focus is on the study and outlook of the latest MADRL algorithms in subtasks. The paper summarizes the algorithms applicable to MARL, reveals the difficulties and some solutions in the field of MAS, and provides some innovative ideas for the future development of MARL.</div></span></li></ul></div></section><section id="sec1.4"><div class="u-margin-s-bottom" id="d1e4782">The rest of the paper is organized as follows, Section 2 gives a careful statistical overview of the literature on currently available algorithms and presents them visually employing CiteSpace graphing. Section 3 describes the traditional algorithms of MARL. In Section 4, some commonly used quality evaluation metrics in MARL algorithms are listed, existing MARL algorithms are categorized, and some existing and imperfect MARL algorithms are introduced. In Section 5, algorithms that have emerged and are not widely used are briefly discussed and presented. Section 6 summarizes the problems in the existing algorithms and suggests some future research directions.</div></section></section></div></div><div id="preview-section-snippets"><div class="PageDivider"></div><div class="Snippets u-font-serif"><h2 class="u-h4 u-margin-l-ver">Section snippets</h2><section><section id="sec2"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Citespace statistic</h2><div class="u-margin-s-bottom" id="d1e4815">Since the introduction of MARL, it has been widely used in the field of artificial agents. As shown in the figure below, this paper searches the Web of Science Core Collection with the keywords of â€œmulti-agent systemâ€, â€œmulti-agent reinforcement learningâ€, â€œmulti-agent reinforcement learning algorithmâ€, etc., and obtains a total of 732 papers published in the past 20 years. Science Core Collection and obtained 732 MARL papers published in the past 20 years. On this basis, this paper produced a</div></section></section><section><section id="sec3"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">RL conventional algorithm</h2><div class="u-margin-s-bottom" id="d1e4858">Traditional RL algorithms describe the interaction between agents and the decision-making process by defining the state space, action space, reward function, and policy function. Different algorithms may use multiple learning methods and models to process this information, such as the two major research directions of Model-Free Reinforcement Learning(MFRL), and Model-Based Reinforcement Learning(MBRL). The common goal of these approaches is to bring the MAS to an overall optimal state, rather</div></section></section><section><section id="sec4"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">MARL algorithm</h2><div class="u-margin-s-bottom" id="d1e6791">In recent years, DL has been widely used in multi-agent collaboration techniques due to its excellent performance in big data processing, adaptive learning, and handling of diverse data types, as well as improving accuracy and scalability. Meanwhile, MARL algorithms have been rapidly developed.</div></section></section><section><section id="sec5"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Discussion</h2><div class="u-margin-s-bottom" id="d1e8318">One of the key goals of artificial agents is to develop agents with excellent decision-making capabilities in complex and uncertain environments. In recent years, the rapid development of DQNs has enabled RL-based agents to show good performance in complex tasks, such as aircraft control, plant scheduling tasks [102], [103], and industrial process control in the field of autonomy control, etc. The advantages of RL over traditional control policies lie in its applicability to situations where</div></section></section><section><section id="sec6"><section id="sec6.1"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Summary</h2><div class="u-margin-s-bottom" id="d1e8417">After years of development and innovation, RL has received widespread attention as an important AI technology. In the field of single-agent reinforcement learning, many excellent algorithms have emerged. However, with the increase and complexity of practical application scenarios, the limitations of RL methods have gradually emerged, mainly in the inability to effectively deal with the problems of synergy or competition. The field of artificial intelligence needs to introduce new methods to</div></section></section></section><section><section id="d1e8509"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">CRediT authorship contribution statement</h2><div class="u-margin-s-bottom" id="d1e8512"><strong>Kai Hu:</strong> Supervision, Conceptualization, Methodology, Writing â€“ review &amp; editing. <strong>Mingyang Li:</strong> Data curation, Investigation, Validation, Writing â€“ original draft. <strong>Zhiqiang Song:</strong> Methodology, Supervision, Validation, Writing â€“ review &amp; editing. <strong>Keer Xu:</strong> Data curation, Software, Visualization, Writing â€“ original draft. <strong>Qingfeng Xia:</strong> Project administration, Validation, Writing â€“ original draft, Writing â€“ review &amp; editing. <strong>Ning Sun:</strong> Formal analysis, Investigation, Writing â€“ original draft. <strong>Peng Zhou:</strong></div></section></section><section><section id="d1e8538"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Declaration of Generative AI and AI-assisted technologies in the writing process</h2><div class="u-margin-s-bottom" id="d1e8541">During the preparation of this work the author (s) used [ChatGPT] in order to [improve language and readability]. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.</div></section></section><section><section id="coi1"><h2 id="d1e8544" class="u-h4 u-margin-l-top u-margin-xs-bottom">Declaration of competing interest</h2><div class="u-margin-s-bottom" id="d1e8546">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</div></section></section><section><section id="d1e8548"><h2 id="d1e8549" class="u-h4 u-margin-l-top u-margin-xs-bottom">Acknowledgments</h2><div class="u-margin-s-bottom" id="d1e8551">The research in this article is supported by the financial support of <span id="GS1">Qing Lan Project of Jiangsu Province, China</span> is deeply appreciated. The authors would like to express heartfelt thanks to the reviewers and editors who submitted valuable revisions to this article.</div><section id="d1e8560"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Funding</h2><div class="u-margin-s-bottom" id="d1e8563">The Research in this article is supported by the National Natural Science Foundation of China (42075130).</div></section></section></section><section><div class="article-biography article-biography-has-image" id="bio1"><div class="article-biography-image"></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14256"><strong>Kai Hu</strong>, male, Ph.D, 1981.4. Associate professor and Doctoral Supervisor of School of Automation, Nanjing University of Information Science and Technology, China. His research interest areas include remote sensing, disturbed learning, action recognition, image restoration, and published more than 50 SCI(E) indexed papers.</div><div class="u-margin-s-bottom" id="d1e14260">ORCID: 0000-0001-7181-9935.</div><div class="u-margin-s-bottom" id="d1e14262">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#0c3c3c3d3a3c3c4c6279657f7822696879226f62" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="586868696e686818362d312b2c763d3c2d763b36">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>, <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#620c170b111612030c0603225354514c010d0f" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="335d465a404743525d5752730205001d505c5e">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div></section></div></div><div class="related-content-links u-display-none-from-md"><button class="button-link button-link-primary button-link-small" type="button"><span class="button-link-text-container"><span class="button-link-text">Recommended articles</span></span></button></div><div class="Tail"></div><div id="preview-section-references"><div class="paginatedReferences u-font-serif"><div class="PageDivider"></div><header><h2 class="u-h4 u-margin-l-ver"><span>References</span><span> (120)</span></h2></header><ul><li class="bib-reference u-margin-s-bottom"><span class="author u-font-sans">KrÃ¶se<span>B.J.A. </span></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/092188909500026C"><span class="anchor-text-container"><span class="anchor-text">Learning from delayed rewards</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Robot. Auton. Syst.</h3></div><div class="series">(1995)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Lin<span>Y.-P. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0020025503000069"><span class="anchor-text-container"><span class="anchor-text">Reinforcement learning based on local state feature learning and policy adjustment</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Inform. Sci.</h3></div><div class="series">(2003)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Aissani<span>N. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0952197609000384"><span class="anchor-text-container"><span class="anchor-text">Dynamic scheduling of maintenance tasks in the petroleum industry: A reinforcement approach</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Eng. Appl. Artif. Intell.</h3></div><div class="series">(2009)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Derhami<span>V. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0165011409002450"><span class="anchor-text-container"><span class="anchor-text">Exploration and exploitation balance management in fuzzy reinforcement learning</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Fuzzy Sets and Systems</h3></div><div class="series">(2010)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Buckley<span>M. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0167865597000767"><span class="anchor-text-container"><span class="anchor-text">Regularised shortest-path extraction</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Pattern Recognit. Lett.</h3></div><div class="series">(1997)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Luo<span>B. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0020025516312853"><span class="anchor-text-container"><span class="anchor-text">Multi-step heuristic dynamic programming for optimal control of nonlinear discrete-time systems</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Inform. Sci.</h3></div><div class="series">(2017)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Voelkel<span>M.A. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0377221719307052"><span class="anchor-text-container"><span class="anchor-text">An aggregation-based approximate dynamic programming approach for the periodic review model with random yield</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">European J. Oper. Res.</h3></div><div class="series">(2020)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Bian<span>T. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S000510981630187X"><span class="anchor-text-container"><span class="anchor-text">Value iteration and adaptive dynamic programming for data-driven adaptive optimal control design</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Automatica</h3></div><div class="series">(2016)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Vamvoudakis<span>K.G. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0005109812002476"><span class="anchor-text-container"><span class="anchor-text">Multi-agent differential graphical games: Online adaptive learning solution for synchronization with optimality</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Automatica</h3></div><div class="series">(2012)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Shen<span>R. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0306261922001829"><span class="anchor-text-container"><span class="anchor-text">Multi-agent deep reinforcement learning optimization framework for building energy system with renewable energy</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Appl. Energy</h3></div><div class="series">(2022)</div></span></li></ul><div class="u-display-none"><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Wang<span>Y. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0306261922000563"><span class="anchor-text-container"><span class="anchor-text">Multi-agent deep reinforcement learning for resilience-driven routing and scheduling of mobile energy storage systems</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Appl. Energy</h3></div><div class="series">(2022)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Zhu<span>X. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S2214209622000389"><span class="anchor-text-container"><span class="anchor-text">Path planning of multi-uavs based on deep q-network for energy-efficient data collection in uavs-assisted iot</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Veh. Commun.</h3></div><div class="series">(2022)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Bai<span>Y. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0952197623007182"><span class="anchor-text-container"><span class="anchor-text">Smart mobile robot fleet management based on hierarchical multi-agent deep q network towards intelligent manufacturing</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Eng. Appl. Artif. Intell.</h3></div><div class="series">(2023)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Yuan<span>M. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S1474034623003580"><span class="anchor-text-container"><span class="anchor-text">A multi-agent double deep-q-network based on state machine and event stream for flexible job shop scheduling problem</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Adv. Eng. Inform.</h3></div><div class="series">(2023)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Li<span>D. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0968090X22003461"><span class="anchor-text-container"><span class="anchor-text">Coor-plt: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Transp. Res. C</h3></div><div class="series">(2023)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Yan<span>Y. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S1389128622005473"><span class="anchor-text-container"><span class="anchor-text">A networked multi-agent reinforcement learning approach for cooperative femtocaching assisted wireless heterogeneous networks</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Comput. Netw.</h3></div><div class="series">(2023)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Ying<span>C.-s. </span></span><em> et al.</em></span><h3><a class="anchor title anchor-primary" href="/science/article/pii/S0191261522000728"><span class="anchor-text-container"><span class="anchor-text">Multi-agent deep reinforcement learning for adaptive coordinated metro service operations with flexible train composition</span></span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Transp. Res. B</h3></div><div class="series">(2022)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="author u-font-sans">Weiss<span>G. </span></span><h3 class="title">Multiagent Systems: A Modern Approach To Distributed Artificial Intelligence</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(1999)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Gronauer<span>S. </span></span><em> et al.</em></span><h3 class="title">Multi-agent deep reinforcement learning: A survey</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Artif. Intell. Rev.</h3></div><div class="series">(2022)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Hu<span>K. </span></span><em> et al.</em></span><h3 class="title">Mcanet: A multi-branch network for cloud/snow segmentation in high-resolution remote sensing images</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Remote Sens.</h3></div><div class="series">(2023)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Hu<span>K. </span></span><em> et al.</em></span><h3 class="title">Overview of underwater 3d reconstruction technology based on optical images</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">J. Mar. Sci. Eng.</h3></div><div class="series">(2023)</div></span></li><li class="bib-reference u-margin-s-bottom"><span>M. Tan, Multi-agent reinforcement learning: Independent vs. cooperative agents, in: Proceedings of the Tenth...</span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Claus<span>C. </span></span><em> et al.</em></span><h3 class="title">The dynamics of reinforcement learning in cooperative multiagent systems</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">AAAI/IAAI</h3></div><div class="series">(1998)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Stone<span>P. </span></span><em> et al.</em></span><h3 class="title">Multiagent systems: A survey from a machine learning perspective</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Auton. Robots</h3></div><div class="series">(2000)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Shoham<span>Y. </span></span><em> et al.</em></span><h3 class="title">Multi-Agent Reinforcement Learning: A Critical Survey<!-- -->Tech. rep.</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2003)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Hoen<span>P.J. </span></span><em> et al.</em></span><h3 class="title">An overview of cooperative and competitive multiagent learning</h3><span class="host u-clr-grey6 u-font-sans"></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Busoniu<span>L. </span></span><em> et al.</em></span><h3 class="title">A comprehensive survey of multiagent reinforcement learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Syst. Man Cybern. C (Appl. Rev.)</h3></div><div class="series">(2008)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Matignon<span>L. </span></span><em> et al.</em></span><h3 class="title">Independent reinforcement learners in cooperative markov games: A survey regarding coordination problems</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Knowl. Eng. Rev.</h3></div><div class="series">(2012)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Tuyls<span>K. </span></span><em> et al.</em></span><h3 class="title">Multiagent learning: Basics, challenges, and prospects</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">AI Mag.</h3></div><div class="series">(2012)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Baker<span>B. </span></span><em> et al.</em></span><h3 class="title">Emergent tool use from multi-agent autocurricula</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Berner<span>C. </span></span><em> et al.</em></span><h3 class="title">Dota 2 with large scale deep reinforcement learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Jaderberg<span>M. </span></span><em> et al.</em></span><h3 class="title">Human-level performance in 3d multiplayer games with population-based reinforcement learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Science</h3></div><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Vinyals<span>O. </span></span><em> et al.</em></span><h3 class="title">Grandmaster level in starcraft ii using multi-agent reinforcement learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Nature</h3></div><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Nguyen<span>T.T. </span></span><em> et al.</em></span><h3 class="title">Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Cybern.</h3></div><div class="series">(2020)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Hernandez-Leal<span>P. </span></span><em> et al.</em></span><h3 class="title">A survey and critique of multiagent deep reinforcement learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Auton. Agents Multi-Agent Syst.</h3></div><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Oroojlooy<span>A. </span></span><em> et al.</em></span><h3 class="title">A review of cooperative multi-agent deep reinforcement learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Appl. Intell.</h3></div><div class="series">(2023)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Da Silva<span>F.L. </span></span><em> et al.</em></span><h3 class="title">A survey on transfer learning for multiagent reinforcement learning systems</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">J. Artificial Intelligence Res.</h3></div><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Da Silva<span>F.L. </span></span><em> et al.</em></span><h3 class="title">Agents teaching agents: A survey on inter-agent transfer learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Auton. Agents Multi-Agent Syst.</h3></div><div class="series">(2020)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Lazaridou<span>A. </span></span><em> et al.</em></span><h3 class="title">Emergent multi-agent communication in the deep learning era</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2020)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Zhang<span>K. </span></span><em> et al.</em></span><h3 class="title">Multi-agent reinforcement learning: A selective overview of theories and algorithms</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Handb. Reinf. Learn. Control</h3></div><div class="series">(2021)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Guojun<span>M. </span></span><em> et al.</em></span><h3 class="title">Improved q-learning algorithm and its application in path planning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">J. Taiyuan Univ. Technol.</h3></div><div class="series">(2021)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Guo<span>M. </span></span><em> et al.</em></span><h3 class="title">A new q-learning algorithm based on the metropolis criterion</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Syst. Man Cybern. B</h3></div><div class="series">(2004)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Sharma<span>R. </span></span><em> et al.</em></span><h3 class="title">A markov game-adaptive fuzzy controller for robot manipulators</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Fuzzy Syst.</h3></div><div class="series">(2008)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Boubertakh<span>H. </span></span><em> et al.</em></span><h3 class="title">A new mobile robot navigation method using fuzzy logic and a modified q-learning algorithm</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">J. Intell. Fuzzy Systems</h3></div><div class="series">(2010)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Rahimiyan<span>M. </span></span><em> et al.</em></span><h3 class="title">An adaptive <span class="math"><math><mi is="true">q</mi></math></span>-learning algorithm developed for agent-based computational modeling of electricity market</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Syst. Man Cybern. C (Appl. Rev.)</h3></div><div class="series">(2010)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Hwang<span>K.-S. </span></span><em> et al.</em></span><h3 class="title">Cooperative strategy based on adaptive q-learning for robot soccer systems</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Fuzzy Syst.</h3></div><div class="series">(2004)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Zhou<span>Y. </span></span><em> et al.</em></span><h3 class="title">Subcarrier assignment schemes based on q-learning in wideband cognitive radio networks</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Veh. Technol.</h3></div><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Chung<span>W.-C. </span></span><em> et al.</em></span><h3 class="title">An mimo configuration mode and mcs level selection scheme by fuzzy q-learning for hspa<span class="math"><math><msup is="true"><mrow is="true"></mrow><mrow is="true"><mo is="true">+</mo></mrow></msup></math></span> systems</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Mob. Comput.</h3></div><div class="series">(2012)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Shams<span>F. </span></span><em> et al.</em></span><h3 class="title">Energy-efficient power control for multiple-relay cooperative networks using <span class="math"><math><mi is="true">q</mi></math></span>-learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Trans. Wireless Commun.</h3></div><div class="series">(2014)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Zhang<span>X. </span></span><em> et al.</em></span><h3 class="title">A cooperative-learning path planning algorithm for originâ€“destination pairs in urban road networks</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Math. Probl. Eng.</h3></div><div class="series">(2015)</div></span></li></div><button class="button-alternative button-alternative-secondary u-font-sans u-margin-l-bottom large-alternative button-alternative-icon-left" type="button" id="show-more-refs-btn"><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">View more references</span></span></button></div></div><div id="preview-section-cited-by"><section aria-label="Cited by" class="ListArticles preview"><div class="PageDivider"></div><header id="citing-articles-header"><h2 class="u-h4 u-margin-l-ver u-font-serif">Cited by (5)</h2></header><div aria-describedby="citing-articles-header"><div class="citing-articles u-margin-l-bottom"><ul><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-0-title"><a class="anchor anchor-primary" href="/science/article/pii/S0959652624031251"><span class="anchor-text-container"><span class="anchor-text">Intelligent machines as information and communication technology and their influence on sustainable marketing practices for beneficial impact on business performance: A conceptual framework</span></span></a></h3><div>2024, Journal of Cleaner Production</div><div class="CitedSection u-margin-s-top"><div class="u-margin-s-left"><div class="cite-header u-text-italic u-font-sans">Citation Excerpt :</div><p class="u-font-serif text-xs">To increase the accuracy and focus of the final solution, data must be pre-processed and structured in a way to solve the problem by applying the most suitable algorithm and establishing a model (i.e., emulating decision-making based on sourced data), which represents the blueprint of what needs to be done. In continuation, to enable AI learning in machines, several machine intelligence techniques have been proposed, such as reinforcement learning (Hu et al., 2024) and imitation learning (Huang et al., 2024). In reinforcement learning, a machine learns to do a task through repeated trial-and-error interactions in a dynamic operating environment.</p></div></div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-0-title" aria-controls="citing-articles-article-0" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="abspara0010"><u>Intelligent machines</u> are the machines or devices that make use of artificial intelligence and robotics technologies. It has the ability to accomplish a specific task in the presence of uncertainty and variability in its operating environment. Certainly, it can be <u>used to</u> support <u>information and communication technology</u> to streamline the creation, collection, processing, transmission, and storage of information for <u>sustainable marketing practices</u>. The flawless application of sustainable marketing practices results in <u>beneficial impacts</u> on <u>business performance</u>. In fact, the issue of unsustainable marketing practices can be effectively managed by intelligent machines. Therefore, this study is undertaken to uncover how intelligent machines can influence sustainable marketing practices for beneficial impacts on retailersâ€™ business performance by proposing a unique conceptual framework. The theoretical contributions discuss two <u>techno-sustainable marketing applications</u>. First, intelligent machines improve incremental innovation. This allows retailers to balance technology risk with sustainable marketing and lower the cost of innovations. Second, intelligent machines increase business efficiency by automating sustainable marketing practices. This allows retailers to efficiently manage the inventory, improve fulfilment efficiency, and optimise stock levels. The managerial implications discuss two goals of sustainable marketing practices. First, it can attract sustainability-minded customers who support the retail business for their own well-being. Second, it builds a strong sustainable brand reputation that can lower the <u>price sensitivity</u>.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-1-title"><a class="anchor anchor-primary" href="https://doi.org/10.3390/rs16183394" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Review of Satellite Remote Sensing of Carbon Dioxide Inversion and Assimilation</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2024, Remote Sensing</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-2-title"><a class="anchor anchor-primary" href="https://doi.org/10.1080/01431161.2024.2411069" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Classification and extraction method of hidden dangers along railway lines based on semantic segmentation network</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2024, International Journal of Remote Sensing</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-3-title"><a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2024.3471798" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Robust Optimal Control for Cable-Driven Parallel Robots via Event-Triggered ADP</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2024, IEEE Access</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-4-title"><a class="anchor anchor-primary" href="https://doi.org/10.14569/IJACSA.2024.0150839" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Optimization of Knitting Path of Flat Knitting Machine Based on Reinforcement Learning</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2024, International Journal of Advanced Computer Science and Applications</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li></ul></div></div></section></div><div class="PageDivider"></div><div class="article-biography article-biography-has-image" id="bio1"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224008397-fx1.jpg" height="132" alt=""/></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14256"><strong>Kai Hu</strong>, male, Ph.D, 1981.4. Associate professor and Doctoral Supervisor of School of Automation, Nanjing University of Information Science and Technology, China. His research interest areas include remote sensing, disturbed learning, action recognition, image restoration, and published more than 50 SCI(E) indexed papers.</div><div class="u-margin-s-bottom" id="d1e14260">ORCID: 0000-0001-7181-9935.</div><div class="u-margin-s-bottom" id="d1e14262">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#74444445424444341a011d07005a1110015a171a" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="2f1f1f1e191f1f6f415a465c5b014a4b5a014c41">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>, <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#b5dbc0dcc6c1c5d4dbd1d4f58483869bd6dad8" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="157b607c666165747b7174552423263b767a78">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div><div class="article-biography article-biography-has-image" id="bio2"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224008397-fx2.jpg" height="132" alt=""/></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14272"><strong>Mingyang Li</strong>, male, Candidate for Masterâ€™s degree, 1999. Student of School of Automation, Nanjing University of Information Science and Technology, China. His research interest area is Multi-agent system collaboration and control.</div><div class="u-margin-s-bottom" id="d1e14276">ORCID: 0000-0002-4879-6437.</div><div class="u-margin-s-bottom" id="d1e14278">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#73414341414241474a43464344331d061a00075d1617065d101d" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="0e3c3e3c3c3f3c3a373e3b3e394e607b677d7a206b6a7b206d60">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div><div class="article-biography article-biography-has-image" id="bio3"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224008397-fx3.jpg" height="132" alt=""/></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14285"><strong>Zhiqiang SONG</strong>, born in 1977, Ph.D, associate professor. His research interests include intelligent control, swarm intelligence system task planning and optimal scheduling, etc.</div><div class="u-margin-s-bottom" id="d1e14289">ORCID: 0009-0000-5120-3447</div><div class="u-margin-s-bottom" id="d1e14291">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#2d575c5e42434a6d4e5a555803484958034e43" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="91ebe0e2fefff6d1f2e6e9e4bff4f5e4bff2ff">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div><div class="article-biography article-biography-has-image" id="bio4"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224008397-fx4.jpg" height="132" alt=""/></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14298"><strong>Keer Xu</strong>, famale, Candidate for Masterâ€™s degree, 1999. Student of School of Automation, Nanjing University of Information Science and Technology, China. Her research interest area is Multi-agent system collaboration and control.</div><div class="u-margin-s-bottom" id="d1e14302">ORCID: 0000-0003-0799-483X.</div><div class="u-margin-s-bottom" id="d1e14304">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#ccfefcfefefdfef8f5fcf9fcfe8ca2b9a5bfb8e2a9a8b9e2afa2" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="02303230303330363b32373230426c776b71762c6766772c616c">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div><div class="article-biography article-biography-has-image" id="bio5"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224008397-fx5.jpg" height="132" alt=""/></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14311"><strong>Qingfeng Xia</strong>, male, Ph.D, 1982, associate professor. Wuxi University, China. His research interest area is Multi-agent system collaboration and control.</div><div class="u-margin-s-bottom" id="d1e14315">ORCID: 0000-0003-0829-6750.</div><div class="u-margin-s-bottom" id="d1e14317">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#067e77604665717e7328636273286568" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="92eae3f4d2f1e5eae7bcf7f6e7bcf1fc">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div><div class="article-biography article-biography-has-image" id="bio6"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224008397-fx6.jpg" height="132" alt=""/></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14326"><strong>Ning Sun</strong>, born in 1981, professor. His research interests include Artificial Intelligence and System Integration, etc.</div><div class="u-margin-s-bottom" id="d1e14330">ORCID: 0000-0002-0280-5983</div><div class="u-margin-s-bottom" id="d1e14332">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#4f7f7f7e78797b0f2c38373a612a2b3a612c21" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="5f6f6f6e68696b1f3c28272a713a3b2a713c31">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div><div class="article-biography article-biography-has-image" id="bio7"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224008397-fx7.jpg" height="132" alt=""/></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14339"><strong>Peng Zhou</strong>, male, Candidate for Masterâ€™s degree, 2000. Student of School of Automation, Nanjing University of Information Science and Technology, China. His research interest areas include Multi-agent system collaboration and control.</div><div class="u-margin-s-bottom" id="d1e14343">ORCID: 0009-0002-1914-1666.</div><div class="u-margin-s-bottom" id="d1e14345">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#60525052535152545950535758200e150913144e0504154e030e" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="be8c8e8c8d8f8c8a878e8d8986fed0cbd7cdca90dbdacb90ddd0">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div><div class="article-biography article-biography-has-image" id="bio8"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231224008397-fx8.jpg" height="132" alt=""/></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="d1e14352"><strong>Min Xia</strong>, male, Ph.D., 1983.10. Professor and Ph.D. Supervisor of School of Automation, Nanjing University of Information Science and Technology. Now his main research interests are artificial intelligence and big data analysis theory, specifically: relevance knowledge mining of large-scale heterogeneous data, machine learning theory, multi-source image data analysis, satellite remote sensing image classification, etc. He has published more than 100 papers in domestic and international journals and conferences, with more than 2000 citations.</div><div class="u-margin-s-bottom" id="d1e14356">ORCID: 0000-0003-4681-9129.</div><div class="u-margin-s-bottom" id="d1e14358">Email: <a class="anchor anchor-primary" href="/cdn-cgi/l/email-protection#cfb7a6aea2a6a18fa1baa6bcbbe1aaabbae1aca1" target="_blank"><span class="anchor-text-container"><span class="anchor-text"><span class="__cf_email__" data-cfemail="add5c4ccc0c4c3edc3d8c4ded983c8c9d883cec3">[email&#160;protected]</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div><a class="anchor full-text-link anchor-primary" href="/science/article/pii/S0925231224008397" aria-disabled="true" tabindex="-1"><span class="anchor-text-container"><span class="anchor-text">View full text</span></span></a><div class="Copyright"><span class="copyright-line">Â© 2024 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.</span></div></article><div class="u-display-block-from-md col-lg-6 col-md-8 pad-right u-padding-s-top"><aside class="RelatedContent u-clr-grey8" aria-label="Related content"><section class="RelatedContentPanel u-margin-s-bottom"><header id="recommended-articles-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Recommended articles</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles" class="text-xs"><ul><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article0-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0925231224008774" title="Dual separated attention-based graph neural network"><span class="anchor-text-container"><span class="anchor-text"><span>Dual separated attention-based graph neural network</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neurocomputing, Volume 599, 2024, Article 128106</div></div><div class="authors"><span>Xiao</span> <span>Shen</span>, â€¦, <span>Xi</span> <span>Zhou</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article1-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0925231224008312" title="Lp- and risk consistency of localized SVMs"><span class="anchor-text-container"><span class="anchor-text"><span>Lp- and risk consistency of localized SVMs</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neurocomputing, Volume 598, 2024, Article 128060</div></div><div class="authors"><span>Hannes</span> <span>KÃ¶hler</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article2-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0020025524009472" title="Knowledge-embedded constrained multiobjective evolutionary algorithm based on structural network control principles for personalized drug targets recognition in cancer"><span class="anchor-text-container"><span class="anchor-text"><span>Knowledge-embedded constrained multiobjective evolutionary algorithm based on structural network control principles for personalized drug targets recognition in cancer</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Information Sciences, Volume 679, 2024, Article 121033</div></div><div class="authors"><span>Kangjia</span> <span>Qiao</span>, â€¦, <span>P.N.</span> <span>Suganthan</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article3-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0957417424012922" title="Accelerated gradient descent using improved Selective Backpropagation"><span class="anchor-text-container"><span class="anchor-text"><span>Accelerated gradient descent using improved Selective Backpropagation</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Expert Systems with Applications, Volume 255, Part A, 2024, Article 124426</div></div><div class="authors"><span>Farzad</span> <span>Hosseinali</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article4-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0925231224008452" title="Neuroadaptive nonsingular fixed-time tracking control for deferred state-constrained systems based on improved command filter"><span class="anchor-text-container"><span class="anchor-text"><span>Neuroadaptive nonsingular fixed-time tracking control for deferred state-constrained systems based on improved command filter</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neurocomputing, Volume 599, 2024, Article 128074</div></div><div class="authors"><span>Xiaoning</span> <span>Lv</span>, â€¦, <span>Weihai</span> <span>Zhang</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article5-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S2214785322044170" title="Photonic density of states and photonic bandgap of deformed titanium dioxide inverse opal structure"><span class="anchor-text-container"><span class="anchor-text"><span>Photonic density of states and photonic bandgap of deformed titanium dioxide inverse opal structure</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Materials Today: Proceedings, Volume 66, Part 6, 2022, pp. 3174-3177</div></div><div class="authors"><span>Nonthanan</span> <span>Sitpathom</span>, â€¦, <span>Tanakorn</span> <span>Osotchan</span></div></div><div class="buttons"></div></li></ul></div><button class="button-link more-recommendations-button u-margin-s-bottom button-link-primary button-link-icon-right" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 3 more articles</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></section></aside></div></div></div></div><footer role="contentinfo" class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.elsevier.com/" target="_blank" aria-label="Elsevier home page (opens in a new tab)" rel="nofollow"><img class="footer-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/47/images/elsevier-non-solus-new-with-wordmark.svg" alt="Elsevier logo with wordmark" height="64" width="58" loading="lazy"/></a></div><div class="els-footer-content"><div class="u-remove-if-print"><ul class="els-footer-links u-margin-xs-bottom" style="list-style:none"><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/solutions/sciencedirect" target="_blank" id="els-footer-about-science-direct" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">About ScienceDirect</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0925231224008397" id="els-footer-remote-access" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Remote access</span></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://sd-cart.elsevier.com/?" target="_blank" id="els-footer-shopping-cart" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Shopping cart</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsmediakits.com" target="_blank" id="els-footer-advertise" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Advertise</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" target="_blank" id="els-footer-contact-support" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Contact and support</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" target="_blank" id="els-footer-terms-condition" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Terms and conditions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/privacy-policy" target="_blank" id="els-footer-privacy-policy" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Privacy policy</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li></ul></div><p id="els-footer-cookie-message" class="u-remove-if-print">Cookies are used by this site. <!-- --> <button class="button-link ot-sdk-show-settings cookie-btn button-link-primary button-link-small" id="ot-sdk-btn" type="button"><span class="button-link-text-container"><span class="button-link-text"><strong>Cookie Settings</strong></span></span></button></p><p id="els-footer-copyright">All content on this site: Copyright Â© <!-- -->2025<!-- --> Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.relx.com/" target="_blank" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx" rel="nofollow"><img loading="lazy" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/60/images/logo-relx-tm.svg" width="93" height="20" alt="RELX group home page"/></a></div></footer></div></div></div></div>
      <div id="floating-ui-node" class="floating-ui-node" data-sd-ui-floating-ui="true"></div>
      <iframe style="display: none" src="//acw.clinicalkey.com/SSOCore/update?acw=abd5e8df27df7944d579e0439fa50121e935gxrqb%7C%24%7C2B0B896E4E77701C2DB83658ABDCE5FF1F9C807A2A02798BC822C60BA7153B232FDF8648AE69D34953859A3645904E8AF4347650A1DE78E90E9169905BBD791CB0469A67597464825D387A21AFA2E514&utt=3a44-b8d18eb5491eea985a6abf2cb7b405d0f9-KTn" tabindex="-1"></iframe><iframe style="display: none" src="//acw.scopus.com/SSOCore/update?acw=abd5e8df27df7944d579e0439fa50121e935gxrqb%7C%24%7C2B0B896E4E77701C2DB83658ABDCE5FF1F9C807A2A02798BC822C60BA7153B232FDF8648AE69D34953859A3645904E8AF4347650A1DE78E90E9169905BBD791CB0469A67597464825D387A21AFA2E514&utt=3a44-b8d18eb5491eea985a6abf2cb7b405d0f9-KTn" tabindex="-1"></iframe><iframe style="display: none" src="//acw.sciencedirect.com/SSOCore/update?acw=abd5e8df27df7944d579e0439fa50121e935gxrqb%7C%24%7C2B0B896E4E77701C2DB83658ABDCE5FF1F9C807A2A02798BC822C60BA7153B232FDF8648AE69D34953859A3645904E8AF4347650A1DE78E90E9169905BBD791CB0469A67597464825D387A21AFA2E514&utt=3a44-b8d18eb5491eea985a6abf2cb7b405d0f9-KTn" tabindex="-1"></iframe><iframe style="display: none" src="//acw.elsevier.com/SSOCore/update?acw=abd5e8df27df7944d579e0439fa50121e935gxrqb%7C%24%7C2B0B896E4E77701C2DB83658ABDCE5FF1F9C807A2A02798BC822C60BA7153B232FDF8648AE69D34953859A3645904E8AF4347650A1DE78E90E9169905BBD791CB0469A67597464825D387A21AFA2E514&utt=3a44-b8d18eb5491eea985a6abf2cb7b405d0f9-KTn" tabindex="-1"></iframe>
      <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/launch-a6263b31083f.min.js" type="41105e43d13887f67dd11be4-text/javascript" async></script>
      
<script type="41105e43d13887f67dd11be4-text/javascript">
    window.pageData = {"content":[{"contentType":"JL","format":"MIME-XHTML","id":"sd:article:pii:S0925231224008397","type":"sd:article:JL:scope-abstract","detail":"sd:article:subtype:ssu","publicationType":"journal","issn":"0925-2312","volumeNumber":"599","suppl":"C","provider":"elsevier","entitlementType":"unsubscribed"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"abstract-redirected","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1736709674615,"loadTime":""},"visitor":{"accessType":"ae:ANON_GUEST","accountId":"ae:228598","accountName":"ae:ScienceDirect Guests","loginStatus":"anonymous","userId":"ae:12975512","ipAddress":"223.187.113.111","appSessionId":"54da0b20-ae9f-4b58-bb22-55af60cc2039"}};
    window.pageData.page.loadTime = performance ? Math.round(performance.now()).toString() : '';

    try {
      appData.push({
      event: 'pageLoad',
      page: pageData.page,
      visitor: pageData.visitor,
      content: pageData.content
      })
    } catch(e) {
        console.warn("There was an error loading or running Adobe DTM: ", e);
    }
</script>
      <script nomodule src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/73/js/core-js/3.20.2/core-js.es.minified.js" type="41105e43d13887f67dd11be4-text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react/18.3.1/react.production.min.js" type="41105e43d13887f67dd11be4-text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react-dom/18.3.1/react-dom.production.min.js" type="41105e43d13887f67dd11be4-text/javascript"></script>
      <script src='https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/e444bc27e06cd87e0d0139060246bdab684588aa/arp.js' async type="41105e43d13887f67dd11be4-text/javascript"></script>
      <script type="41105e43d13887f67dd11be4-text/javascript">
    const pendoData = {"visitor":{"pageName":"SD:product:journal:article","pageType":"CP-CA","pageProduct":"SD","pageLanguage":"en","pageEnvironment":"prod","accessType":"ae:ANON_GUEST","countryCode":"IN"},"account":{"id":"ae:228598","name":"ae:ScienceDirect Guests"},"events":{}};;
    pendoData.events = {
      ready: function () {
        pendo.addAltText();
      },
    };
    function runPendo(data, options) {
  const {
    firstDelay,
    maxRetries,
    urlPrefix,
    urlSuffix,
    apiKey
  } = options;
  (function (apiKey) {
    (function (p, e, n, d, o) {
      var v, w, x, y, z;
      o = p[d] = p[d] || {};
      o._q = [];
      v = ['initialize', 'identify', 'updateOptions', 'pageLoad'];
      for (w = 0, x = v.length; w < x; ++w) (function (m) {
        o[m] = o[m] || function () {
          o._q[m === v[0] ? 'unshift' : 'push']([m].concat([].slice.call(arguments, 0)));
        };
      })(v[w]);
      y = e.createElement(n);
      y.async = !0;
      y.src = urlPrefix + apiKey + urlSuffix;
      z = e.getElementsByTagName(n)[0];
      z.parentNode.insertBefore(y, z);
    })(window, document, 'script', 'pendo');
    pendo.addAltText = function () {
      var target = document.querySelector('body');
      var observer = new MutationObserver(function (mutations) {
        mutations.forEach(function (mutation) {
          if (mutation?.addedNodes?.length) {
            if (mutation.addedNodes[0]?.className?.includes("_pendo-badge")) {
              const badge = mutation.addedNodes[0];
              const altText = badge?.attributes['aria-label'].value ? badge?.attributes['aria-label'].value : 'Feedback';
              const pendoBadgeImage = pendo.dom(`#${badge?.attributes?.id.value} img`);
              if (pendoBadgeImage.length) {
                pendoBadgeImage[0]?.setAttribute('alt', altText);
              }
            }
          }
        });
      });
      var config = {
        attributeFilter: ['data-layout'],
        attributes: true,
        childList: true,
        characterData: true,
        subtree: false
      };
      observer.observe(target, config);
    };
  })(apiKey);
  (function watchAndSetPendo(nextDelay, retryAttempt) {
    if (typeof pageDataTracker === 'object' && typeof pageDataTracker.getVisitorId === 'function' && pageDataTracker.getVisitorId()) {
      data.visitor.id = pageDataTracker.getVisitorId();
      console.debug(`initializing pendo`);
      pendo.initialize(data);
    } else {
      if (retryAttempt > 0) {
        return setTimeout(function () {
          watchAndSetPendo(nextDelay * 2, retryAttempt - 1);
        }, nextDelay);
      }
      pendo.initialize(data);
      console.debug(`gave up ... pendo initialized`);
    }
  })(firstDelay, maxRetries);
}
    runPendo(pendoData, {
      firstDelay: 100,
      maxRetries: 5,
      urlPrefix: 'https://cdn.pendo.io/agent/static/',
      urlSuffix: '/pendo.js',
      apiKey: 'd6c1d995-bc7e-4e53-77f1-2ea4ecbb9565',
    });
  </script>
      <span id="pendo-answer-rating"></span>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
      <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="41105e43d13887f67dd11be4-text/javascript"></script>
      <script async src="https://www.googletagservices.com/tag/js/gpt.js" type="41105e43d13887f67dd11be4-text/javascript"></script>
      <script async src="https://scholar.google.com/scholar_js/casa.js" type="41105e43d13887f67dd11be4-text/javascript"></script>
      <script data-cfasync="false">
      (function initOneTrust()  {
        const monitor = {
  init: () => {},
  loaded: () => {},
};
        function enableGroup(group) {
  document.querySelectorAll(`script[type*="ot-${group}"]`).forEach(script => {
    script.type = 'text/javascript';
    document.head.appendChild(script);
  });
}
        function runOneTrustCookies(doClear, monitor) {
  const oneTrustConsentSdkId = 'onetrust-consent-sdk';
  const emptyNodeSelectors = 'h3.ot-host-name, h4.ot-host-desc, button.ot-host-box';
  const ariaLabelledByButtonNodes = 'div.ot-accordion-layout > button';
  const ariaAttribute = 'aria-labelledby';
  function adjustOneTrustDOM() {
    const oneTrustRoot = document.getElementById('onetrust-consent-sdk');

    /* remove empty nodes */
    [...(oneTrustRoot?.querySelectorAll(emptyNodeSelectors) ?? [])].filter(e => e.textContent === '').forEach(e => e.remove());

    /* remove invalid aria-labelledby values */
    oneTrustRoot?.querySelectorAll(ariaLabelledByButtonNodes).forEach(e => {
      const presentIdValue = e.getAttribute(ariaAttribute)?.split(' ').filter(label => document.getElementById(label)).join(' ');
      if (presentIdValue) {
        e.setAttribute(ariaAttribute, presentIdValue);
      }
    });
  }
  function observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent) {
    const cb = (mutationList, observer) => {
      const oneTrustRoot = mutationList.filter(mutationRecord => mutationRecord.type === 'childList' && mutationRecord.addedNodes.length).map(mutationRecord => [...mutationRecord.addedNodes]).flat().find(e => e.id === oneTrustConsentSdkId);
      if (oneTrustRoot && typeof OneTrust !== 'undefined') {
        monitor.loaded(true);
        OneTrust.OnConsentChanged(() => {
          const perfAllowed = decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1]?.match('2:([0|1])')[1] === '1';
          if (perfAllowed) {
            enableGroup('performance');
          }
        });
        if (!isConsentPresent && (shouldSetOTDefaults || OneTrust.GetDomainData().ConsentModel.Name === 'implied consent')) {
          OneTrust.AllowAll();
        }
        document.dispatchEvent(new CustomEvent('@sdtech/onetrust/loaded', {}));
        observer.disconnect();
        adjustOneTrustDOM();
      }
    };
    const observer = new MutationObserver(cb);
    observer.observe(document.querySelector('body'), {
      childList: true
    });
  }
  if (doClear) {
    document.cookie = 'OptanonAlertBoxClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC; samesite=lax; path=/';
  }
  const isConsentPresent = !!decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1];
  const shouldSetOTDefaults = 'true' === 'false' && !document.cookie?.match('OptanonAlertBoxClosed=');
  if (shouldSetOTDefaults) {
    const date = new Date();
    date.setFullYear(date.getFullYear() + 1);
    document.cookie = `OptanonAlertBoxClosed=${new Date().toISOString()}; expires=${date.toUTCString()}; samesite=lax; path=/; domain=sciencedirect.com`;
  }
  observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent, monitor);
  window.addOTScript = () => {
    const otSDK = document.createElement('script');
    otSDK.setAttribute('data-cfasync', 'false');
    otSDK.setAttribute('src', 'https://cdn.cookielaw.org/scripttemplates/otSDKStub.js');
    otSDK.setAttribute('data-document-language', 'true');
    otSDK.setAttribute('data-domain-script', '865ea198-88cc-4e41-8952-1df75d554d02');
    window.addOTScript = () => {};
    document.head.appendChild(otSDK);
    monitor.init();
  };
  window.addEventListener('load', () => window.addOTScript());
}
        if (document.location.host.match(/.sciencedirect.com$/)) {
          runOneTrustCookies(true, monitor);
        }
        else {
          window.addEventListener('load', (event) => {
            enableGroup('performance');
          });
        }
      }());
    </script>
    <script src="/cdn-cgi/scripts/7d0fa10a/cloudflare-static/rocket-loader.min.js" data-cf-settings="41105e43d13887f67dd11be4-|49" defer></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'900f82263ec77eb5',t:'MTczNjcwOTY3NS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
  </html>